{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 蔡秉翰\n",
    "\n",
    "Student ID: 107064527\n",
    "\n",
    "GitHub ID: k52276hank\n",
    "\n",
    "Kaggle name: Pin-Han, Tsai\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re, csv\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Raw Data to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Identification Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv and restructure dataframe (rename)\n",
    "df_iden = pd.read_csv(\"data/data_identification.csv\", sep=\"\\t\")\n",
    "df_iden['tweet_id'] = df_iden['tweet_id,identification'].str.split(',', expand=True)[0]\n",
    "df_iden['identification'] = df_iden['tweet_id,identification'].str.split(',', expand=True)[1]\n",
    "df_iden = df_iden.drop('tweet_id,identification', axis=1)\n",
    "df_iden.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Emotion Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion = pd.read_csv(\"data/emotion.csv\", sep=\"\\t\")\n",
    "df_emotion['tweet_id'] = df_emotion['tweet_id,emotion'].str.split(',', expand=True)[0]\n",
    "df_emotion['emotion'] = df_emotion['tweet_id,emotion'].str.split(',', expand=True)[1]\n",
    "df_emotion = df_emotion.drop('tweet_id,emotion', axis=1)\n",
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Duplicated of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          False\n",
       "1          False\n",
       "2          False\n",
       "3          False\n",
       "4          False\n",
       "5          False\n",
       "6          False\n",
       "7          False\n",
       "8          False\n",
       "9          False\n",
       "10         False\n",
       "11         False\n",
       "12         False\n",
       "13         False\n",
       "14         False\n",
       "15         False\n",
       "16         False\n",
       "17         False\n",
       "18         False\n",
       "19         False\n",
       "20         False\n",
       "21         False\n",
       "22         False\n",
       "23         False\n",
       "24         False\n",
       "25         False\n",
       "26         False\n",
       "27         False\n",
       "28         False\n",
       "29         False\n",
       "           ...  \n",
       "1867505    False\n",
       "1867506    False\n",
       "1867507    False\n",
       "1867508    False\n",
       "1867509    False\n",
       "1867510    False\n",
       "1867511    False\n",
       "1867512    False\n",
       "1867513    False\n",
       "1867514    False\n",
       "1867515    False\n",
       "1867516    False\n",
       "1867517    False\n",
       "1867518    False\n",
       "1867519    False\n",
       "1867520    False\n",
       "1867521    False\n",
       "1867522    False\n",
       "1867523    False\n",
       "1867524    False\n",
       "1867525    False\n",
       "1867526    False\n",
       "1867527    False\n",
       "1867528    False\n",
       "1867529    False\n",
       "1867530    False\n",
       "1867531    False\n",
       "1867532    False\n",
       "1867533    False\n",
       "1867534    False\n",
       "Length: 1867535, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if duplicated\n",
    "df_emotion.duplicated(keep='first')\n",
    "df_iden.duplicated(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read All Raw Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_index</th>\n",
       "      <th>_score</th>\n",
       "      <th>_source</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>391</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'text': '...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>433</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>232</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'text':...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>376</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'text': 'Now ISSA i...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>989</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'text': '\"Trust is ...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _crawldate          _index  _score  \\\n",
       "0  2015-05-23 11:42:47  hashtag_tweets     391   \n",
       "1  2016-01-28 04:52:09  hashtag_tweets     433   \n",
       "2  2017-12-25 04:39:20  hashtag_tweets     232   \n",
       "3  2016-01-24 23:53:05  hashtag_tweets     376   \n",
       "4  2016-01-08 17:18:59  hashtag_tweets     989   \n",
       "\n",
       "                                             _source   _type  \n",
       "0  {'tweet': {'hashtags': ['Snapchat'], 'text': '...  tweets  \n",
       "1  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...  tweets  \n",
       "2  {'tweet': {'hashtags': ['bibleverse'], 'text':...  tweets  \n",
       "3  {'tweet': {'hashtags': [], 'text': 'Now ISSA i...  tweets  \n",
       "4  {'tweet': {'hashtags': [], 'text': '\"Trust is ...  tweets  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load raw data of twitter\n",
    "total_datas = open('data/tweets_DM.json').read()\n",
    "tweet_DM = pd.read_json(total_datas, lines = True)\n",
    "tweet_DM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "2                   [bibleverse]   \n",
       "3                             []   \n",
       "4                             []   \n",
       "\n",
       "                                                text  tweet_id  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350  \n",
       "2  Confident of your obedience, I write to you, k...  0x28b412  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0  \n",
       "4  \"Trust is not the same as faith. A friend is s...  0x2de201  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some useful information from all data\n",
    "df_total = json_normalize(tweet_DM['_source'])\n",
    "df_total.rename(columns={'tweet.tweet_id':'tweet_id', \n",
    "                 'tweet.text':'text', \n",
    "                 'tweet.hashtags': 'hashtags'}, inplace=True)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Total Tweet Data and Emotion and Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "2                   [bibleverse]   \n",
       "3                             []   \n",
       "4                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "2  Confident of your obedience, I write to you, k...  0x28b412           NaN   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "4  \"Trust is not the same as faith. A friend is s...  0x2de201           NaN   \n",
       "\n",
       "  identification  \n",
       "0          train  \n",
       "1          train  \n",
       "2           test  \n",
       "3          train  \n",
       "4           test  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data sets\n",
    "df_total_merge = pd.merge(df_total, df_emotion, how='outer', on=['tweet_id'])\n",
    "df_total_merge = pd.merge(df_total_merge, df_iden, how='outer', on=['tweet_id'])\n",
    "df_total_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate Train DataFrame and Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate training and testing data\n",
    "train_df = df_total_merge[df_total_merge[\"identification\"]==\"train\"]\n",
    "test_df = df_total_merge[df_total_merge[\"identification\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification  \n",
       "0          train  \n",
       "1          train  \n",
       "3          train  \n",
       "5          train  \n",
       "6          train  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[]</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             hashtags  \\\n",
       "2                        [bibleverse]   \n",
       "4                                  []   \n",
       "9   [materialism, money, possessions]   \n",
       "30               [GodsPlan, GodsWork]   \n",
       "33                                 []   \n",
       "\n",
       "                                                 text  tweet_id emotion  \\\n",
       "2   Confident of your obedience, I write to you, k...  0x28b412     NaN   \n",
       "4   \"Trust is not the same as faith. A friend is s...  0x2de201     NaN   \n",
       "9   When do you have enough ? When are you satisfi...  0x218443     NaN   \n",
       "30  God woke you up, now chase the day #GodsPlan #...  0x2939d5     NaN   \n",
       "33  In these tough times, who do YOU turn to as yo...  0x26289a     NaN   \n",
       "\n",
       "   identification  \n",
       "2            test  \n",
       "4            test  \n",
       "9            test  \n",
       "30           test  \n",
       "33           test  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Pickle before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "train_df.to_pickle(\"data/train_df_nonpreprocess.pkl\") \n",
    "test_df.to_pickle(\"data/test_df_nonpreprocess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pickle which is Non-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(\"data/train_df_nonpreprocess.pkl\")\n",
    "test_df = pd.read_pickle(\"data/test_df_nonpreprocess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize at first without filter anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...  \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]  \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...  \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input = train_df\n",
    "df_input[\"split_to_word\"] = df_input[\"text\"].str.split()\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Replace @tag to [tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...  \n",
       "3        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]  \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...  \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"replace_tag\"] = df_input.split_to_word.apply(\n",
    "    lambda list_word: \n",
    "    [\"[tag]\" if word[0]=='@' else word for word in list_word]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Replace emoji to word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup emoji to emotion dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'😄': 'joy',\n",
       " '😛': 'joy',\n",
       " '😗': 'joy',\n",
       " '🌮': 'anticipation',\n",
       " '🍗': 'joy',\n",
       " '🏡': 'joy',\n",
       " '🗒': 'joy',\n",
       " '🕌': 'joy',\n",
       " '➡': 'joy',\n",
       " '⛓': 'joy',\n",
       " '💻': 'joy',\n",
       " '🍋': 'joy',\n",
       " '☑': 'joy',\n",
       " '\\U0001f929': 'joy',\n",
       " '🐋': 'joy',\n",
       " '📊': 'joy',\n",
       " '🏢': 'joy',\n",
       " '⬛': 'joy',\n",
       " '☎': 'joy',\n",
       " '📞': 'joy',\n",
       " '👒': 'joy',\n",
       " '💅': 'joy',\n",
       " '⛵': 'joy',\n",
       " '🗣': 'joy',\n",
       " '👢': 'joy',\n",
       " '🐖': 'joy',\n",
       " '🚩': 'joy',\n",
       " '🍸': 'joy',\n",
       " '🌸': 'joy',\n",
       " '⬆': 'joy',\n",
       " '👔': 'joy',\n",
       " '‼': 'joy',\n",
       " '\\U0001f9db': 'joy',\n",
       " '🔮': 'joy',\n",
       " '💀': 'fear',\n",
       " '🌆': 'anticipation',\n",
       " '\\U0001f94c': 'anticipation',\n",
       " '\\U0001f9d8': 'joy',\n",
       " '🇪': 'anger',\n",
       " '🐫': 'joy',\n",
       " '💮': 'sadness',\n",
       " '☪': 'anticipation',\n",
       " '🌚': 'joy',\n",
       " '😐': 'sadness',\n",
       " '⛔': 'disgust',\n",
       " '🎎': 'joy',\n",
       " '⁉': 'disgust',\n",
       " '↖': 'trust',\n",
       " '🌈': 'trust',\n",
       " '♀': 'joy',\n",
       " '🚞': 'joy',\n",
       " '🐒': 'joy',\n",
       " '🔛': 'joy',\n",
       " '🖊': 'joy',\n",
       " '🕤': 'anticipation',\n",
       " '🚎': 'joy',\n",
       " '⛴': 'joy',\n",
       " '\\U0001f954': 'joy',\n",
       " '\\U0001f98c': 'joy',\n",
       " '🛩': 'joy',\n",
       " '\\U0001f94b': 'trust',\n",
       " '\\U0001f95b': 'joy',\n",
       " '㊙': 'joy',\n",
       " '🔋': 'joy',\n",
       " '▫': 'joy',\n",
       " '☸': 'joy',\n",
       " '👶': 'joy',\n",
       " '\\U0001f940': 'joy',\n",
       " '🎰': 'joy',\n",
       " '🕘': 'anger',\n",
       " '☁': 'joy',\n",
       " '🔺': 'joy',\n",
       " '😡': 'anger',\n",
       " '🔠': 'joy',\n",
       " '💁': 'joy',\n",
       " '🔘': 'sadness',\n",
       " '❣': 'joy',\n",
       " '🙌': 'joy',\n",
       " '🗼': 'joy',\n",
       " '🆑': 'joy',\n",
       " '\\U0001f957': 'trust',\n",
       " '🐙': 'joy',\n",
       " '⛰': 'joy',\n",
       " '👥': 'joy',\n",
       " '\\U0001f955': 'trust',\n",
       " '🚋': 'joy',\n",
       " '🙅': 'joy',\n",
       " '⚓': 'trust',\n",
       " '🎭': 'joy',\n",
       " '✝': 'anticipation',\n",
       " '🙃': 'joy',\n",
       " '🍪': 'joy',\n",
       " '🎑': 'joy',\n",
       " '🐽': 'joy',\n",
       " '🇩': 'joy',\n",
       " '🚁': 'joy',\n",
       " '👟': 'joy',\n",
       " '👝': 'joy',\n",
       " '🔷': 'anger',\n",
       " '💺': 'joy',\n",
       " '\\U0001f923': 'joy',\n",
       " '🏰': 'joy',\n",
       " '⚛': 'anticipation',\n",
       " '☮': 'joy',\n",
       " '♣': 'joy',\n",
       " '🍘': 'joy',\n",
       " '🕡': 'anticipation',\n",
       " '🌐': 'joy',\n",
       " '🏧': 'joy',\n",
       " '🐉': 'joy',\n",
       " '🇬': 'anger',\n",
       " '⛸': 'anticipation',\n",
       " '🚃': 'joy',\n",
       " '⛅': 'joy',\n",
       " '💼': 'joy',\n",
       " '☃': 'joy',\n",
       " '😶': 'sadness',\n",
       " '🚗': 'joy',\n",
       " '✉': 'joy',\n",
       " '🐾': 'joy',\n",
       " '😇': 'joy',\n",
       " '🐗': 'joy',\n",
       " '👬': 'joy',\n",
       " '\\U0001f969': 'anticipation',\n",
       " '\\U0001f9d5': 'joy',\n",
       " '👵': 'joy',\n",
       " '🇷': 'anger',\n",
       " '🕊': 'joy',\n",
       " '➿': 'joy',\n",
       " '🏎': 'joy',\n",
       " '🌯': 'anticipation',\n",
       " '😏': 'joy',\n",
       " '\\U0001f956': 'anticipation',\n",
       " '🍴': 'joy',\n",
       " '🍻': 'joy',\n",
       " '🕕': 'joy',\n",
       " '😦': 'surprise',\n",
       " '\\U0001f98e': 'joy',\n",
       " '🐂': 'anticipation',\n",
       " '\\U0001f93e': 'trust',\n",
       " '😂': 'joy',\n",
       " '🚰': 'trust',\n",
       " '💧': 'joy',\n",
       " '🕟': 'anticipation',\n",
       " '⚡': 'joy',\n",
       " '💩': 'disgust',\n",
       " '📫': 'joy',\n",
       " '💢': 'joy',\n",
       " '🗨': 'joy',\n",
       " '💔': 'sadness',\n",
       " '🎾': 'joy',\n",
       " '⏭': 'joy',\n",
       " '\\U0001f93c': 'anticipation',\n",
       " '😣': 'sadness',\n",
       " '👷': 'joy',\n",
       " '\\U0001f95d': 'trust',\n",
       " '\\U0001f953': 'joy',\n",
       " '💤': 'joy',\n",
       " '🏒': 'anticipation',\n",
       " '\\U0001f950': 'joy',\n",
       " '😹': 'joy',\n",
       " '🔙': 'joy',\n",
       " '🎴': 'joy',\n",
       " '🚂': 'joy',\n",
       " '🕔': 'joy',\n",
       " '🔻': 'joy',\n",
       " '🐘': 'joy',\n",
       " '🎊': 'joy',\n",
       " '🌊': 'joy',\n",
       " '\\U0001f985': 'joy',\n",
       " '🙍': 'sadness',\n",
       " '🛍': 'joy',\n",
       " '🍞': 'anticipation',\n",
       " '🌞': 'joy',\n",
       " '📧': 'joy',\n",
       " '👻': 'fear',\n",
       " '👪': 'joy',\n",
       " '📸': 'joy',\n",
       " '🛎': 'disgust',\n",
       " '👃': 'joy',\n",
       " '🔃': 'joy',\n",
       " '🙋': 'joy',\n",
       " '🌒': 'joy',\n",
       " '🎶': 'joy',\n",
       " '🅰': 'joy',\n",
       " '👐': 'joy',\n",
       " '🚵': 'joy',\n",
       " '🎁': 'joy',\n",
       " '🍥': 'joy',\n",
       " '🚯': 'joy',\n",
       " '🐛': 'joy',\n",
       " '🚛': 'joy',\n",
       " '🍙': 'joy',\n",
       " '👓': 'joy',\n",
       " '🌹': 'joy',\n",
       " '🚳': 'joy',\n",
       " '🎤': 'joy',\n",
       " '🚣': 'joy',\n",
       " '🌇': 'joy',\n",
       " '🙉': 'joy',\n",
       " '🍶': 'joy',\n",
       " '🔯': 'joy',\n",
       " '👑': 'joy',\n",
       " '😋': 'joy',\n",
       " '\\U0001f924': 'joy',\n",
       " '🚓': 'trust',\n",
       " '🏥': 'joy',\n",
       " '⤵': 'sadness',\n",
       " '🇧': 'anger',\n",
       " '💽': 'joy',\n",
       " '🕚': 'joy',\n",
       " '🌙': 'joy',\n",
       " '🤔': 'sadness',\n",
       " '📒': 'joy',\n",
       " '☝': 'joy',\n",
       " '👁': 'joy',\n",
       " '♏': 'joy',\n",
       " '⛑': 'joy',\n",
       " '✒': 'joy',\n",
       " '🕛': 'anticipation',\n",
       " '🤒': 'anger',\n",
       " '🔀': 'fear',\n",
       " '💆': 'joy',\n",
       " '\\U0001f9df': 'disgust',\n",
       " '🌲': 'joy',\n",
       " '👧': 'joy',\n",
       " '\\U0001f98b': 'joy',\n",
       " '🛂': 'sadness',\n",
       " '♉': 'trust',\n",
       " '🐤': 'anticipation',\n",
       " '🍦': 'joy',\n",
       " '🚅': 'joy',\n",
       " '\\U0001f92f': 'joy',\n",
       " '🤐': 'anticipation',\n",
       " '♨': 'joy',\n",
       " '⚜': 'joy',\n",
       " '🔡': 'joy',\n",
       " '✋': 'joy',\n",
       " '👠': 'joy',\n",
       " '🅱': 'joy',\n",
       " '😷': 'disgust',\n",
       " '🏝': 'joy',\n",
       " '💞': 'joy',\n",
       " '🚺': 'joy',\n",
       " '📹': 'joy',\n",
       " '🐔': 'joy',\n",
       " '◼': 'sadness',\n",
       " '👀': 'joy',\n",
       " '\\U0001f997': 'anticipation',\n",
       " '\\U0001f926': 'sadness',\n",
       " '💥': 'joy',\n",
       " '🔚': 'joy',\n",
       " '\\U0001f9e6': 'joy',\n",
       " '\\U0001f961': 'anticipation',\n",
       " '🐼': 'joy',\n",
       " '👏': 'joy',\n",
       " '⛪': 'anticipation',\n",
       " '➰': 'trust',\n",
       " '\\U0001f933': 'joy',\n",
       " '🐁': 'joy',\n",
       " '👽': 'joy',\n",
       " '😤': 'anger',\n",
       " '📋': 'joy',\n",
       " '🐬': 'joy',\n",
       " '🚿': 'joy',\n",
       " '😲': 'surprise',\n",
       " '\\U0001f98f': 'anticipation',\n",
       " '📩': 'joy',\n",
       " '🔢': 'trust',\n",
       " '🙆': 'joy',\n",
       " '🏙': 'joy',\n",
       " '🍅': 'trust',\n",
       " '👘': 'joy',\n",
       " '😪': 'sadness',\n",
       " '📅': 'anticipation',\n",
       " '🈴': 'joy',\n",
       " '😁': 'joy',\n",
       " '⤴': 'trust',\n",
       " '🏛': 'joy',\n",
       " '🚴': 'joy',\n",
       " '✔': 'joy',\n",
       " '✏': 'joy',\n",
       " '🌼': 'joy',\n",
       " '📠': 'joy',\n",
       " '\\U0001f927': 'joy',\n",
       " '🇮': 'joy',\n",
       " '🔱': 'joy',\n",
       " '🐊': 'joy',\n",
       " '💟': 'joy',\n",
       " '🖋': 'trust',\n",
       " '🛬': 'joy',\n",
       " '🇽': 'joy',\n",
       " '⏪': 'joy',\n",
       " '🎡': 'anticipation',\n",
       " '\\U0001f945': 'anticipation',\n",
       " '🇨': 'anger',\n",
       " '📪': 'joy',\n",
       " '🏩': 'joy',\n",
       " '👂': 'joy',\n",
       " '🏿': 'joy',\n",
       " '🔸': 'anticipation',\n",
       " '💇': 'joy',\n",
       " '🍁': 'joy',\n",
       " '🏭': 'joy',\n",
       " '🇹': 'joy',\n",
       " '🍌': 'trust',\n",
       " '🔅': 'joy',\n",
       " '📿': 'joy',\n",
       " '⛲': 'joy',\n",
       " '😻': 'joy',\n",
       " '⛳': 'joy',\n",
       " '🌨': 'joy',\n",
       " '📟': 'joy',\n",
       " '\\U0001f947': 'joy',\n",
       " '🛁': 'joy',\n",
       " '🐚': 'joy',\n",
       " '👤': 'sadness',\n",
       " '🛌': 'joy',\n",
       " '📕': 'anticipation',\n",
       " '🎩': 'joy',\n",
       " '🇻': 'anticipation',\n",
       " '💑': 'joy',\n",
       " '🌟': 'joy',\n",
       " '🚄': 'trust',\n",
       " '🏨': 'joy',\n",
       " '😎': 'joy',\n",
       " '🔞': 'joy',\n",
       " '🔖': 'anticipation',\n",
       " '🐇': 'joy',\n",
       " '🗃': 'joy',\n",
       " '📚': 'joy',\n",
       " '😅': 'joy',\n",
       " '🆚': 'joy',\n",
       " '🍖': 'joy',\n",
       " '🔝': 'joy',\n",
       " '🔳': 'anticipation',\n",
       " '❤': 'joy',\n",
       " '🆔': 'joy',\n",
       " '\\U0001f95c': 'trust',\n",
       " '🎇': 'joy',\n",
       " '🐐': 'joy',\n",
       " '\\U0001f93a': 'joy',\n",
       " '😕': 'sadness',\n",
       " '⚗': 'sadness',\n",
       " '🍊': 'sadness',\n",
       " '\\U0001f92c': 'disgust',\n",
       " '🦀': 'joy',\n",
       " '🚙': 'joy',\n",
       " '🍳': 'trust',\n",
       " '▪': 'anticipation',\n",
       " '✴': 'anticipation',\n",
       " '🚮': 'sadness',\n",
       " '📑': 'joy',\n",
       " '\\U0001f95e': 'joy',\n",
       " '🏤': 'joy',\n",
       " '🔒': 'joy',\n",
       " '😢': 'sadness',\n",
       " '🎒': 'joy',\n",
       " '🛄': 'joy',\n",
       " '🍕': 'joy',\n",
       " '\\U0001f944': 'anticipation',\n",
       " '⚔': 'joy',\n",
       " '📰': 'joy',\n",
       " '\\U0001f922': 'disgust',\n",
       " '🎆': 'joy',\n",
       " '\\U0001f9dd': 'joy',\n",
       " '🐶': 'joy',\n",
       " '🗽': 'joy',\n",
       " '\\U0001f948': 'trust',\n",
       " '🌬': 'joy',\n",
       " '📶': 'joy',\n",
       " '🐭': 'joy',\n",
       " '🆖': 'joy',\n",
       " '🌜': 'joy',\n",
       " '🔨': 'joy',\n",
       " '⌛': 'joy',\n",
       " '🚟': 'joy',\n",
       " '🍉': 'joy',\n",
       " '🌴': 'trust',\n",
       " '🍨': 'joy',\n",
       " '⛎': 'joy',\n",
       " '㊗': 'joy',\n",
       " '🌪': 'anticipation',\n",
       " '🐦': 'joy',\n",
       " '⚠': 'joy',\n",
       " '🈹': 'trust',\n",
       " '🐥': 'joy',\n",
       " '🏂': 'joy',\n",
       " '📺': 'joy',\n",
       " '🌦': 'joy',\n",
       " '🔪': 'joy',\n",
       " '😊': 'joy',\n",
       " '📨': 'trust',\n",
       " '🛐': 'joy',\n",
       " '®': 'joy',\n",
       " '💊': 'joy',\n",
       " '💲': 'joy',\n",
       " '\\U0001f935': 'joy',\n",
       " '🐕': 'joy',\n",
       " '\\U0001f958': 'joy',\n",
       " '\\U0001f9d2': 'joy',\n",
       " '🐝': 'joy',\n",
       " '🚜': 'joy',\n",
       " '🔬': 'trust',\n",
       " '🈵': 'anticipation',\n",
       " '❕': 'joy',\n",
       " '🏫': 'joy',\n",
       " '😮': 'joy',\n",
       " '⏯': 'joy',\n",
       " '\\U0001f939': 'joy',\n",
       " '🍬': 'joy',\n",
       " '📓': 'joy',\n",
       " '🏽': 'joy',\n",
       " '🚪': 'joy',\n",
       " '\\U0001f964': 'anticipation',\n",
       " '♎': 'joy',\n",
       " '⛩': 'joy',\n",
       " '🙂': 'joy',\n",
       " '🏍': 'joy',\n",
       " '💓': 'joy',\n",
       " '📷': 'joy',\n",
       " '↕': 'trust',\n",
       " '💄': 'joy',\n",
       " '🔄': 'joy',\n",
       " '♍': 'joy',\n",
       " '😩': 'joy',\n",
       " '🚏': 'joy',\n",
       " '▶': 'joy',\n",
       " '😴': 'joy',\n",
       " '❗': 'joy',\n",
       " '🚚': 'joy',\n",
       " '📻': 'joy',\n",
       " '🚾': 'sadness',\n",
       " '🏅': 'trust',\n",
       " '\\U0001f9d0': 'joy',\n",
       " '🇴': 'joy',\n",
       " '🐱': 'joy',\n",
       " '🍹': 'joy',\n",
       " '🌳': 'joy',\n",
       " '💳': 'joy',\n",
       " '\\U0001f930': 'joy',\n",
       " '☂': 'joy',\n",
       " '\\U0001f9d7': 'joy',\n",
       " '🕗': 'joy',\n",
       " '👋': 'joy',\n",
       " '🐷': 'joy',\n",
       " '\\U0001f919': 'joy',\n",
       " '💡': 'joy',\n",
       " '©': 'joy',\n",
       " '🍰': 'joy',\n",
       " '🎵': 'joy',\n",
       " '☯': 'joy',\n",
       " '🔧': 'joy',\n",
       " '🍵': 'joy',\n",
       " '🚡': 'joy',\n",
       " '🐈': 'joy',\n",
       " '🎗': 'joy',\n",
       " '⛷': 'joy',\n",
       " '🇭': 'joy',\n",
       " '⏬': 'sadness',\n",
       " '\\U0001f921': 'sadness',\n",
       " '🐜': 'joy',\n",
       " '⏺': 'trust',\n",
       " '😵': 'disgust',\n",
       " '🏆': 'joy',\n",
       " '👌': 'joy',\n",
       " '👱': 'joy',\n",
       " '😺': 'joy',\n",
       " '🕹': 'joy',\n",
       " '🕳': 'disgust',\n",
       " '⚒': 'joy',\n",
       " '🕸': 'fear',\n",
       " '🏳': 'trust',\n",
       " '👯': 'joy',\n",
       " '💷': 'anticipation',\n",
       " '🌉': 'joy',\n",
       " '🐞': 'joy',\n",
       " '🛳': 'joy',\n",
       " '🕖': 'joy',\n",
       " '🍀': 'joy',\n",
       " '⚖': 'joy',\n",
       " '⏱': 'anticipation',\n",
       " '◽': 'trust',\n",
       " '🧀': 'joy',\n",
       " '⏹': 'joy',\n",
       " '\\U0001f942': 'joy',\n",
       " '💴': 'joy',\n",
       " '\\U0001f9d3': 'joy',\n",
       " '⬅': 'joy',\n",
       " '🐌': 'sadness',\n",
       " '💶': 'joy',\n",
       " '🍿': 'joy',\n",
       " '🚈': 'joy',\n",
       " '🚆': 'joy',\n",
       " '💨': 'joy',\n",
       " '🎮': 'joy',\n",
       " '🏵': 'sadness',\n",
       " '👎': 'disgust',\n",
       " '⬇': 'sadness',\n",
       " '❄': 'joy',\n",
       " '🚽': 'sadness',\n",
       " '😔': 'sadness',\n",
       " '🎷': 'joy',\n",
       " '🐍': 'disgust',\n",
       " '🍾': 'joy',\n",
       " '🆘': 'disgust',\n",
       " '🏔': 'joy',\n",
       " '➕': 'joy',\n",
       " '😘': 'joy',\n",
       " '🌗': 'joy',\n",
       " '👳': 'joy',\n",
       " '👰': 'joy',\n",
       " '👹': 'anticipation',\n",
       " '🏪': 'joy',\n",
       " '\\U0001f94a': 'anticipation',\n",
       " '🖕': 'disgust',\n",
       " '💒': 'anticipation',\n",
       " '🍟': 'anticipation',\n",
       " '📵': 'sadness',\n",
       " '\\U0001f92b': 'joy',\n",
       " '↪': 'anticipation',\n",
       " '👭': 'joy',\n",
       " '📙': 'trust',\n",
       " '\\U0001f990': 'joy',\n",
       " '🎺': 'anticipation',\n",
       " '🏴': 'trust',\n",
       " '🔹': 'anticipation',\n",
       " '🍔': 'joy',\n",
       " '📗': 'joy',\n",
       " '🌽': 'joy',\n",
       " '🌀': 'joy',\n",
       " '⛽': 'joy',\n",
       " '⭐': 'joy',\n",
       " '🏀': 'joy',\n",
       " '\\U0001f9e0': 'joy',\n",
       " '🛋': 'joy',\n",
       " '⚕': 'joy',\n",
       " '💪': 'joy',\n",
       " '🔜': 'joy',\n",
       " '🍜': 'joy',\n",
       " '🕓': 'joy',\n",
       " '🖼': 'joy',\n",
       " '🔆': 'joy',\n",
       " '📽': 'joy',\n",
       " '🚐': 'anticipation',\n",
       " '🤘': 'joy',\n",
       " '📌': 'joy',\n",
       " '🌿': 'joy',\n",
       " '🔩': 'joy',\n",
       " '👦': 'joy',\n",
       " '🅾': 'joy',\n",
       " '🐡': 'joy',\n",
       " '↘': 'sadness',\n",
       " '♦': 'joy',\n",
       " '❎': 'joy',\n",
       " '😯': 'surprise',\n",
       " '\\U0001f992': 'joy',\n",
       " '♠': 'joy',\n",
       " '😓': 'sadness',\n",
       " '🇳': 'anger',\n",
       " '🌥': 'joy',\n",
       " '\\U0001f920': 'joy',\n",
       " '🏋': 'joy',\n",
       " '😫': 'sadness',\n",
       " '🕉': 'joy',\n",
       " '🙇': 'joy',\n",
       " '🐺': 'joy',\n",
       " '♿': 'joy',\n",
       " '🏖': 'joy',\n",
       " '🐰': 'joy',\n",
       " '🎐': 'joy',\n",
       " '🐸': 'anticipation',\n",
       " '📝': 'joy',\n",
       " '😾': 'disgust',\n",
       " '🚹': 'anticipation',\n",
       " '👆': 'joy',\n",
       " '🎯': 'joy',\n",
       " '💃': 'joy',\n",
       " '🌓': 'joy',\n",
       " '👉': 'joy',\n",
       " '😞': 'sadness',\n",
       " '🍒': 'trust',\n",
       " '📣': 'joy',\n",
       " '\\U0001f98a': 'joy',\n",
       " '🏮': 'joy',\n",
       " '🌱': 'joy',\n",
       " '😑': 'disgust',\n",
       " '📭': 'trust',\n",
       " '🍼': 'joy',\n",
       " '👺': 'sadness',\n",
       " '🐧': 'joy',\n",
       " '🤗': 'joy',\n",
       " '🐀': 'joy',\n",
       " '🍩': 'joy',\n",
       " '🌰': 'joy',\n",
       " '✡': 'anticipation',\n",
       " '🗺': 'joy',\n",
       " '👛': 'joy',\n",
       " '🎳': 'joy',\n",
       " '🍲': 'joy',\n",
       " '🏼': 'joy',\n",
       " '♌': 'joy',\n",
       " '\\U0001f994': 'joy',\n",
       " '🆕': 'joy',\n",
       " '\\U0001f952': 'trust',\n",
       " '😙': 'joy',\n",
       " '🌭': 'joy',\n",
       " '💍': 'joy',\n",
       " '🍓': 'trust',\n",
       " '\\U0001f928': 'disgust',\n",
       " '📖': 'anticipation',\n",
       " '🔁': 'joy',\n",
       " '➖': 'joy',\n",
       " '🛡': 'anticipation',\n",
       " '🛫': 'anticipation',\n",
       " '😭': 'joy',\n",
       " '📉': 'sadness',\n",
       " '😨': 'surprise',\n",
       " '👕': 'joy',\n",
       " '🐻': 'anticipation',\n",
       " '\\U0001f934': 'joy',\n",
       " '🏐': 'trust',\n",
       " '🕙': 'anticipation',\n",
       " '⏰': 'anticipation',\n",
       " '\\U0001f93d': 'joy',\n",
       " '✍': 'anticipation',\n",
       " '\\U0001f91c': 'joy',\n",
       " '😌': 'joy',\n",
       " '🇶': 'joy',\n",
       " '👡': 'joy',\n",
       " '👣': 'joy',\n",
       " '🤓': 'joy',\n",
       " '\\U0001f6f7': 'anticipation',\n",
       " '😸': 'joy',\n",
       " '✌': 'joy',\n",
       " '🈂': 'anticipation',\n",
       " '\\U0001f6f8': 'joy',\n",
       " '🔶': 'anger',\n",
       " '♒': 'joy',\n",
       " '🐮': 'joy',\n",
       " '\\U0001f989': 'joy',\n",
       " '🐏': 'joy',\n",
       " '💝': 'joy',\n",
       " '🏗': 'joy',\n",
       " '🏠': 'joy',\n",
       " '💌': 'joy',\n",
       " '📴': 'sadness',\n",
       " '🗄': 'sadness',\n",
       " '🌄': 'joy',\n",
       " '🚔': 'trust',\n",
       " '🏃': 'trust',\n",
       " '\\U0001f9d1': 'joy',\n",
       " '🏌': 'joy',\n",
       " '🗓': 'joy',\n",
       " '🎪': 'anticipation',\n",
       " '🍏': 'trust',\n",
       " '🖐': 'joy',\n",
       " '💱': 'joy',\n",
       " '📍': 'joy',\n",
       " '🔌': 'joy',\n",
       " '\\U0001f965': 'joy',\n",
       " '🌺': 'joy',\n",
       " '😃': 'joy',\n",
       " '🎨': 'trust',\n",
       " '🔣': 'joy',\n",
       " '🍛': 'joy',\n",
       " '💠': 'joy',\n",
       " '\\U0001f91e': 'joy',\n",
       " '💰': 'joy',\n",
       " '🐑': 'joy',\n",
       " '⭕': 'joy',\n",
       " '🕞': 'disgust',\n",
       " '🗾': 'joy',\n",
       " '🚇': 'joy',\n",
       " '♾': 'joy',\n",
       " '⚪': 'anticipation',\n",
       " '🏊': 'joy',\n",
       " '📡': 'joy',\n",
       " '🐳': 'joy',\n",
       " '😆': 'joy',\n",
       " '🙊': 'joy',\n",
       " '⛹': 'joy',\n",
       " '🐿': 'joy',\n",
       " '🚢': 'trust',\n",
       " '👚': 'joy',\n",
       " '🎖': 'joy',\n",
       " '⚾': 'joy',\n",
       " '💾': 'joy',\n",
       " '\\U0001f991': 'joy',\n",
       " '🚊': 'joy',\n",
       " '🍣': 'joy',\n",
       " '\\U0001f966': 'trust',\n",
       " '\\U0001f968': 'joy',\n",
       " '🔦': 'joy',\n",
       " '\\U0001f995': 'joy',\n",
       " '✅': 'joy',\n",
       " '💣': 'joy',\n",
       " '🚶': 'joy',\n",
       " '🔗': 'joy',\n",
       " '🐆': 'joy',\n",
       " '🎥': 'joy',\n",
       " '💿': 'joy',\n",
       " '☔': 'joy',\n",
       " '🎓': 'joy',\n",
       " '⏲': 'anticipation',\n",
       " '🗻': 'joy',\n",
       " '⚰': 'fear',\n",
       " '🛀': 'joy',\n",
       " '🤖': 'joy',\n",
       " '📳': 'trust',\n",
       " '👖': 'joy',\n",
       " '💗': 'joy',\n",
       " '\\U0001f91d': 'joy',\n",
       " '\\U0001f986': 'joy',\n",
       " '\\U0001f6f4': 'joy',\n",
       " '🐨': 'joy',\n",
       " '🇺': 'anticipation',\n",
       " '🛣': 'joy',\n",
       " '🌡': 'joy',\n",
       " '📯': 'joy',\n",
       " '🏓': 'joy',\n",
       " '\\U0001f987': 'joy',\n",
       " '⛺': 'joy',\n",
       " '🆒': 'joy',\n",
       " '🎙': 'joy',\n",
       " '👅': 'joy',\n",
       " '🆎': 'disgust',\n",
       " '🕯': 'joy',\n",
       " '📇': 'joy',\n",
       " '❓': 'joy',\n",
       " '📁': 'joy',\n",
       " '🙄': 'disgust',\n",
       " '\\U0001f9e1': 'joy',\n",
       " '\\U0001f9d6': 'joy',\n",
       " '🐟': 'joy',\n",
       " '\\U0001f943': 'joy',\n",
       " '📆': 'joy',\n",
       " '🖌': 'joy',\n",
       " '💦': 'joy',\n",
       " '🕷': 'fear',\n",
       " '☄': 'joy',\n",
       " '✂': 'joy',\n",
       " '🎞': 'joy',\n",
       " '⏳': 'anticipation',\n",
       " '\\U0001f988': 'joy',\n",
       " '♂': 'joy',\n",
       " '🐹': 'trust',\n",
       " '💬': 'joy',\n",
       " '📂': 'trust',\n",
       " '🐠': 'joy',\n",
       " '🚘': 'joy',\n",
       " '🎅': 'joy',\n",
       " '🎦': 'anticipation',\n",
       " '📎': 'joy',\n",
       " '🎫': 'anticipation',\n",
       " '🐄': 'joy',\n",
       " '😈': 'joy',\n",
       " '😼': 'joy',\n",
       " '😱': 'surprise',\n",
       " '🚦': 'joy',\n",
       " '🔲': 'joy',\n",
       " '🚉': 'joy',\n",
       " '⚫': 'anticipation',\n",
       " 'ℹ': 'joy',\n",
       " '🦁': 'trust',\n",
       " '🖖': 'joy',\n",
       " '🔓': 'joy',\n",
       " '🍡': 'joy',\n",
       " '\\U0001f967': 'trust',\n",
       " '💕': 'joy',\n",
       " '🌵': 'joy',\n",
       " '🏄': 'joy',\n",
       " '♈': 'trust',\n",
       " '\\U0001f9d9': 'joy',\n",
       " '\\U0001f9dc': 'joy',\n",
       " '👜': 'joy',\n",
       " '📄': 'joy',\n",
       " '👮': 'trust',\n",
       " '🏦': 'joy',\n",
       " '🙀': 'anticipation',\n",
       " '🃏': 'anticipation',\n",
       " '⛱': 'joy',\n",
       " '🕢': 'anticipation',\n",
       " '\\U0001f95a': 'joy',\n",
       " '🍯': 'joy',\n",
       " '🌛': 'joy',\n",
       " '\\U0001f96b': 'trust',\n",
       " '🌻': 'joy',\n",
       " '⛄': 'joy',\n",
       " '☹': 'sadness',\n",
       " '🐃': 'joy',\n",
       " '🎣': 'joy',\n",
       " '🏁': 'joy',\n",
       " '🌔': 'joy',\n",
       " '♋': 'joy',\n",
       " '🇫': 'joy',\n",
       " '🏟': 'joy',\n",
       " '🎠': 'joy',\n",
       " '🕦': 'sadness',\n",
       " '🐎': 'joy',\n",
       " '💎': 'joy',\n",
       " '✈': 'joy',\n",
       " '💚': 'joy',\n",
       " '🇼': 'trust',\n",
       " '🇯': 'joy',\n",
       " '🌘': 'joy',\n",
       " '😳': 'sadness',\n",
       " '🕶': 'joy',\n",
       " '🏹': 'joy',\n",
       " '\\U0001f9de': 'joy',\n",
       " '💙': 'joy',\n",
       " '\\U0001f92e': 'disgust',\n",
       " '🏉': 'anticipation',\n",
       " '🗡': 'joy',\n",
       " '🎀': 'joy',\n",
       " '⬜': 'joy',\n",
       " '\\U0001f932': 'trust',\n",
       " '👫': 'joy',\n",
       " '☘': 'joy',\n",
       " '🉑': 'joy',\n",
       " '🅿': 'joy',\n",
       " '💐': 'joy',\n",
       " '🔫': 'joy',\n",
       " '♊': 'joy',\n",
       " '⌚': 'joy',\n",
       " '🕝': 'anticipation',\n",
       " '💂': 'joy',\n",
       " '🏾': 'joy',\n",
       " '🔑': 'joy',\n",
       " '⏫': 'joy',\n",
       " '👾': 'joy',\n",
       " '〰': 'joy',\n",
       " '🌃': 'joy',\n",
       " '💜': 'joy',\n",
       " '🚤': 'joy',\n",
       " '👈': 'joy',\n",
       " '🤕': 'sadness',\n",
       " '🍎': 'joy',\n",
       " '🍄': 'joy',\n",
       " '🍧': 'joy',\n",
       " '🙈': 'joy',\n",
       " '\\U0001f938': 'joy',\n",
       " '🏣': 'joy',\n",
       " '🍑': 'joy',\n",
       " '🚨': 'joy',\n",
       " '🚭': 'trust',\n",
       " '👞': 'joy',\n",
       " '🐯': 'joy',\n",
       " '🗑': 'disgust',\n",
       " '🕜': 'joy',\n",
       " '🗝': 'joy',\n",
       " '🏕': 'anticipation',\n",
       " '⛏': 'joy',\n",
       " '🏺': 'joy',\n",
       " '🍐': 'joy',\n",
       " '↔': 'joy',\n",
       " '📱': 'joy',\n",
       " '♓': 'joy',\n",
       " '😿': 'sadness',\n",
       " '\\U0001f91a': 'joy',\n",
       " '💈': 'joy',\n",
       " '🎹': 'joy',\n",
       " '👨': 'joy',\n",
       " '🌏': 'joy',\n",
       " '🤑': 'joy',\n",
       " '🐴': 'joy',\n",
       " '🔊': 'joy',\n",
       " '🔉': 'joy',\n",
       " '👩': 'joy',\n",
       " '🦂': 'joy',\n",
       " '📢': 'joy',\n",
       " '🎱': 'joy',\n",
       " '🕠': 'trust',\n",
       " '\\U0001f9e2': 'joy',\n",
       " '🌁': 'joy',\n",
       " '🎸': 'joy',\n",
       " '🌅': 'joy',\n",
       " '🇦': 'anger',\n",
       " '💫': 'joy',\n",
       " '\\U0001f925': 'sadness',\n",
       " '👄': 'joy',\n",
       " '⛈': 'joy',\n",
       " '✖': 'joy',\n",
       " '🕒': 'anticipation',\n",
       " '🏬': 'joy',\n",
       " '🐢': 'joy',\n",
       " '☦': 'trust',\n",
       " '🚲': 'joy',\n",
       " '⏸': 'joy',\n",
       " '\\U0001f959': 'joy',\n",
       " '🙎': 'joy',\n",
       " '👇': 'joy',\n",
       " '😍': 'joy',\n",
       " '📜': 'joy',\n",
       " '🏘': 'joy',\n",
       " '🏏': 'anticipation',\n",
       " '\\U0001f57a': 'joy',\n",
       " '☢': 'joy',\n",
       " '🛅': 'joy',\n",
       " '👲': 'joy',\n",
       " '😝': 'joy',\n",
       " '🚫': 'joy',\n",
       " '♻': 'joy',\n",
       " '🦄': 'joy',\n",
       " '🚀': 'joy',\n",
       " '📛': 'joy',\n",
       " '\\U0001f949': 'trust',\n",
       " '🍷': 'joy',\n",
       " '😒': 'disgust',\n",
       " '🕍': 'anticipation',\n",
       " '🗞': 'joy',\n",
       " '🎋': 'joy',\n",
       " '💉': 'joy',\n",
       " '🖍': 'joy',\n",
       " '🎚': 'anticipation',\n",
       " '🗿': 'anticipation',\n",
       " '⚱': 'fear',\n",
       " '🔍': 'joy',\n",
       " '🦃': 'trust',\n",
       " '☣': 'joy',\n",
       " '\\U0001f9e5': 'anticipation',\n",
       " '🏑': 'anticipation',\n",
       " '💵': 'joy',\n",
       " '🕧': 'trust',\n",
       " '❔': 'anticipation',\n",
       " '🚱': 'sadness',\n",
       " '🇲': 'joy',\n",
       " '\\U0001f936': 'joy',\n",
       " '〽': 'joy',\n",
       " '◾': 'joy',\n",
       " '🔇': 'joy',\n",
       " '💋': 'joy',\n",
       " '🌎': 'joy',\n",
       " '⚽': 'anticipation',\n",
       " '\\U0001f9e4': 'joy',\n",
       " '🌕': 'joy',\n",
       " '👴': 'joy',\n",
       " '🙏': 'joy',\n",
       " '\\U0001f98d': 'joy',\n",
       " '💛': 'joy',\n",
       " '🍱': 'joy',\n",
       " '🔵': 'anticipation',\n",
       " '🖨': 'joy',\n",
       " '\\U0001f960': 'joy',\n",
       " '↙': 'anticipation',\n",
       " '📃': 'joy',\n",
       " '🎽': 'trust',\n",
       " '↗': 'trust',\n",
       " '😉': 'joy',\n",
       " '☕': 'joy',\n",
       " '🌍': 'joy',\n",
       " '🌫': 'joy',\n",
       " '🔴': 'anticipation',\n",
       " '📘': 'joy',\n",
       " '📲': 'joy',\n",
       " '🚼': 'joy',\n",
       " '🚌': 'anticipation',\n",
       " '☀': 'joy',\n",
       " '🐲': 'joy',\n",
       " '🕵': 'joy',\n",
       " '\\U0001f931': 'joy',\n",
       " '🍆': 'joy',\n",
       " '🕥': 'anticipation',\n",
       " '📥': 'joy',\n",
       " '🍇': 'joy',\n",
       " '🚷': 'joy',\n",
       " '\\U0001f963': 'joy',\n",
       " '🎬': 'joy',\n",
       " '\\U0001f6d1': 'joy',\n",
       " '😬': 'anticipation',\n",
       " '🐣': 'joy',\n",
       " '🍫': 'joy',\n",
       " '\\U0001f962': 'joy',\n",
       " '🕑': 'joy',\n",
       " '✨': 'joy',\n",
       " '🖥': 'joy',\n",
       " '🐪': 'joy',\n",
       " '\\U0001f937': 'joy',\n",
       " '🎟': 'anticipation',\n",
       " '🍭': 'joy',\n",
       " '🚻': 'anticipation',\n",
       " '💖': 'joy',\n",
       " '\\U0001f6f6': 'joy',\n",
       " '🖱': 'trust',\n",
       " '♑': 'joy',\n",
       " '🍮': 'joy',\n",
       " '☺': 'joy',\n",
       " '🐓': 'joy',\n",
       " '🆓': 'joy',\n",
       " '◀': 'joy',\n",
       " '⌨': 'joy',\n",
       " '🇵': 'joy',\n",
       " '👊': 'joy',\n",
       " '🎃': 'fear',\n",
       " '🕴': 'joy',\n",
       " '🕐': 'anticipation',\n",
       " '📔': 'joy',\n",
       " '🏇': 'joy',\n",
       " '\\U0001f5a4': 'joy',\n",
       " '♥': 'joy',\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.unicode_codes import EMOJI_UNICODE, UNICODE_EMOJI\n",
    "\n",
    "list_cal_emoji = df_input.replace_tag.apply(\n",
    "    lambda list_word: \n",
    "    [char for char in \" \".join(list_word) if char in UNICODE_EMOJI]\n",
    ")\n",
    "\n",
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']\n",
    "dict_emoji_all_emotion = defaultdict(list)\n",
    "dict_emoji_emotion = {}\n",
    "seties_emotion = df_input.emotion\n",
    "for idx, list_emoji in enumerate(list_cal_emoji): \n",
    "    list_emoji = list(set(list_emoji))  \n",
    "    for emoji in list_emoji:\n",
    "        if not dict_emoji_all_emotion[emoji]: dict_emoji_all_emotion[emoji]=[0]*len(emotion_array)\n",
    "        dict_emoji_all_emotion[emoji][emotion_array.index(seties_emotion.iloc[idx])] += 1\n",
    "\n",
    "for emoji, list_emoji_num in dict_emoji_all_emotion.items():\n",
    "    dict_emoji_emotion[emoji] = emotion_array[np.argmax(list_emoji_num)]\n",
    "\n",
    "dict_emoji_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...  \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]  \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...  \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_replaced_emoji = []\n",
    "for list_word in df_input.replace_tag:\n",
    "    sentence_before = \" \".join(list_word)\n",
    "    sentence_after = \"\"\n",
    "    for idx, char in enumerate(sentence_before):\n",
    "#         if idx: \n",
    "#             print(char, sentence_before[idx-1], (char == sentence_before[idx-1]))\n",
    "        if char in UNICODE_EMOJI :\n",
    "            if not idx: sentence_after += dict_emoji_emotion[char]\n",
    "            elif char != sentence_before[idx-1]: sentence_after += dict_emoji_emotion[char]\n",
    "            else: continue\n",
    "        else: sentence_after += char\n",
    "    list_replaced_emoji.append(sentence_after.split())\n",
    "df_input[\"replace_emoji\"] = np.array(list_replaced_emoji)\n",
    "\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Replace abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup abbreviation to sentence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7K': ['Sick'],\n",
       " ':-D': ['Laugher'],\n",
       " 'A3': ['Anytime', 'Anywhere', 'Anyplace'],\n",
       " 'AFAIK': ['As', 'Far', 'As', 'I', 'Know'],\n",
       " 'AFK': ['Away', 'From', 'Keyboard'],\n",
       " 'ASAP': ['As', 'Soon', 'As', 'Possible'],\n",
       " 'ASL': ['Age', 'Sex', 'Location'],\n",
       " 'ATK': ['At', 'The', 'Keyboard'],\n",
       " 'ATM': ['At', 'The', 'Moment'],\n",
       " 'B4': ['Before'],\n",
       " 'B4N': ['Bye', 'For', 'Now'],\n",
       " 'BAK': ['Back', 'At', 'Keyboard'],\n",
       " 'BBL': ['Be', 'Back', 'Later'],\n",
       " 'BBS': ['Be', 'Back', 'Soon'],\n",
       " 'BFN': ['Bye', 'For', 'Now'],\n",
       " 'BRB': ['Be', 'Right', 'Back'],\n",
       " 'BRT': ['Be', 'Right', 'There'],\n",
       " 'BTW': ['By', 'The', 'Way'],\n",
       " 'CU': ['See', 'You'],\n",
       " 'CUL8R': ['See', 'You', 'Later'],\n",
       " 'CYA': ['See', 'You'],\n",
       " 'FAQ': ['Frequently', 'Asked', 'Questions'],\n",
       " 'FC': ['Fingers', 'Crossed'],\n",
       " 'FWIW': ['For', 'What', \"It's\", 'Worth'],\n",
       " 'FYI': ['For', 'Your', 'Information'],\n",
       " 'G9': ['Genius'],\n",
       " 'GAL': ['Get', 'A', 'Life'],\n",
       " 'GG': ['Good', 'Game'],\n",
       " 'GMTA': ['Great', 'Minds', 'Think', 'Alike'],\n",
       " 'GN': ['Good', 'Night'],\n",
       " 'GR8': ['Great'],\n",
       " 'IC': ['I', 'See'],\n",
       " 'ICQ': ['I', 'Seek', 'you'],\n",
       " 'ILU': ['I', 'Love', 'You'],\n",
       " 'IMHO': ['In', 'My', 'Honest'],\n",
       " 'IMO': ['In', 'My', 'Opinion'],\n",
       " 'IOW': ['In', 'Other', 'Words'],\n",
       " 'IRL': ['In', 'Real', 'Life'],\n",
       " 'KISS': ['Keep', 'It', 'Simple,', 'Stupid'],\n",
       " 'L8R': ['Later'],\n",
       " 'LDR': ['Long', 'Distance', 'Relationship'],\n",
       " 'LMAO': ['Laugh', 'My', 'A', 'Off'],\n",
       " 'LOL': ['Laughing', 'Out', 'Loud'],\n",
       " 'LTNS': ['Long', 'Time', 'No', 'See'],\n",
       " 'M8': ['Mate'],\n",
       " 'MTE': ['My', 'Thoughts', 'Exactly'],\n",
       " 'NRN': ['No', 'Reply', 'Necessary'],\n",
       " 'OIC': ['Oh', 'I', 'See'],\n",
       " 'PITA': ['Pain', 'In', 'The', 'A'],\n",
       " 'PRT': ['Party'],\n",
       " 'PRW': ['Parents', 'Are', 'Watching'],\n",
       " 'QPSA': ['Que', 'Pasa'],\n",
       " 'ROFL': ['Rolling', 'On', 'The', 'Floor', 'Laughing'],\n",
       " 'ROFLOL': ['Rolling', 'On', 'The', 'Floor', 'Laughing', 'Out', 'Loud'],\n",
       " 'ROTFLMAO': ['Rolling', 'On', 'The', 'Floor', 'Laughing', 'My', 'A', 'Off'],\n",
       " 'SK8': ['Skate'],\n",
       " 'STATS': ['Your', 'sex', 'and', 'age'],\n",
       " 'THX': ['Thank', 'You'],\n",
       " 'TTFN': ['Ta-Ta', 'For', 'Now'],\n",
       " 'TTYL': ['Talk', 'To', 'You', 'Later'],\n",
       " 'U': ['You'],\n",
       " 'U2': ['You', 'Too'],\n",
       " 'U4E': ['Yours', 'For', 'Ever'],\n",
       " 'W8': ['Wait'],\n",
       " 'WB': ['Welcome', 'Back'],\n",
       " 'WTF': ['What', 'The', 'Fuck'],\n",
       " 'WTG': ['Way', 'To', 'Go'],\n",
       " 'WUF': ['Where', 'Are', 'You', 'From']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_abbreviation = {}\n",
    "with open(\"data/abbreviations.txt\", 'r') as f_abbreviation:\n",
    "    for idx, line in enumerate(f_abbreviation.readlines()):\n",
    "        abbreviation, sentence = line.strip().split('=')\n",
    "        dict_abbreviation[abbreviation] = sentence.split()\n",
    "dict_abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...  \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]  \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...  \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"replace_abbreviation\"] = df_input.replace_emoji.apply(\n",
    "    lambda list_word: \n",
    "    [word for list_word_after in [dict_abbreviation[word] if word in dict_abbreviation else [word] for word in list_word] for word in list_word_after]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Remove punctuation, LH and turn to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "      <th>remove_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], as, we, see, trump, is, dangerous, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[now, issa, is, stalking, tasha, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                  remove_punctuation  \n",
       "0  [people, who, post, add, me, on, snapchat, mus...  \n",
       "1  [[tag], as, we, see, trump, is, dangerous, to,...  \n",
       "3              [now, issa, is, stalking, tasha, joy]  \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...  \n",
       "6      [still, waiting, on, those, supplies, liscus]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"remove_punctuation\"] = df_input.replace_abbreviation.apply(\n",
    "    lambda list_word: \n",
    "    [word for word in [re.sub('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'), '', word.replace(\"<LH>\", '').lower()) if word!=\"[tag]\" else word for word in list_word ] if word]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "      <th>remove_punctuation</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>remove_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[[people, who, post, add, me, on, snapchat, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], as, we, see, trump, is, dangerous, to,...</td>\n",
       "      <td>[[tag], a, we, see, trump, be, dangerous, to, ...</td>\n",
       "      <td>[[[tag], a, we, see, trump, be, dangerous, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[now, issa, is, stalking, tasha, joy]</td>\n",
       "      <td>[now, issa, be, stalk, tasha, joy]</td>\n",
       "      <td>[[now, issa, be, stalk, tasha, joy]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[[tag], [tag], thx, for, the, best, time, ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus]</td>\n",
       "      <td>[still, wait, on, those, supply, liscus]</td>\n",
       "      <td>[[still, wait, on, those, supply, liscus]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                  remove_punctuation  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], as, we, see, trump, is, dangerous, to,...   \n",
       "3              [now, issa, is, stalking, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6      [still, waiting, on, those, supplies, liscus]   \n",
       "\n",
       "                                       lemmatization  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], a, we, see, trump, be, dangerous, to, ...   \n",
       "3                 [now, issa, be, stalk, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6           [still, wait, on, those, supply, liscus]   \n",
       "\n",
       "                                     remove_stopword  \n",
       "0  [[people, who, post, add, me, on, snapchat, mu...  \n",
       "1  [[[tag], a, we, see, trump, be, dangerous, to,...  \n",
       "3               [[now, issa, be, stalk, tasha, joy]]  \n",
       "5  [[[tag], [tag], thx, for, the, best, time, ton...  \n",
       "6         [[still, wait, on, those, supply, liscus]]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(list_word):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(list_word):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n",
    "\n",
    "# lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split()])\n",
    "df_input[\"lemmatization\"] = df_input.remove_punctuation.apply(\n",
    "    lambda list_word: \n",
    "    lemmatize_sentence(list_word)\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Remove stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup stopword with tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stopword_all.txt\", 'r') as f_stopword:\n",
    "    list_word = stopwords.words('english')\n",
    "    for line in f_stopword.readlines():\n",
    "#         print(line.strip().split(','))\n",
    "        list_word.extend(line.strip().split(','))\n",
    "    list_word = list(set(list_word))\n",
    "\n",
    "with open(\"data/stopword.txt\", 'w') as f_stopword:\n",
    "    f_stopword.write(\"\\n\".join(list_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read stopword file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['broken',\n",
       " 'rice',\n",
       " '',\n",
       " 'recent',\n",
       " 'uploaded',\n",
       " 'viv',\n",
       " 'created',\n",
       " 'builds',\n",
       " 'bloggers',\n",
       " 'requests',\n",
       " 'couple',\n",
       " 'enables',\n",
       " 'ordering',\n",
       " 'eaten',\n",
       " 'dinner',\n",
       " 'rental',\n",
       " 'money',\n",
       " 'byeeee',\n",
       " 'serves',\n",
       " \"mustn't\",\n",
       " 'online',\n",
       " 'courses',\n",
       " 'whatev',\n",
       " 'newly',\n",
       " 'whaow',\n",
       " 'events',\n",
       " 'dear',\n",
       " 'neways',\n",
       " \"should've\",\n",
       " 'gift',\n",
       " 'programs',\n",
       " 'updates',\n",
       " 'lols',\n",
       " 'nooo',\n",
       " 'tired',\n",
       " 'doubles',\n",
       " 'ymmv',\n",
       " 'shops',\n",
       " 'waited',\n",
       " 'feels',\n",
       " 'com',\n",
       " 'shouldn',\n",
       " 'morning',\n",
       " 'distract',\n",
       " 'excluded',\n",
       " 'recovers',\n",
       " 'writes',\n",
       " 'occured',\n",
       " 'yhoo',\n",
       " 'gave',\n",
       " 'treated',\n",
       " '.com',\n",
       " 'homeworks',\n",
       " 'upgrade',\n",
       " 'afaik',\n",
       " 'projects',\n",
       " 'issue',\n",
       " 'albums',\n",
       " 'isn',\n",
       " 'catch',\n",
       " 'hee',\n",
       " 'btw',\n",
       " 'box',\n",
       " 'february',\n",
       " 'is',\n",
       " 'singled',\n",
       " 'hehehe',\n",
       " 'safe',\n",
       " 'produces',\n",
       " 'approve',\n",
       " 'wahaha',\n",
       " 'buys',\n",
       " 'dont',\n",
       " '#followme',\n",
       " 'furnish',\n",
       " 'does',\n",
       " 'sms',\n",
       " 'loaned',\n",
       " 'primary',\n",
       " 'june',\n",
       " 'refuses',\n",
       " 'talked',\n",
       " 'whom',\n",
       " 'removes',\n",
       " 'dated',\n",
       " 'duper',\n",
       " 'condominium',\n",
       " 'huge',\n",
       " 'obvious',\n",
       " 'tbh',\n",
       " 'woohoo',\n",
       " 'runs',\n",
       " 'yeah',\n",
       " 'company',\n",
       " 'balance',\n",
       " 'saved',\n",
       " 'months',\n",
       " 'today',\n",
       " 'choice',\n",
       " 'interacting',\n",
       " 'iirc',\n",
       " 'purchased',\n",
       " 'weeks',\n",
       " 'hahahaha',\n",
       " 'no',\n",
       " 'hmmm',\n",
       " 'form',\n",
       " 'ftl',\n",
       " 'significant',\n",
       " 'holidays',\n",
       " \"you've\",\n",
       " 'both',\n",
       " 'watever',\n",
       " 'kid',\n",
       " 'organize',\n",
       " 'here',\n",
       " 'order',\n",
       " 'study',\n",
       " 'mens',\n",
       " 'wasnt',\n",
       " 'that',\n",
       " 'sets',\n",
       " 'sqft',\n",
       " 'gg',\n",
       " 'bed',\n",
       " 'allergy',\n",
       " 'resort',\n",
       " 'update',\n",
       " 'omg',\n",
       " 'early',\n",
       " 'why',\n",
       " 'only',\n",
       " 'hour',\n",
       " 'his',\n",
       " 'hopefully',\n",
       " 'consultant',\n",
       " 'announcement',\n",
       " 'bought',\n",
       " 'yah',\n",
       " 'saves',\n",
       " 'replaced',\n",
       " \"it's\",\n",
       " 'some',\n",
       " 'down',\n",
       " 'shitz',\n",
       " 'locations',\n",
       " 'hadn',\n",
       " 'lalalalalalala',\n",
       " 'lmaolmao',\n",
       " 'sunday',\n",
       " 'searches',\n",
       " 'email',\n",
       " 'introduction',\n",
       " 'lose',\n",
       " 'affirmation',\n",
       " 'pics',\n",
       " 'accounts',\n",
       " 'service',\n",
       " 'presentation',\n",
       " 'girl',\n",
       " 'hihi',\n",
       " 'complete',\n",
       " 'congratulation',\n",
       " 'whenz',\n",
       " 'secs',\n",
       " 'tumblr',\n",
       " 'ate',\n",
       " 'notified',\n",
       " 'app',\n",
       " 'female',\n",
       " 'takes',\n",
       " 'country',\n",
       " 'hey',\n",
       " 'each',\n",
       " 'years',\n",
       " 'course',\n",
       " \"weren't\",\n",
       " 'ahead',\n",
       " 'byeee',\n",
       " 'recovered',\n",
       " 'easier',\n",
       " 'bag',\n",
       " 'profiles',\n",
       " 'due',\n",
       " 'clothes',\n",
       " 'most',\n",
       " 'customer',\n",
       " 'single',\n",
       " 'seems',\n",
       " 'turns',\n",
       " 'wish',\n",
       " 'whee',\n",
       " 'yw',\n",
       " 'ideas',\n",
       " 'very',\n",
       " 'activities',\n",
       " 'exclude',\n",
       " 'notes',\n",
       " 'lights',\n",
       " 'acne',\n",
       " 'votes',\n",
       " 'right',\n",
       " 'tuesday',\n",
       " 'yeahhhhh',\n",
       " 'thanky',\n",
       " 'bright',\n",
       " 'argh',\n",
       " 'yall',\n",
       " 'parents',\n",
       " 'minutes',\n",
       " 'j/k',\n",
       " 'slightly',\n",
       " 'common',\n",
       " 'again',\n",
       " 'pm',\n",
       " 'mentions',\n",
       " 'screened',\n",
       " 'double',\n",
       " 'goodnight',\n",
       " 'indicating',\n",
       " 'juz',\n",
       " 'soft',\n",
       " 'fun',\n",
       " 'conv',\n",
       " 'presentations',\n",
       " 'heart',\n",
       " 'comments',\n",
       " 'kill',\n",
       " 'key',\n",
       " 'source',\n",
       " 'problem',\n",
       " 'detail',\n",
       " 'feed',\n",
       " 'having',\n",
       " 'hahah',\n",
       " 'signed',\n",
       " 'awesome',\n",
       " 'bad',\n",
       " 'show',\n",
       " 'im',\n",
       " 'boxes',\n",
       " 'light',\n",
       " 'dad',\n",
       " 'fail',\n",
       " 'when',\n",
       " 'blog',\n",
       " 'yar',\n",
       " 'reviews',\n",
       " 'about',\n",
       " 'stks',\n",
       " 'var',\n",
       " 'y',\n",
       " 'involve',\n",
       " 'stupids',\n",
       " 'msgs',\n",
       " 'ran',\n",
       " 'install',\n",
       " 'rented',\n",
       " 'pair',\n",
       " 'pissed',\n",
       " 'yeh',\n",
       " 'quick',\n",
       " 'http',\n",
       " 'hohoho',\n",
       " 'who',\n",
       " 'safety',\n",
       " 'year',\n",
       " 'monday',\n",
       " 'gifted',\n",
       " 'hears',\n",
       " 'parent',\n",
       " 'pwned',\n",
       " 'targets',\n",
       " 'candies',\n",
       " 'issues',\n",
       " 'hahahahahahahahaha',\n",
       " 'previous',\n",
       " 'stories',\n",
       " 'they',\n",
       " 'places',\n",
       " 'ltd',\n",
       " 'neato',\n",
       " 'until',\n",
       " 'opened',\n",
       " 'sucked',\n",
       " 'sell',\n",
       " 'dunno',\n",
       " 'fast',\n",
       " 'this',\n",
       " 'ini',\n",
       " 'cuz',\n",
       " 'gigs',\n",
       " \"you'll\",\n",
       " 'customers',\n",
       " 'details',\n",
       " 'attend',\n",
       " 'prices',\n",
       " 'bored',\n",
       " 'affiliate',\n",
       " 'fake',\n",
       " 'asses',\n",
       " 'started',\n",
       " 'resulted',\n",
       " 'hosted',\n",
       " 'pictures',\n",
       " 'tweet',\n",
       " 'consults',\n",
       " 'noooooooooo',\n",
       " 'time',\n",
       " 'allow',\n",
       " 'removed',\n",
       " 'boy',\n",
       " 'returned',\n",
       " 'singapore',\n",
       " 'visits',\n",
       " 'shld',\n",
       " 'canceled',\n",
       " 'thinks',\n",
       " 'wow',\n",
       " 'feeds',\n",
       " 'live',\n",
       " 'yeahhh',\n",
       " 'door',\n",
       " 'females',\n",
       " 'with',\n",
       " 'sues',\n",
       " 'hello',\n",
       " 'wrote',\n",
       " 'under',\n",
       " 'late',\n",
       " 'laaa',\n",
       " 'rename',\n",
       " 'loh',\n",
       " 'sync',\n",
       " 'monthly',\n",
       " 'lfg',\n",
       " 'breakfast',\n",
       " 'stfu',\n",
       " 'cum',\n",
       " 'enter',\n",
       " 'for',\n",
       " 'anycase',\n",
       " 'taking',\n",
       " 'close',\n",
       " 'once',\n",
       " 'adding',\n",
       " 'completing',\n",
       " 'lunch',\n",
       " 'cloth',\n",
       " 'managed',\n",
       " 'costs',\n",
       " 'nite',\n",
       " 'specific',\n",
       " 'start',\n",
       " 'launches',\n",
       " 'sides',\n",
       " 'life',\n",
       " 'those',\n",
       " 'area',\n",
       " 'continues',\n",
       " 'hahahahahahaha',\n",
       " 'provides',\n",
       " 'into',\n",
       " 'companies',\n",
       " 'bloody',\n",
       " 'lololol',\n",
       " 'met',\n",
       " 'spends',\n",
       " 'found',\n",
       " 'kinda',\n",
       " 'closed',\n",
       " 'lives',\n",
       " 'me',\n",
       " 'roles',\n",
       " 'coz',\n",
       " 'throats',\n",
       " 'brought',\n",
       " 'think',\n",
       " 'whats',\n",
       " 'doubled',\n",
       " 'stopped',\n",
       " 'means',\n",
       " 'condo',\n",
       " 'min',\n",
       " 'rss',\n",
       " 'pay',\n",
       " 'mustn',\n",
       " 'geez',\n",
       " 'favorite',\n",
       " 'packs',\n",
       " 't',\n",
       " 'worse',\n",
       " 'i',\n",
       " 'wooohoooo',\n",
       " 'sung',\n",
       " 'merry',\n",
       " 'asks',\n",
       " 'friday',\n",
       " 'earns',\n",
       " 'approves',\n",
       " 'stays',\n",
       " 'types',\n",
       " 'wa',\n",
       " 'haah',\n",
       " 'replace',\n",
       " 'minute',\n",
       " 'visited',\n",
       " 'tm',\n",
       " 'during',\n",
       " \"won't\",\n",
       " 'tht',\n",
       " 'shouldnt',\n",
       " 'line',\n",
       " 'report',\n",
       " 'yeahhhh',\n",
       " 'related',\n",
       " 'photo',\n",
       " 'page',\n",
       " 'stops',\n",
       " 'wondered',\n",
       " 'abt',\n",
       " 'website',\n",
       " 'season',\n",
       " 'womens',\n",
       " 'mad',\n",
       " 'fwah',\n",
       " 'vn',\n",
       " 'interacted',\n",
       " 'search',\n",
       " 'yahoocurrency',\n",
       " 'lolol',\n",
       " 'watnot',\n",
       " 'free',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'ask',\n",
       " 'hates',\n",
       " 'flights',\n",
       " 'followed',\n",
       " 'posted',\n",
       " 'baby',\n",
       " 'plzz',\n",
       " 'ya',\n",
       " 'admission',\n",
       " 'totals',\n",
       " 'wrong',\n",
       " 'irl',\n",
       " 'sent',\n",
       " 'him',\n",
       " 'its',\n",
       " 'hoo',\n",
       " 'doesn',\n",
       " 'thks',\n",
       " 'checked',\n",
       " 'pte',\n",
       " 'annoy',\n",
       " 'eng',\n",
       " 'home',\n",
       " 'leavg',\n",
       " 'screens',\n",
       " 'finished',\n",
       " 'main',\n",
       " 'ly',\n",
       " 'locates',\n",
       " 'cookies',\n",
       " 'mine',\n",
       " 'supporting',\n",
       " 'info',\n",
       " 'lanuch',\n",
       " 'guess',\n",
       " 'highest',\n",
       " 'eats',\n",
       " 'win',\n",
       " 'holiday',\n",
       " 'ftw',\n",
       " 'closes',\n",
       " 'yr',\n",
       " 'sue',\n",
       " '#200aday',\n",
       " 'hours',\n",
       " 'great',\n",
       " 'mixes',\n",
       " 'changed',\n",
       " 'idk',\n",
       " 'rated',\n",
       " 'interact',\n",
       " 'opportunity',\n",
       " 'https',\n",
       " 'twittering',\n",
       " 'passed',\n",
       " 'ain',\n",
       " 'emos',\n",
       " 'spots',\n",
       " 'yup',\n",
       " 'pls',\n",
       " 'enable',\n",
       " 'cluttered',\n",
       " 'funny',\n",
       " 'old',\n",
       " 'served',\n",
       " 'jobs',\n",
       " 'liked',\n",
       " 'tree',\n",
       " 'tel',\n",
       " 'ahahahahah',\n",
       " 'car',\n",
       " 'lor',\n",
       " 'fucks',\n",
       " 'shudder',\n",
       " 'book',\n",
       " 'give',\n",
       " \"doesn't\",\n",
       " 'contacted',\n",
       " 'screen',\n",
       " 'flat',\n",
       " 'opportunities',\n",
       " 'himself',\n",
       " 'rooms',\n",
       " 'cove',\n",
       " 'ordered',\n",
       " 'purchase',\n",
       " 'favourite',\n",
       " 'tbl',\n",
       " 'result',\n",
       " 'enters',\n",
       " 'readers',\n",
       " \"couldn't\",\n",
       " 'were',\n",
       " 'boys',\n",
       " 'create',\n",
       " 'shows',\n",
       " 'products',\n",
       " 'zz',\n",
       " 'treats',\n",
       " 'over',\n",
       " 'daily',\n",
       " 'imo',\n",
       " 'renamed',\n",
       " 'retweets',\n",
       " 'articles',\n",
       " 'seasons',\n",
       " 'hurt',\n",
       " 'pf',\n",
       " 'slept',\n",
       " 'emo',\n",
       " 'excuses',\n",
       " 'killed',\n",
       " 'voted',\n",
       " 'being',\n",
       " 'enabling',\n",
       " 'reminded',\n",
       " 'such',\n",
       " 'contents',\n",
       " 'huh',\n",
       " 'take',\n",
       " 'upgraded',\n",
       " 'ni',\n",
       " 'tym',\n",
       " 'w00t',\n",
       " 'presented',\n",
       " 'remembers',\n",
       " 'watch',\n",
       " 'realize',\n",
       " 'dinners',\n",
       " 'movie',\n",
       " 'month',\n",
       " 'bai',\n",
       " 'occur',\n",
       " 'gtg',\n",
       " 'flight',\n",
       " 'fadein',\n",
       " 'tweeting',\n",
       " 'blogging',\n",
       " 'profile',\n",
       " 'heard',\n",
       " '#followback',\n",
       " 'special',\n",
       " 'hihihi',\n",
       " 'calls',\n",
       " 'diff',\n",
       " 'indicates',\n",
       " '#ifollowback',\n",
       " 'should',\n",
       " 'selected',\n",
       " 'version',\n",
       " 'pretty',\n",
       " 'locked',\n",
       " 'classes',\n",
       " 'sleep',\n",
       " 'looks',\n",
       " 'comeback',\n",
       " 'zzzzz',\n",
       " 'seemed',\n",
       " 'docs',\n",
       " 'kills',\n",
       " 'resorts',\n",
       " 'laughs',\n",
       " 'consecutive',\n",
       " 'students',\n",
       " 'choices',\n",
       " 'notify',\n",
       " 'soup',\n",
       " 'women',\n",
       " 'pick',\n",
       " 'de',\n",
       " 'gonna',\n",
       " 'throat',\n",
       " 'picks',\n",
       " 'sang',\n",
       " 'afternoon',\n",
       " 'lunches',\n",
       " 'had',\n",
       " 'umm',\n",
       " 'learns',\n",
       " 'watching',\n",
       " 'vote',\n",
       " 'kyou',\n",
       " 'allowed',\n",
       " 'hand',\n",
       " 'hated',\n",
       " 'searched',\n",
       " 'areas',\n",
       " 'present',\n",
       " 'mean',\n",
       " 'whatevar',\n",
       " 'd',\n",
       " 'the',\n",
       " 'doing',\n",
       " \"wasn't\",\n",
       " 'while',\n",
       " 'refuse',\n",
       " 'biz',\n",
       " 'jus',\n",
       " 'interactions',\n",
       " 'obviously',\n",
       " 'idiots',\n",
       " 'requires',\n",
       " 'haha',\n",
       " 'yum',\n",
       " 'movies',\n",
       " 'rents',\n",
       " 'damn',\n",
       " 'plz',\n",
       " 'forming',\n",
       " 'eat',\n",
       " 'lightly',\n",
       " 'messages',\n",
       " 'world',\n",
       " 'by',\n",
       " \"isn't\",\n",
       " 'off',\n",
       " 'tomolow',\n",
       " 'learn',\n",
       " 'fucking',\n",
       " 'false',\n",
       " 'shares',\n",
       " 'dies',\n",
       " 'couldn',\n",
       " 'wham',\n",
       " 'dreamt',\n",
       " 'son',\n",
       " 'wasn',\n",
       " 'furnished',\n",
       " 'accented',\n",
       " '#followfriday',\n",
       " 'site',\n",
       " 'below',\n",
       " 'tyty',\n",
       " 'fall',\n",
       " 'yay',\n",
       " 'through',\n",
       " 'yumyum',\n",
       " 'follows',\n",
       " 'hairs',\n",
       " 'few',\n",
       " 'usd',\n",
       " 'check',\n",
       " 'spending',\n",
       " 'banks',\n",
       " 'families',\n",
       " 'interacts',\n",
       " 'distracted',\n",
       " 'huhuhu',\n",
       " 'project',\n",
       " 'depend',\n",
       " 'link',\n",
       " 'chicken',\n",
       " 'replied',\n",
       " 'issued',\n",
       " 'hehe',\n",
       " 'above',\n",
       " 'information',\n",
       " 'wat',\n",
       " 'cancel',\n",
       " 'love',\n",
       " 'nor',\n",
       " 'lender',\n",
       " 'wtt',\n",
       " 'adds',\n",
       " 'mini',\n",
       " 'posts',\n",
       " 'july',\n",
       " 'hands',\n",
       " 'she',\n",
       " 'important',\n",
       " 'regular',\n",
       " 'clutter',\n",
       " 'presents',\n",
       " 'web',\n",
       " 'happened',\n",
       " 'wanna',\n",
       " 'message',\n",
       " 'hmm',\n",
       " 'kkkk',\n",
       " 'request',\n",
       " 'noodles',\n",
       " 'photos',\n",
       " 'share',\n",
       " 'wrt',\n",
       " 'dark',\n",
       " 'tsk',\n",
       " 'prefer',\n",
       " 'pre',\n",
       " 'lah',\n",
       " 'phones',\n",
       " 'music',\n",
       " 'zomg',\n",
       " 'phew',\n",
       " 'scratch',\n",
       " 'ish',\n",
       " 'parks',\n",
       " 'girls',\n",
       " 'whatcha',\n",
       " 'wb',\n",
       " 'location',\n",
       " 'receive',\n",
       " 'be',\n",
       " 'listed',\n",
       " 'attended',\n",
       " 'chocolates',\n",
       " 'bye',\n",
       " 'can',\n",
       " 'activity',\n",
       " 'yesterday',\n",
       " 'themselves',\n",
       " 'returns',\n",
       " 'teehee',\n",
       " 'youre',\n",
       " 'watched',\n",
       " 'didnt',\n",
       " 'contact',\n",
       " 'va',\n",
       " 'faster',\n",
       " 'lawl',\n",
       " 'pro',\n",
       " 'creats',\n",
       " 'havent',\n",
       " 'rofl',\n",
       " 'did',\n",
       " 'curr',\n",
       " 'follow',\n",
       " 'fries',\n",
       " 'lots',\n",
       " 'whore',\n",
       " \"mightn't\",\n",
       " 'bday',\n",
       " 'involves',\n",
       " 'sounds',\n",
       " 'byebye',\n",
       " 'bags',\n",
       " 'apparently',\n",
       " 'tart',\n",
       " 'between',\n",
       " 'was',\n",
       " 'blogger',\n",
       " 'further',\n",
       " 'thing',\n",
       " 'learned',\n",
       " 'shown',\n",
       " 'net',\n",
       " 'isnt',\n",
       " 'fuck',\n",
       " 'arrives',\n",
       " 'harder',\n",
       " 'waiting',\n",
       " 'doesnt',\n",
       " 'class',\n",
       " 'installations',\n",
       " 'loss',\n",
       " 'reminds',\n",
       " 'justwit',\n",
       " 'lenders',\n",
       " 'mix',\n",
       " 'phone',\n",
       " 'orders',\n",
       " 'job',\n",
       " 'find',\n",
       " 'kthxbai',\n",
       " 'completes',\n",
       " 'an',\n",
       " 'careers',\n",
       " 'shoe',\n",
       " 'congrats',\n",
       " 'html',\n",
       " 'woah',\n",
       " 'food',\n",
       " 'eh',\n",
       " 'sells',\n",
       " 'ohgod',\n",
       " 'aft',\n",
       " 'updated',\n",
       " 'review',\n",
       " 'chickens',\n",
       " 'reported',\n",
       " 'completed',\n",
       " 'teach',\n",
       " 'humm',\n",
       " 'packed',\n",
       " 'stayed',\n",
       " 'album',\n",
       " 'media',\n",
       " 'imho',\n",
       " 'set',\n",
       " 'wondering',\n",
       " 'hahahahha',\n",
       " 'sq',\n",
       " 'talk',\n",
       " 'belong',\n",
       " 'then',\n",
       " \"hadn't\",\n",
       " 'realised',\n",
       " 'mushroom',\n",
       " 'ticket',\n",
       " 'from',\n",
       " 'whatever',\n",
       " 'congratulations',\n",
       " 'worlds',\n",
       " 'relate',\n",
       " 'myself',\n",
       " 'screws',\n",
       " 'october',\n",
       " 'production',\n",
       " 'earned',\n",
       " 'earn',\n",
       " 'hers',\n",
       " 'idiot',\n",
       " 'esp',\n",
       " 'full',\n",
       " '2moro',\n",
       " 'pains',\n",
       " 'dun',\n",
       " 'reviewed',\n",
       " 'hear',\n",
       " 'productions',\n",
       " 'launched',\n",
       " 'asked',\n",
       " 'sing',\n",
       " 'butter',\n",
       " 'supported',\n",
       " 'career',\n",
       " 'paid',\n",
       " 'pigs',\n",
       " 'males',\n",
       " 'friend',\n",
       " 'announcements',\n",
       " 'problems',\n",
       " 'previews',\n",
       " 'sale',\n",
       " 'requested',\n",
       " 'finds',\n",
       " 'miss',\n",
       " 'works',\n",
       " 'suckz',\n",
       " 'srsly',\n",
       " 'hope',\n",
       " '.org',\n",
       " '#teamfollowback',\n",
       " 'white',\n",
       " 'crowded',\n",
       " 'laa',\n",
       " \"shan't\",\n",
       " 'a',\n",
       " 'tat',\n",
       " 'wth',\n",
       " 'november',\n",
       " 'work',\n",
       " 'brings',\n",
       " 'wished',\n",
       " 'versions',\n",
       " 'tweets',\n",
       " 'ehhh',\n",
       " 'doc',\n",
       " 'whores',\n",
       " 'mom',\n",
       " 'veh',\n",
       " 've',\n",
       " 'side',\n",
       " 'cost',\n",
       " 'famous',\n",
       " 'whatz',\n",
       " 'opens',\n",
       " '#autofollow',\n",
       " 'lined',\n",
       " 'changes',\n",
       " 'noodle',\n",
       " 'theirs',\n",
       " 'ad',\n",
       " 'wonder',\n",
       " 'locks',\n",
       " 'huhu',\n",
       " 'broke',\n",
       " 'like',\n",
       " 'received',\n",
       " '.net',\n",
       " 'sources',\n",
       " 'studies',\n",
       " 'doors',\n",
       " 'million',\n",
       " 'sqm',\n",
       " 'cuties',\n",
       " 'contacts',\n",
       " 'sold',\n",
       " 'sites',\n",
       " 'candy',\n",
       " 'change',\n",
       " 'yeahh',\n",
       " 'took',\n",
       " 'emails',\n",
       " '#followgain',\n",
       " 'reply',\n",
       " 'realizes',\n",
       " 'suck',\n",
       " 'not',\n",
       " 'caught',\n",
       " 'tips',\n",
       " 'trees',\n",
       " 'my',\n",
       " 'tgif',\n",
       " 'previewed',\n",
       " 'target',\n",
       " \"haven't\",\n",
       " 'realises',\n",
       " 'o',\n",
       " 'buy',\n",
       " 'breakfasts',\n",
       " 'on',\n",
       " 'listened',\n",
       " 'at',\n",
       " 'affiliates',\n",
       " \"you're\",\n",
       " 'drs',\n",
       " 'you',\n",
       " 'flats',\n",
       " 'ppl',\n",
       " 'offices',\n",
       " 'shit',\n",
       " 'bleh',\n",
       " 'settled',\n",
       " 'gotta',\n",
       " 'rate',\n",
       " 'road',\n",
       " 'manages',\n",
       " 'office',\n",
       " 'happen',\n",
       " 'bcoz',\n",
       " 'shoes',\n",
       " 'wts',\n",
       " 'article',\n",
       " 'unit',\n",
       " 'formed',\n",
       " 'powerful',\n",
       " 'listening',\n",
       " 'will',\n",
       " 'deposit',\n",
       " 'own',\n",
       " 'later',\n",
       " 'sg',\n",
       " 'wateva',\n",
       " 'product',\n",
       " 'document',\n",
       " 'slipped',\n",
       " 'thursday',\n",
       " 'autoindustry',\n",
       " 'exploded',\n",
       " '#500aday',\n",
       " 'm',\n",
       " 'wins',\n",
       " 'hahahahahah',\n",
       " 'other',\n",
       " 'allows',\n",
       " 're',\n",
       " '#follow',\n",
       " 'story',\n",
       " 'painful',\n",
       " 'nw',\n",
       " '#followngain',\n",
       " 'anymore',\n",
       " 'wooohooo',\n",
       " 'good',\n",
       " 'notifies',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/stopword.txt\", 'r') as f_stopword:\n",
    "    list_stopword = []\n",
    "    for line in f_stopword.readlines():\n",
    "        list_stopword.append(line.strip())\n",
    "    \n",
    "list_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "      <th>remove_punctuation</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>remove_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[snapchat, must, dehydrate, thats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], as, we, see, trump, is, dangerous, to,...</td>\n",
       "      <td>[[tag], a, we, see, trump, be, dangerous, to, ...</td>\n",
       "      <td>[[tag], see, trump, dangerous, freepress, arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, 😂😂😂, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[now, issa, is, stalking, tasha, joy]</td>\n",
       "      <td>[now, issa, be, stalk, tasha, joy]</td>\n",
       "      <td>[issa, stalk, tasha, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[tag], [tag], thx, best, heartbreakingly, aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus]</td>\n",
       "      <td>[still, wait, on, those, supply, liscus]</td>\n",
       "      <td>[still, supply, liscus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, 😂😂😂, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                  remove_punctuation  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], as, we, see, trump, is, dangerous, to,...   \n",
       "3              [now, issa, is, stalking, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6      [still, waiting, on, those, supplies, liscus]   \n",
       "\n",
       "                                       lemmatization  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], a, we, see, trump, be, dangerous, to, ...   \n",
       "3                 [now, issa, be, stalk, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6           [still, wait, on, those, supply, liscus]   \n",
       "\n",
       "                                     remove_stopword  \n",
       "0                 [snapchat, must, dehydrate, thats]  \n",
       "1  [[tag], see, trump, dangerous, freepress, arou...  \n",
       "3                          [issa, stalk, tasha, joy]  \n",
       "5  [[tag], [tag], thx, best, heartbreakingly, aut...  \n",
       "6                            [still, supply, liscus]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"remove_stopword\"] = df_input.lemmatization.apply(\n",
    "    lambda list_word: \n",
    "    [word for word in list_word if word not in list_stopword]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Pickle after Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.to_pickle(\"data/train_df_han_preprocess.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.unicode_codes import EMOJI_UNICODE, UNICODE_EMOJI\n",
    "\n",
    "list_cal_emoji = df_input.replace_tag.apply(\n",
    "    lambda list_word: \n",
    "    [char for char in \" \".join(list_word) if char in UNICODE_EMOJI]\n",
    ")\n",
    "\n",
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']\n",
    "dict_emoji_all_emotion = defaultdict(list)\n",
    "dict_emoji_emotion = {}\n",
    "seties_emotion = df_input.emotion\n",
    "for idx, list_emoji in enumerate(list_cal_emoji): \n",
    "    list_emoji = list(set(list_emoji))  \n",
    "    for emoji in list_emoji:\n",
    "        if not dict_emoji_all_emotion[emoji]: dict_emoji_all_emotion[emoji]=[0]*len(emotion_array)\n",
    "        dict_emoji_all_emotion[emoji][emotion_array.index(seties_emotion.iloc[idx])] += 1\n",
    "\n",
    "for emoji, list_emoji_num in dict_emoji_all_emotion.items():\n",
    "    dict_emoji_emotion[emoji] = emotion_array[np.argmax(list_emoji_num)]\n",
    "\n",
    "dict_emoji_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_abbreviation = {}\n",
    "with open(\"data/abbreviations.txt\", 'r') as f_abbreviation:\n",
    "    for idx, line in enumerate(f_abbreviation.readlines()):\n",
    "        abbreviation, sentence = line.strip().split('=')\n",
    "        dict_abbreviation[abbreviation] = sentence.split()\n",
    "dict_abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(list_word):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(list_word):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stopword.txt\", 'r') as f_stopword:\n",
    "    list_stopword = []\n",
    "    for line in f_stopword.readlines():\n",
    "        list_stopword.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(name, df, dict_emoji_emotion, dict_abbreviation, list_stopword):\n",
    "    df[\"preprocessing\"] = df[\"text\"].str.split()\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [\"[tag]\" if word[0]=='@' else word for word in list_word]\n",
    "    )\n",
    "    \n",
    "    list_replaced_emoji = []\n",
    "    for list_word in df.preprocessing:\n",
    "        sentence_before = \" \".join(list_word)\n",
    "        sentence_after = \"\"\n",
    "        for idx, char in enumerate(sentence_before):\n",
    "    #         if idx: \n",
    "    #             print(char, sentence_before[idx-1], (char == sentence_before[idx-1]))\n",
    "            if char in UNICODE_EMOJI :\n",
    "                emotion = dict_emoji_emotion.get(char)\n",
    "                if not emotion: continue\n",
    "                if not idx: sentence_after += emotion\n",
    "                elif char != sentence_before[idx-1]: \n",
    "                    sentence_after += emotion\n",
    "                else: continue\n",
    "            else: sentence_after += char\n",
    "        list_replaced_emoji.append(sentence_after.split())\n",
    "    df[\"preprocessing\"] = np.array(list_replaced_emoji)\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [word for list_word_after in [dict_abbreviation[word] if word in dict_abbreviation else [word] for word in list_word] for word in list_word_after]\n",
    "    )\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [word for word in [re.sub('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'), '', word.replace(\"<LH>\", '').lower()) if word!=\"[tag]\" else word for word in list_word ] if word]\n",
    "    )\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        lemmatize_sentence(list_word)\n",
    "    )\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [word for word in list_word if word not in list_stopword]\n",
    "    )\n",
    "\n",
    "    df.to_pickle(\"data/{}.pkl\".format(name)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(\"test_df_han_preprocess\", test_df, dict_emoji_emotion, dict_abbreviation, list_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load a pickle file\n",
    "trains = pd.read_pickle(\"data/train_df_han_preprocess.pkl\")\n",
    "tests = pd.read_pickle(\"data/test_df_han_preprocess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re, csv\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors   \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "# TF-IDF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    use_idf=True, \n",
    "    tokenizer=tknzr.tokenize,\n",
    "    stop_words=stopwords.words('english'))\n",
    "\n",
    "tfidf_vectorizer.fit(trains['remove_stopword'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Data to Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = int(trains.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "BOW_train = trains.remove_stopword[:sep]\n",
    "BOW_train_label = trains.emotion[:sep]\n",
    "BOW_val = trains.remove_stopword[sep:]\n",
    "BOW_val_label = trains.emotion[sep:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.transform(BOW_train)\n",
    "y_train = BOW_train_label\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(BOW_val)\n",
    "y_val = BOW_val_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Emoji List for Encode / Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = emotion_to_sequence(y_train)\n",
    "y_val = emotion_to_sequence(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW Model Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE1_DIM = 256\n",
    "DENSE2_DIM = 256\n",
    "DENSE3_DIM = 64\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 40\n",
    "DROPOUT_DENSE = 0.2\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "TIME_NOW = datetime.now().isoformat().split('.')[0]\n",
    "\n",
    "#############################################################################################\n",
    "MODEL_DESCRIPTION = \"{}_{}\".format(\"BOW\", \"ThreeHidden\")\n",
    "#############################################################################################\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    'logs/{}_{}.csv'.format(MODEL_DESCRIPTION, TIME_NOW))\n",
    "\n",
    "SUMMARY_REPORT_PATH = 'summary/{}_{}.txt'.format(MODEL_DESCRIPTION, TIME_NOW)\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, texts_val, labels_val):\n",
    "    #######################################\n",
    "    # build up the model \n",
    "    # input: texts (np), labels (np)\n",
    "    # output: callback\n",
    "    #######################################\n",
    "    \n",
    "    shuffle_indices = np.random.permutation(texts.shape[0],)\n",
    "    texts = texts[shuffle_indices]\n",
    "    labels = labels[shuffle_indices]\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    model.add(Dense(DENSE3_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    \n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.summary()\n",
    "    with open(SUMMARY_REPORT_PATH, mode='w') as file_summary:\n",
    "        model.summary(print_fn=lambda line: file_summary.write(line+'\\n'))\n",
    "        \n",
    "    history_callback = model.fit(\n",
    "        texts_train, labels_train, \n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    model.save('model/{}_{}.h5'.format(MODEL_DESCRIPTION,TIME_NOW))\n",
    "    \n",
    "    return history_callback\n",
    "\n",
    "history_callback = model_setting(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package for Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors   \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Embedding, Dense, Bidirectional, LSTM, GRU, Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "def text_to_sequence(list_list_word):\n",
    "    tokenized_texts = []\n",
    "    for list_word in list_list_word:\n",
    "        tokenized_text = []\n",
    "        for idx, word in enumerate(list_word):\n",
    "            \n",
    "            # truncate\n",
    "            if idx >= MAX_SEQUENCE_LENGTH: break \n",
    "                \n",
    "            try:\n",
    "                token = token_dict[word.lower()]\n",
    "            except:\n",
    "                token = MAX_WORD\n",
    "                \n",
    "            tokenized_text.append(token)\n",
    "            \n",
    "        # padding \n",
    "        if len(tokenized_text) < MAX_SEQUENCE_LENGTH: \n",
    "            tokenized_text.extend([MAX_WORD]*(MAX_SEQUENCE_LENGTH-len(tokenized_text)))\n",
    "                \n",
    "        tokenized_texts.append(tokenized_text)\n",
    "    \n",
    "    return np.array(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "MAX_WORD = 15000\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_WORD, \n",
    "    use_idf=True, \n",
    "    )\n",
    "tfidf_vectorizer.fit(trains['remove_stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "for idx, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    token_dict[token] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model and Setup Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\n",
    "    \"data/model_ENC3.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "def word2vec_matrix():\n",
    "    matrix_emb = np.zeros((len(token_dict)+1, EMBEDDING_DIM)) # replace MAX_WORD to len(token_dict)\n",
    "    for word, idx in token_dict.items():\n",
    "        try:\n",
    "            vector =  word2vec_model.wv[word]\n",
    "        except:\n",
    "            vector = np.zeros(300,)\n",
    "        if idx < MAX_WORD:\n",
    "            matrix_emb[idx] = np.array(vector)\n",
    "    matrix_emb[MAX_WORD] = np.zeros(300,)     \n",
    "    return matrix_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Emoji List for Encode / Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = text_to_sequence(trains['remove_stopword'])\n",
    "train_label = emotion_to_sequence(trains['emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM1_DIM = 256\n",
    "DENSE1_DIM = 256\n",
    "DENSE2_DIM = 256\n",
    "DENSE3_DIM = 64\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 7\n",
    "DROPOUT_LSTM = 0.2\n",
    "DROPOUT_DENSE = 0.2\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "TIME_NOW = datetime.now().isoformat().split('.')[0]\n",
    "\n",
    "#############################################################################################\n",
    "MODEL_DESCRIPTION = \"{}_{}_{}\".format(\"BiGRU_TwoHidden\", str(LSTM1_DIM), str(DROPOUT_LSTM))\n",
    "#############################################################################################\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    'logs/{}_{}.csv'.format(MODEL_DESCRIPTION, TIME_NOW))\n",
    "\n",
    "SUMMARY_REPORT_PATH = 'summary/{}_{}.txt'.format(MODEL_DESCRIPTION, TIME_NOW)\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, matrix_emb=\"uniform\"):\n",
    "    #######################################\n",
    "    # build up the model \n",
    "    # input: texts (np), labels (np), embedding matrix (np)\n",
    "    # output: callback\n",
    "    #######################################\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n",
    "        \n",
    "    shuffle_indices = np.random.permutation(texts.shape[0],)\n",
    "    texts = texts[shuffle_indices]\n",
    "    labels = labels[shuffle_indices]\n",
    "        \n",
    "    #  split data to train & validation\n",
    "    validation_size = int(texts.shape[0]*VALIDATION_SPLIT)\n",
    "    texts_train = texts[:-validation_size]\n",
    "    texts_val = texts[-validation_size:]\n",
    "    labels_train = labels[:-validation_size]\n",
    "    labels_val = labels[-validation_size:]\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORD+1, EMBEDDING_DIM, embeddings_initializer=matrix_emb, input_length=MAX_SEQUENCE_LENGTH, trainable=True)) # with embedding matrix\n",
    "    model.add(GRU(LSTM1_DIM, dropout=DROPOUT_LSTM, recurrent_dropout=DROPOUT_LSTM))\n",
    "\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    \n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.summary()\n",
    "    with open(SUMMARY_REPORT_PATH, mode='w') as file_summary:\n",
    "        model.summary(print_fn=lambda line: file_summary.write(line+'\\n'))\n",
    "        \n",
    "    history_callback = model.fit(\n",
    "        texts_train, labels_train, \n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    model.save('model/{}_{}.h5'.format(MODEL_DESCRIPTION,TIME_NOW))\n",
    "    \n",
    "    return history_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback = model_setting(train_sequence, train_label, word2vec_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss and Accuracy of Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "df_his_acc = pd.DataFrame({'train_acc':history_callback.history['acc'], 'validation_acc':history_callback.history['val_acc']})\n",
    "show_train_history(history_callback, 'acc', 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "df_his_acc = pd.DataFrame({'train_loss':history_callback.history['loss'], 'validation_loss':history_callback.history['val_loss']})\n",
    "show_train_history(history_callback, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "tests = pd.read_pickle(\"data/train_df_han_preprocess.pkl\")\n",
    "X_test = text_to_sequence(tests['text'])\n",
    "model = load_model(\"model/{}_{}.h5\".format(MODEL_DESCRIPTION, TIME_NOW))\n",
    "\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result = np.array(\n",
    "    [emotion_array[pred_result_idx] for pred_result_idx in np.argmax(pred_result, axis=1)])\n",
    "\n",
    "tests['emotion'] = pred_result\n",
    "tests = tests.drop('hashtags', axis=1)\n",
    "tests = tests.drop('text', axis=1)\n",
    "tests = tests.drop('identification', axis=1)\n",
    "tests.rename(columns={'tweet_id':'id'}, inplace=True)\n",
    "tests.to_csv('predict_result/prediction_{}_{}.csv'.format(MODEL_DESCRIPTION, TIME_NOW), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "datamining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
