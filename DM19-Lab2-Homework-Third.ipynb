{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: è”¡ç§‰ç¿°\n",
    "\n",
    "Student ID: 107064527\n",
    "\n",
    "GitHub ID: k52276hank\n",
    "\n",
    "Kaggle name: Pin-Han, Tsai\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re, csv\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Raw Data to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Identification Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv and restructure dataframe (rename)\n",
    "df_iden = pd.read_csv(\"data/data_identification.csv\", sep=\"\\t\")\n",
    "df_iden['tweet_id'] = df_iden['tweet_id,identification'].str.split(',', expand=True)[0]\n",
    "df_iden['identification'] = df_iden['tweet_id,identification'].str.split(',', expand=True)[1]\n",
    "df_iden = df_iden.drop('tweet_id,identification', axis=1)\n",
    "df_iden.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Emotion Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion = pd.read_csv(\"data/emotion.csv\", sep=\"\\t\")\n",
    "df_emotion['tweet_id'] = df_emotion['tweet_id,emotion'].str.split(',', expand=True)[0]\n",
    "df_emotion['emotion'] = df_emotion['tweet_id,emotion'].str.split(',', expand=True)[1]\n",
    "df_emotion = df_emotion.drop('tweet_id,emotion', axis=1)\n",
    "df_emotion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Duplicated of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          False\n",
       "1          False\n",
       "2          False\n",
       "3          False\n",
       "4          False\n",
       "5          False\n",
       "6          False\n",
       "7          False\n",
       "8          False\n",
       "9          False\n",
       "10         False\n",
       "11         False\n",
       "12         False\n",
       "13         False\n",
       "14         False\n",
       "15         False\n",
       "16         False\n",
       "17         False\n",
       "18         False\n",
       "19         False\n",
       "20         False\n",
       "21         False\n",
       "22         False\n",
       "23         False\n",
       "24         False\n",
       "25         False\n",
       "26         False\n",
       "27         False\n",
       "28         False\n",
       "29         False\n",
       "           ...  \n",
       "1867505    False\n",
       "1867506    False\n",
       "1867507    False\n",
       "1867508    False\n",
       "1867509    False\n",
       "1867510    False\n",
       "1867511    False\n",
       "1867512    False\n",
       "1867513    False\n",
       "1867514    False\n",
       "1867515    False\n",
       "1867516    False\n",
       "1867517    False\n",
       "1867518    False\n",
       "1867519    False\n",
       "1867520    False\n",
       "1867521    False\n",
       "1867522    False\n",
       "1867523    False\n",
       "1867524    False\n",
       "1867525    False\n",
       "1867526    False\n",
       "1867527    False\n",
       "1867528    False\n",
       "1867529    False\n",
       "1867530    False\n",
       "1867531    False\n",
       "1867532    False\n",
       "1867533    False\n",
       "1867534    False\n",
       "Length: 1867535, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if duplicated\n",
    "df_emotion.duplicated(keep='first')\n",
    "df_iden.duplicated(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read All Raw Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_index</th>\n",
       "      <th>_score</th>\n",
       "      <th>_source</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>391</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'text': '...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>433</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>232</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'text':...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>376</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'text': 'Now ISSA i...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>989</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'text': '\"Trust is ...</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _crawldate          _index  _score  \\\n",
       "0  2015-05-23 11:42:47  hashtag_tweets     391   \n",
       "1  2016-01-28 04:52:09  hashtag_tweets     433   \n",
       "2  2017-12-25 04:39:20  hashtag_tweets     232   \n",
       "3  2016-01-24 23:53:05  hashtag_tweets     376   \n",
       "4  2016-01-08 17:18:59  hashtag_tweets     989   \n",
       "\n",
       "                                             _source   _type  \n",
       "0  {'tweet': {'hashtags': ['Snapchat'], 'text': '...  tweets  \n",
       "1  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...  tweets  \n",
       "2  {'tweet': {'hashtags': ['bibleverse'], 'text':...  tweets  \n",
       "3  {'tweet': {'hashtags': [], 'text': 'Now ISSA i...  tweets  \n",
       "4  {'tweet': {'hashtags': [], 'text': '\"Trust is ...  tweets  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load raw data of twitter\n",
    "total_datas = open('data/tweets_DM.json').read()\n",
    "tweet_DM = pd.read_json(total_datas, lines = True)\n",
    "tweet_DM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "2                   [bibleverse]   \n",
       "3                             []   \n",
       "4                             []   \n",
       "\n",
       "                                                text  tweet_id  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350  \n",
       "2  Confident of your obedience, I write to you, k...  0x28b412  \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0  \n",
       "4  \"Trust is not the same as faith. A friend is s...  0x2de201  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some useful information from all data\n",
    "df_total = json_normalize(tweet_DM['_source'])\n",
    "df_total.rename(columns={'tweet.tweet_id':'tweet_id', \n",
    "                 'tweet.text':'text', \n",
    "                 'tweet.hashtags': 'hashtags'}, inplace=True)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Total Tweet Data and Emotion and Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "2                   [bibleverse]   \n",
       "3                             []   \n",
       "4                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "2  Confident of your obedience, I write to you, k...  0x28b412           NaN   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "4  \"Trust is not the same as faith. A friend is s...  0x2de201           NaN   \n",
       "\n",
       "  identification  \n",
       "0          train  \n",
       "1          train  \n",
       "2           test  \n",
       "3          train  \n",
       "4           test  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data sets\n",
    "df_total_merge = pd.merge(df_total, df_emotion, how='outer', on=['tweet_id'])\n",
    "df_total_merge = pd.merge(df_total_merge, df_iden, how='outer', on=['tweet_id'])\n",
    "df_total_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate Train DataFrame and Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate training and testing data\n",
    "train_df = df_total_merge[df_total_merge[\"identification\"]==\"train\"]\n",
    "test_df = df_total_merge[df_total_merge[\"identification\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification  \n",
       "0          train  \n",
       "1          train  \n",
       "3          train  \n",
       "5          train  \n",
       "6          train  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[]</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             hashtags  \\\n",
       "2                        [bibleverse]   \n",
       "4                                  []   \n",
       "9   [materialism, money, possessions]   \n",
       "30               [GodsPlan, GodsWork]   \n",
       "33                                 []   \n",
       "\n",
       "                                                 text  tweet_id emotion  \\\n",
       "2   Confident of your obedience, I write to you, k...  0x28b412     NaN   \n",
       "4   \"Trust is not the same as faith. A friend is s...  0x2de201     NaN   \n",
       "9   When do you have enough ? When are you satisfi...  0x218443     NaN   \n",
       "30  God woke you up, now chase the day #GodsPlan #...  0x2939d5     NaN   \n",
       "33  In these tough times, who do YOU turn to as yo...  0x26289a     NaN   \n",
       "\n",
       "   identification  \n",
       "2            test  \n",
       "4            test  \n",
       "9            test  \n",
       "30           test  \n",
       "33           test  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Pickle before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "train_df.to_pickle(\"data/train_df_nonpreprocess.pkl\") \n",
    "test_df.to_pickle(\"data/test_df_nonpreprocess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pickle which is Non-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pickle file\n",
    "train_df = pd.read_pickle(\"data/train_df_nonpreprocess.pkl\")\n",
    "test_df = pd.read_pickle(\"data/test_df_nonpreprocess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize at first without filter anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...  \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]  \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...  \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input = train_df\n",
    "df_input[\"split_to_word\"] = df_input[\"text\"].str.split()\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Replace @tag to [tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...  \n",
       "3        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]  \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...  \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"replace_tag\"] = df_input.split_to_word.apply(\n",
    "    lambda list_word: \n",
    "    [\"[tag]\" if word[0]=='@' else word for word in list_word]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Replace emoji to word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup emoji to emotion dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ğŸ˜„': 'joy',\n",
       " 'ğŸ˜›': 'joy',\n",
       " 'ğŸ˜—': 'joy',\n",
       " 'ğŸŒ®': 'anticipation',\n",
       " 'ğŸ—': 'joy',\n",
       " 'ğŸ¡': 'joy',\n",
       " 'ğŸ—’': 'joy',\n",
       " 'ğŸ•Œ': 'joy',\n",
       " 'â¡': 'joy',\n",
       " 'â›“': 'joy',\n",
       " 'ğŸ’»': 'joy',\n",
       " 'ğŸ‹': 'joy',\n",
       " 'â˜‘': 'joy',\n",
       " '\\U0001f929': 'joy',\n",
       " 'ğŸ‹': 'joy',\n",
       " 'ğŸ“Š': 'joy',\n",
       " 'ğŸ¢': 'joy',\n",
       " 'â¬›': 'joy',\n",
       " 'â˜': 'joy',\n",
       " 'ğŸ“': 'joy',\n",
       " 'ğŸ‘’': 'joy',\n",
       " 'ğŸ’…': 'joy',\n",
       " 'â›µ': 'joy',\n",
       " 'ğŸ—£': 'joy',\n",
       " 'ğŸ‘¢': 'joy',\n",
       " 'ğŸ–': 'joy',\n",
       " 'ğŸš©': 'joy',\n",
       " 'ğŸ¸': 'joy',\n",
       " 'ğŸŒ¸': 'joy',\n",
       " 'â¬†': 'joy',\n",
       " 'ğŸ‘”': 'joy',\n",
       " 'â€¼': 'joy',\n",
       " '\\U0001f9db': 'joy',\n",
       " 'ğŸ”®': 'joy',\n",
       " 'ğŸ’€': 'fear',\n",
       " 'ğŸŒ†': 'anticipation',\n",
       " '\\U0001f94c': 'anticipation',\n",
       " '\\U0001f9d8': 'joy',\n",
       " 'ğŸ‡ª': 'anger',\n",
       " 'ğŸ«': 'joy',\n",
       " 'ğŸ’®': 'sadness',\n",
       " 'â˜ª': 'anticipation',\n",
       " 'ğŸŒš': 'joy',\n",
       " 'ğŸ˜': 'sadness',\n",
       " 'â›”': 'disgust',\n",
       " 'ğŸ': 'joy',\n",
       " 'â‰': 'disgust',\n",
       " 'â†–': 'trust',\n",
       " 'ğŸŒˆ': 'trust',\n",
       " 'â™€': 'joy',\n",
       " 'ğŸš': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸ”›': 'joy',\n",
       " 'ğŸ–Š': 'joy',\n",
       " 'ğŸ•¤': 'anticipation',\n",
       " 'ğŸš': 'joy',\n",
       " 'â›´': 'joy',\n",
       " '\\U0001f954': 'joy',\n",
       " '\\U0001f98c': 'joy',\n",
       " 'ğŸ›©': 'joy',\n",
       " '\\U0001f94b': 'trust',\n",
       " '\\U0001f95b': 'joy',\n",
       " 'ãŠ™': 'joy',\n",
       " 'ğŸ”‹': 'joy',\n",
       " 'â–«': 'joy',\n",
       " 'â˜¸': 'joy',\n",
       " 'ğŸ‘¶': 'joy',\n",
       " '\\U0001f940': 'joy',\n",
       " 'ğŸ°': 'joy',\n",
       " 'ğŸ•˜': 'anger',\n",
       " 'â˜': 'joy',\n",
       " 'ğŸ”º': 'joy',\n",
       " 'ğŸ˜¡': 'anger',\n",
       " 'ğŸ” ': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸ”˜': 'sadness',\n",
       " 'â£': 'joy',\n",
       " 'ğŸ™Œ': 'joy',\n",
       " 'ğŸ—¼': 'joy',\n",
       " 'ğŸ†‘': 'joy',\n",
       " '\\U0001f957': 'trust',\n",
       " 'ğŸ™': 'joy',\n",
       " 'â›°': 'joy',\n",
       " 'ğŸ‘¥': 'joy',\n",
       " '\\U0001f955': 'trust',\n",
       " 'ğŸš‹': 'joy',\n",
       " 'ğŸ™…': 'joy',\n",
       " 'âš“': 'trust',\n",
       " 'ğŸ­': 'joy',\n",
       " 'âœ': 'anticipation',\n",
       " 'ğŸ™ƒ': 'joy',\n",
       " 'ğŸª': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'ğŸ½': 'joy',\n",
       " 'ğŸ‡©': 'joy',\n",
       " 'ğŸš': 'joy',\n",
       " 'ğŸ‘Ÿ': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'ğŸ”·': 'anger',\n",
       " 'ğŸ’º': 'joy',\n",
       " '\\U0001f923': 'joy',\n",
       " 'ğŸ°': 'joy',\n",
       " 'âš›': 'anticipation',\n",
       " 'â˜®': 'joy',\n",
       " 'â™£': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'ğŸ•¡': 'anticipation',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'ğŸ§': 'joy',\n",
       " 'ğŸ‰': 'joy',\n",
       " 'ğŸ‡¬': 'anger',\n",
       " 'â›¸': 'anticipation',\n",
       " 'ğŸšƒ': 'joy',\n",
       " 'â›…': 'joy',\n",
       " 'ğŸ’¼': 'joy',\n",
       " 'â˜ƒ': 'joy',\n",
       " 'ğŸ˜¶': 'sadness',\n",
       " 'ğŸš—': 'joy',\n",
       " 'âœ‰': 'joy',\n",
       " 'ğŸ¾': 'joy',\n",
       " 'ğŸ˜‡': 'joy',\n",
       " 'ğŸ—': 'joy',\n",
       " 'ğŸ‘¬': 'joy',\n",
       " '\\U0001f969': 'anticipation',\n",
       " '\\U0001f9d5': 'joy',\n",
       " 'ğŸ‘µ': 'joy',\n",
       " 'ğŸ‡·': 'anger',\n",
       " 'ğŸ•Š': 'joy',\n",
       " 'â¿': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸŒ¯': 'anticipation',\n",
       " 'ğŸ˜': 'joy',\n",
       " '\\U0001f956': 'anticipation',\n",
       " 'ğŸ´': 'joy',\n",
       " 'ğŸ»': 'joy',\n",
       " 'ğŸ••': 'joy',\n",
       " 'ğŸ˜¦': 'surprise',\n",
       " '\\U0001f98e': 'joy',\n",
       " 'ğŸ‚': 'anticipation',\n",
       " '\\U0001f93e': 'trust',\n",
       " 'ğŸ˜‚': 'joy',\n",
       " 'ğŸš°': 'trust',\n",
       " 'ğŸ’§': 'joy',\n",
       " 'ğŸ•Ÿ': 'anticipation',\n",
       " 'âš¡': 'joy',\n",
       " 'ğŸ’©': 'disgust',\n",
       " 'ğŸ“«': 'joy',\n",
       " 'ğŸ’¢': 'joy',\n",
       " 'ğŸ—¨': 'joy',\n",
       " 'ğŸ’”': 'sadness',\n",
       " 'ğŸ¾': 'joy',\n",
       " 'â­': 'joy',\n",
       " '\\U0001f93c': 'anticipation',\n",
       " 'ğŸ˜£': 'sadness',\n",
       " 'ğŸ‘·': 'joy',\n",
       " '\\U0001f95d': 'trust',\n",
       " '\\U0001f953': 'joy',\n",
       " 'ğŸ’¤': 'joy',\n",
       " 'ğŸ’': 'anticipation',\n",
       " '\\U0001f950': 'joy',\n",
       " 'ğŸ˜¹': 'joy',\n",
       " 'ğŸ”™': 'joy',\n",
       " 'ğŸ´': 'joy',\n",
       " 'ğŸš‚': 'joy',\n",
       " 'ğŸ•”': 'joy',\n",
       " 'ğŸ”»': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'ğŸŠ': 'joy',\n",
       " 'ğŸŒŠ': 'joy',\n",
       " '\\U0001f985': 'joy',\n",
       " 'ğŸ™': 'sadness',\n",
       " 'ğŸ›': 'joy',\n",
       " 'ğŸ': 'anticipation',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'ğŸ“§': 'joy',\n",
       " 'ğŸ‘»': 'fear',\n",
       " 'ğŸ‘ª': 'joy',\n",
       " 'ğŸ“¸': 'joy',\n",
       " 'ğŸ›': 'disgust',\n",
       " 'ğŸ‘ƒ': 'joy',\n",
       " 'ğŸ”ƒ': 'joy',\n",
       " 'ğŸ™‹': 'joy',\n",
       " 'ğŸŒ’': 'joy',\n",
       " 'ğŸ¶': 'joy',\n",
       " 'ğŸ…°': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'ğŸšµ': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ¥': 'joy',\n",
       " 'ğŸš¯': 'joy',\n",
       " 'ğŸ›': 'joy',\n",
       " 'ğŸš›': 'joy',\n",
       " 'ğŸ™': 'joy',\n",
       " 'ğŸ‘“': 'joy',\n",
       " 'ğŸŒ¹': 'joy',\n",
       " 'ğŸš³': 'joy',\n",
       " 'ğŸ¤': 'joy',\n",
       " 'ğŸš£': 'joy',\n",
       " 'ğŸŒ‡': 'joy',\n",
       " 'ğŸ™‰': 'joy',\n",
       " 'ğŸ¶': 'joy',\n",
       " 'ğŸ”¯': 'joy',\n",
       " 'ğŸ‘‘': 'joy',\n",
       " 'ğŸ˜‹': 'joy',\n",
       " '\\U0001f924': 'joy',\n",
       " 'ğŸš“': 'trust',\n",
       " 'ğŸ¥': 'joy',\n",
       " 'â¤µ': 'sadness',\n",
       " 'ğŸ‡§': 'anger',\n",
       " 'ğŸ’½': 'joy',\n",
       " 'ğŸ•š': 'joy',\n",
       " 'ğŸŒ™': 'joy',\n",
       " 'ğŸ¤”': 'sadness',\n",
       " 'ğŸ“’': 'joy',\n",
       " 'â˜': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'â™': 'joy',\n",
       " 'â›‘': 'joy',\n",
       " 'âœ’': 'joy',\n",
       " 'ğŸ•›': 'anticipation',\n",
       " 'ğŸ¤’': 'anger',\n",
       " 'ğŸ”€': 'fear',\n",
       " 'ğŸ’†': 'joy',\n",
       " '\\U0001f9df': 'disgust',\n",
       " 'ğŸŒ²': 'joy',\n",
       " 'ğŸ‘§': 'joy',\n",
       " '\\U0001f98b': 'joy',\n",
       " 'ğŸ›‚': 'sadness',\n",
       " 'â™‰': 'trust',\n",
       " 'ğŸ¤': 'anticipation',\n",
       " 'ğŸ¦': 'joy',\n",
       " 'ğŸš…': 'joy',\n",
       " '\\U0001f92f': 'joy',\n",
       " 'ğŸ¤': 'anticipation',\n",
       " 'â™¨': 'joy',\n",
       " 'âšœ': 'joy',\n",
       " 'ğŸ”¡': 'joy',\n",
       " 'âœ‹': 'joy',\n",
       " 'ğŸ‘ ': 'joy',\n",
       " 'ğŸ…±': 'joy',\n",
       " 'ğŸ˜·': 'disgust',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸšº': 'joy',\n",
       " 'ğŸ“¹': 'joy',\n",
       " 'ğŸ”': 'joy',\n",
       " 'â—¼': 'sadness',\n",
       " 'ğŸ‘€': 'joy',\n",
       " '\\U0001f997': 'anticipation',\n",
       " '\\U0001f926': 'sadness',\n",
       " 'ğŸ’¥': 'joy',\n",
       " 'ğŸ”š': 'joy',\n",
       " '\\U0001f9e6': 'joy',\n",
       " '\\U0001f961': 'anticipation',\n",
       " 'ğŸ¼': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'â›ª': 'anticipation',\n",
       " 'â°': 'trust',\n",
       " '\\U0001f933': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ‘½': 'joy',\n",
       " 'ğŸ˜¤': 'anger',\n",
       " 'ğŸ“‹': 'joy',\n",
       " 'ğŸ¬': 'joy',\n",
       " 'ğŸš¿': 'joy',\n",
       " 'ğŸ˜²': 'surprise',\n",
       " '\\U0001f98f': 'anticipation',\n",
       " 'ğŸ“©': 'joy',\n",
       " 'ğŸ”¢': 'trust',\n",
       " 'ğŸ™†': 'joy',\n",
       " 'ğŸ™': 'joy',\n",
       " 'ğŸ…': 'trust',\n",
       " 'ğŸ‘˜': 'joy',\n",
       " 'ğŸ˜ª': 'sadness',\n",
       " 'ğŸ“…': 'anticipation',\n",
       " 'ğŸˆ´': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'â¤´': 'trust',\n",
       " 'ğŸ›': 'joy',\n",
       " 'ğŸš´': 'joy',\n",
       " 'âœ”': 'joy',\n",
       " 'âœ': 'joy',\n",
       " 'ğŸŒ¼': 'joy',\n",
       " 'ğŸ“ ': 'joy',\n",
       " '\\U0001f927': 'joy',\n",
       " 'ğŸ‡®': 'joy',\n",
       " 'ğŸ”±': 'joy',\n",
       " 'ğŸŠ': 'joy',\n",
       " 'ğŸ’Ÿ': 'joy',\n",
       " 'ğŸ–‹': 'trust',\n",
       " 'ğŸ›¬': 'joy',\n",
       " 'ğŸ‡½': 'joy',\n",
       " 'âª': 'joy',\n",
       " 'ğŸ¡': 'anticipation',\n",
       " '\\U0001f945': 'anticipation',\n",
       " 'ğŸ‡¨': 'anger',\n",
       " 'ğŸ“ª': 'joy',\n",
       " 'ğŸ©': 'joy',\n",
       " 'ğŸ‘‚': 'joy',\n",
       " 'ğŸ¿': 'joy',\n",
       " 'ğŸ”¸': 'anticipation',\n",
       " 'ğŸ’‡': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ­': 'joy',\n",
       " 'ğŸ‡¹': 'joy',\n",
       " 'ğŸŒ': 'trust',\n",
       " 'ğŸ”…': 'joy',\n",
       " 'ğŸ“¿': 'joy',\n",
       " 'â›²': 'joy',\n",
       " 'ğŸ˜»': 'joy',\n",
       " 'â›³': 'joy',\n",
       " 'ğŸŒ¨': 'joy',\n",
       " 'ğŸ“Ÿ': 'joy',\n",
       " '\\U0001f947': 'joy',\n",
       " 'ğŸ›': 'joy',\n",
       " 'ğŸš': 'joy',\n",
       " 'ğŸ‘¤': 'sadness',\n",
       " 'ğŸ›Œ': 'joy',\n",
       " 'ğŸ“•': 'anticipation',\n",
       " 'ğŸ©': 'joy',\n",
       " 'ğŸ‡»': 'anticipation',\n",
       " 'ğŸ’‘': 'joy',\n",
       " 'ğŸŒŸ': 'joy',\n",
       " 'ğŸš„': 'trust',\n",
       " 'ğŸ¨': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'ğŸ”': 'joy',\n",
       " 'ğŸ”–': 'anticipation',\n",
       " 'ğŸ‡': 'joy',\n",
       " 'ğŸ—ƒ': 'joy',\n",
       " 'ğŸ“š': 'joy',\n",
       " 'ğŸ˜…': 'joy',\n",
       " 'ğŸ†š': 'joy',\n",
       " 'ğŸ–': 'joy',\n",
       " 'ğŸ”': 'joy',\n",
       " 'ğŸ”³': 'anticipation',\n",
       " 'â¤': 'joy',\n",
       " 'ğŸ†”': 'joy',\n",
       " '\\U0001f95c': 'trust',\n",
       " 'ğŸ‡': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " '\\U0001f93a': 'joy',\n",
       " 'ğŸ˜•': 'sadness',\n",
       " 'âš—': 'sadness',\n",
       " 'ğŸŠ': 'sadness',\n",
       " '\\U0001f92c': 'disgust',\n",
       " 'ğŸ¦€': 'joy',\n",
       " 'ğŸš™': 'joy',\n",
       " 'ğŸ³': 'trust',\n",
       " 'â–ª': 'anticipation',\n",
       " 'âœ´': 'anticipation',\n",
       " 'ğŸš®': 'sadness',\n",
       " 'ğŸ“‘': 'joy',\n",
       " '\\U0001f95e': 'joy',\n",
       " 'ğŸ¤': 'joy',\n",
       " 'ğŸ”’': 'joy',\n",
       " 'ğŸ˜¢': 'sadness',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸ›„': 'joy',\n",
       " 'ğŸ•': 'joy',\n",
       " '\\U0001f944': 'anticipation',\n",
       " 'âš”': 'joy',\n",
       " 'ğŸ“°': 'joy',\n",
       " '\\U0001f922': 'disgust',\n",
       " 'ğŸ†': 'joy',\n",
       " '\\U0001f9dd': 'joy',\n",
       " 'ğŸ¶': 'joy',\n",
       " 'ğŸ—½': 'joy',\n",
       " '\\U0001f948': 'trust',\n",
       " 'ğŸŒ¬': 'joy',\n",
       " 'ğŸ“¶': 'joy',\n",
       " 'ğŸ­': 'joy',\n",
       " 'ğŸ†–': 'joy',\n",
       " 'ğŸŒœ': 'joy',\n",
       " 'ğŸ”¨': 'joy',\n",
       " 'âŒ›': 'joy',\n",
       " 'ğŸšŸ': 'joy',\n",
       " 'ğŸ‰': 'joy',\n",
       " 'ğŸŒ´': 'trust',\n",
       " 'ğŸ¨': 'joy',\n",
       " 'â›': 'joy',\n",
       " 'ãŠ—': 'joy',\n",
       " 'ğŸŒª': 'anticipation',\n",
       " 'ğŸ¦': 'joy',\n",
       " 'âš ': 'joy',\n",
       " 'ğŸˆ¹': 'trust',\n",
       " 'ğŸ¥': 'joy',\n",
       " 'ğŸ‚': 'joy',\n",
       " 'ğŸ“º': 'joy',\n",
       " 'ğŸŒ¦': 'joy',\n",
       " 'ğŸ”ª': 'joy',\n",
       " 'ğŸ˜Š': 'joy',\n",
       " 'ğŸ“¨': 'trust',\n",
       " 'ğŸ›': 'joy',\n",
       " 'Â®': 'joy',\n",
       " 'ğŸ’Š': 'joy',\n",
       " 'ğŸ’²': 'joy',\n",
       " '\\U0001f935': 'joy',\n",
       " 'ğŸ•': 'joy',\n",
       " '\\U0001f958': 'joy',\n",
       " '\\U0001f9d2': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸšœ': 'joy',\n",
       " 'ğŸ”¬': 'trust',\n",
       " 'ğŸˆµ': 'anticipation',\n",
       " 'â•': 'joy',\n",
       " 'ğŸ«': 'joy',\n",
       " 'ğŸ˜®': 'joy',\n",
       " 'â¯': 'joy',\n",
       " '\\U0001f939': 'joy',\n",
       " 'ğŸ¬': 'joy',\n",
       " 'ğŸ““': 'joy',\n",
       " 'ğŸ½': 'joy',\n",
       " 'ğŸšª': 'joy',\n",
       " '\\U0001f964': 'anticipation',\n",
       " 'â™': 'joy',\n",
       " 'â›©': 'joy',\n",
       " 'ğŸ™‚': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ’“': 'joy',\n",
       " 'ğŸ“·': 'joy',\n",
       " 'â†•': 'trust',\n",
       " 'ğŸ’„': 'joy',\n",
       " 'ğŸ”„': 'joy',\n",
       " 'â™': 'joy',\n",
       " 'ğŸ˜©': 'joy',\n",
       " 'ğŸš': 'joy',\n",
       " 'â–¶': 'joy',\n",
       " 'ğŸ˜´': 'joy',\n",
       " 'â—': 'joy',\n",
       " 'ğŸšš': 'joy',\n",
       " 'ğŸ“»': 'joy',\n",
       " 'ğŸš¾': 'sadness',\n",
       " 'ğŸ…': 'trust',\n",
       " '\\U0001f9d0': 'joy',\n",
       " 'ğŸ‡´': 'joy',\n",
       " 'ğŸ±': 'joy',\n",
       " 'ğŸ¹': 'joy',\n",
       " 'ğŸŒ³': 'joy',\n",
       " 'ğŸ’³': 'joy',\n",
       " '\\U0001f930': 'joy',\n",
       " 'â˜‚': 'joy',\n",
       " '\\U0001f9d7': 'joy',\n",
       " 'ğŸ•—': 'joy',\n",
       " 'ğŸ‘‹': 'joy',\n",
       " 'ğŸ·': 'joy',\n",
       " '\\U0001f919': 'joy',\n",
       " 'ğŸ’¡': 'joy',\n",
       " 'Â©': 'joy',\n",
       " 'ğŸ°': 'joy',\n",
       " 'ğŸµ': 'joy',\n",
       " 'â˜¯': 'joy',\n",
       " 'ğŸ”§': 'joy',\n",
       " 'ğŸµ': 'joy',\n",
       " 'ğŸš¡': 'joy',\n",
       " 'ğŸˆ': 'joy',\n",
       " 'ğŸ—': 'joy',\n",
       " 'â›·': 'joy',\n",
       " 'ğŸ‡­': 'joy',\n",
       " 'â¬': 'sadness',\n",
       " '\\U0001f921': 'sadness',\n",
       " 'ğŸœ': 'joy',\n",
       " 'âº': 'trust',\n",
       " 'ğŸ˜µ': 'disgust',\n",
       " 'ğŸ†': 'joy',\n",
       " 'ğŸ‘Œ': 'joy',\n",
       " 'ğŸ‘±': 'joy',\n",
       " 'ğŸ˜º': 'joy',\n",
       " 'ğŸ•¹': 'joy',\n",
       " 'ğŸ•³': 'disgust',\n",
       " 'âš’': 'joy',\n",
       " 'ğŸ•¸': 'fear',\n",
       " 'ğŸ³': 'trust',\n",
       " 'ğŸ‘¯': 'joy',\n",
       " 'ğŸ’·': 'anticipation',\n",
       " 'ğŸŒ‰': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ›³': 'joy',\n",
       " 'ğŸ•–': 'joy',\n",
       " 'ğŸ€': 'joy',\n",
       " 'âš–': 'joy',\n",
       " 'â±': 'anticipation',\n",
       " 'â—½': 'trust',\n",
       " 'ğŸ§€': 'joy',\n",
       " 'â¹': 'joy',\n",
       " '\\U0001f942': 'joy',\n",
       " 'ğŸ’´': 'joy',\n",
       " '\\U0001f9d3': 'joy',\n",
       " 'â¬…': 'joy',\n",
       " 'ğŸŒ': 'sadness',\n",
       " 'ğŸ’¶': 'joy',\n",
       " 'ğŸ¿': 'joy',\n",
       " 'ğŸšˆ': 'joy',\n",
       " 'ğŸš†': 'joy',\n",
       " 'ğŸ’¨': 'joy',\n",
       " 'ğŸ®': 'joy',\n",
       " 'ğŸµ': 'sadness',\n",
       " 'ğŸ‘': 'disgust',\n",
       " 'â¬‡': 'sadness',\n",
       " 'â„': 'joy',\n",
       " 'ğŸš½': 'sadness',\n",
       " 'ğŸ˜”': 'sadness',\n",
       " 'ğŸ·': 'joy',\n",
       " 'ğŸ': 'disgust',\n",
       " 'ğŸ¾': 'joy',\n",
       " 'ğŸ†˜': 'disgust',\n",
       " 'ğŸ”': 'joy',\n",
       " 'â•': 'joy',\n",
       " 'ğŸ˜˜': 'joy',\n",
       " 'ğŸŒ—': 'joy',\n",
       " 'ğŸ‘³': 'joy',\n",
       " 'ğŸ‘°': 'joy',\n",
       " 'ğŸ‘¹': 'anticipation',\n",
       " 'ğŸª': 'joy',\n",
       " '\\U0001f94a': 'anticipation',\n",
       " 'ğŸ–•': 'disgust',\n",
       " 'ğŸ’’': 'anticipation',\n",
       " 'ğŸŸ': 'anticipation',\n",
       " 'ğŸ“µ': 'sadness',\n",
       " '\\U0001f92b': 'joy',\n",
       " 'â†ª': 'anticipation',\n",
       " 'ğŸ‘­': 'joy',\n",
       " 'ğŸ“™': 'trust',\n",
       " '\\U0001f990': 'joy',\n",
       " 'ğŸº': 'anticipation',\n",
       " 'ğŸ´': 'trust',\n",
       " 'ğŸ”¹': 'anticipation',\n",
       " 'ğŸ”': 'joy',\n",
       " 'ğŸ“—': 'joy',\n",
       " 'ğŸŒ½': 'joy',\n",
       " 'ğŸŒ€': 'joy',\n",
       " 'â›½': 'joy',\n",
       " 'â­': 'joy',\n",
       " 'ğŸ€': 'joy',\n",
       " '\\U0001f9e0': 'joy',\n",
       " 'ğŸ›‹': 'joy',\n",
       " 'âš•': 'joy',\n",
       " 'ğŸ’ª': 'joy',\n",
       " 'ğŸ”œ': 'joy',\n",
       " 'ğŸœ': 'joy',\n",
       " 'ğŸ•“': 'joy',\n",
       " 'ğŸ–¼': 'joy',\n",
       " 'ğŸ”†': 'joy',\n",
       " 'ğŸ“½': 'joy',\n",
       " 'ğŸš': 'anticipation',\n",
       " 'ğŸ¤˜': 'joy',\n",
       " 'ğŸ“Œ': 'joy',\n",
       " 'ğŸŒ¿': 'joy',\n",
       " 'ğŸ”©': 'joy',\n",
       " 'ğŸ‘¦': 'joy',\n",
       " 'ğŸ…¾': 'joy',\n",
       " 'ğŸ¡': 'joy',\n",
       " 'â†˜': 'sadness',\n",
       " 'â™¦': 'joy',\n",
       " 'â': 'joy',\n",
       " 'ğŸ˜¯': 'surprise',\n",
       " '\\U0001f992': 'joy',\n",
       " 'â™ ': 'joy',\n",
       " 'ğŸ˜“': 'sadness',\n",
       " 'ğŸ‡³': 'anger',\n",
       " 'ğŸŒ¥': 'joy',\n",
       " '\\U0001f920': 'joy',\n",
       " 'ğŸ‹': 'joy',\n",
       " 'ğŸ˜«': 'sadness',\n",
       " 'ğŸ•‰': 'joy',\n",
       " 'ğŸ™‡': 'joy',\n",
       " 'ğŸº': 'joy',\n",
       " 'â™¿': 'joy',\n",
       " 'ğŸ–': 'joy',\n",
       " 'ğŸ°': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ¸': 'anticipation',\n",
       " 'ğŸ“': 'joy',\n",
       " 'ğŸ˜¾': 'disgust',\n",
       " 'ğŸš¹': 'anticipation',\n",
       " 'ğŸ‘†': 'joy',\n",
       " 'ğŸ¯': 'joy',\n",
       " 'ğŸ’ƒ': 'joy',\n",
       " 'ğŸŒ“': 'joy',\n",
       " 'ğŸ‘‰': 'joy',\n",
       " 'ğŸ˜': 'sadness',\n",
       " 'ğŸ’': 'trust',\n",
       " 'ğŸ“£': 'joy',\n",
       " '\\U0001f98a': 'joy',\n",
       " 'ğŸ®': 'joy',\n",
       " 'ğŸŒ±': 'joy',\n",
       " 'ğŸ˜‘': 'disgust',\n",
       " 'ğŸ“­': 'trust',\n",
       " 'ğŸ¼': 'joy',\n",
       " 'ğŸ‘º': 'sadness',\n",
       " 'ğŸ§': 'joy',\n",
       " 'ğŸ¤—': 'joy',\n",
       " 'ğŸ€': 'joy',\n",
       " 'ğŸ©': 'joy',\n",
       " 'ğŸŒ°': 'joy',\n",
       " 'âœ¡': 'anticipation',\n",
       " 'ğŸ—º': 'joy',\n",
       " 'ğŸ‘›': 'joy',\n",
       " 'ğŸ³': 'joy',\n",
       " 'ğŸ²': 'joy',\n",
       " 'ğŸ¼': 'joy',\n",
       " 'â™Œ': 'joy',\n",
       " '\\U0001f994': 'joy',\n",
       " 'ğŸ†•': 'joy',\n",
       " '\\U0001f952': 'trust',\n",
       " 'ğŸ˜™': 'joy',\n",
       " 'ğŸŒ­': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸ“': 'trust',\n",
       " '\\U0001f928': 'disgust',\n",
       " 'ğŸ“–': 'anticipation',\n",
       " 'ğŸ”': 'joy',\n",
       " 'â–': 'joy',\n",
       " 'ğŸ›¡': 'anticipation',\n",
       " 'ğŸ›«': 'anticipation',\n",
       " 'ğŸ˜­': 'joy',\n",
       " 'ğŸ“‰': 'sadness',\n",
       " 'ğŸ˜¨': 'surprise',\n",
       " 'ğŸ‘•': 'joy',\n",
       " 'ğŸ»': 'anticipation',\n",
       " '\\U0001f934': 'joy',\n",
       " 'ğŸ': 'trust',\n",
       " 'ğŸ•™': 'anticipation',\n",
       " 'â°': 'anticipation',\n",
       " '\\U0001f93d': 'joy',\n",
       " 'âœ': 'anticipation',\n",
       " '\\U0001f91c': 'joy',\n",
       " 'ğŸ˜Œ': 'joy',\n",
       " 'ğŸ‡¶': 'joy',\n",
       " 'ğŸ‘¡': 'joy',\n",
       " 'ğŸ‘£': 'joy',\n",
       " 'ğŸ¤“': 'joy',\n",
       " '\\U0001f6f7': 'anticipation',\n",
       " 'ğŸ˜¸': 'joy',\n",
       " 'âœŒ': 'joy',\n",
       " 'ğŸˆ‚': 'anticipation',\n",
       " '\\U0001f6f8': 'joy',\n",
       " 'ğŸ”¶': 'anger',\n",
       " 'â™’': 'joy',\n",
       " 'ğŸ®': 'joy',\n",
       " '\\U0001f989': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸ—': 'joy',\n",
       " 'ğŸ ': 'joy',\n",
       " 'ğŸ’Œ': 'joy',\n",
       " 'ğŸ“´': 'sadness',\n",
       " 'ğŸ—„': 'sadness',\n",
       " 'ğŸŒ„': 'joy',\n",
       " 'ğŸš”': 'trust',\n",
       " 'ğŸƒ': 'trust',\n",
       " '\\U0001f9d1': 'joy',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'ğŸ—“': 'joy',\n",
       " 'ğŸª': 'anticipation',\n",
       " 'ğŸ': 'trust',\n",
       " 'ğŸ–': 'joy',\n",
       " 'ğŸ’±': 'joy',\n",
       " 'ğŸ“': 'joy',\n",
       " 'ğŸ”Œ': 'joy',\n",
       " '\\U0001f965': 'joy',\n",
       " 'ğŸŒº': 'joy',\n",
       " 'ğŸ˜ƒ': 'joy',\n",
       " 'ğŸ¨': 'trust',\n",
       " 'ğŸ”£': 'joy',\n",
       " 'ğŸ›': 'joy',\n",
       " 'ğŸ’ ': 'joy',\n",
       " '\\U0001f91e': 'joy',\n",
       " 'ğŸ’°': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'â­•': 'joy',\n",
       " 'ğŸ•': 'disgust',\n",
       " 'ğŸ—¾': 'joy',\n",
       " 'ğŸš‡': 'joy',\n",
       " 'â™¾': 'joy',\n",
       " 'âšª': 'anticipation',\n",
       " 'ğŸŠ': 'joy',\n",
       " 'ğŸ“¡': 'joy',\n",
       " 'ğŸ³': 'joy',\n",
       " 'ğŸ˜†': 'joy',\n",
       " 'ğŸ™Š': 'joy',\n",
       " 'â›¹': 'joy',\n",
       " 'ğŸ¿': 'joy',\n",
       " 'ğŸš¢': 'trust',\n",
       " 'ğŸ‘š': 'joy',\n",
       " 'ğŸ–': 'joy',\n",
       " 'âš¾': 'joy',\n",
       " 'ğŸ’¾': 'joy',\n",
       " '\\U0001f991': 'joy',\n",
       " 'ğŸšŠ': 'joy',\n",
       " 'ğŸ£': 'joy',\n",
       " '\\U0001f966': 'trust',\n",
       " '\\U0001f968': 'joy',\n",
       " 'ğŸ”¦': 'joy',\n",
       " '\\U0001f995': 'joy',\n",
       " 'âœ…': 'joy',\n",
       " 'ğŸ’£': 'joy',\n",
       " 'ğŸš¶': 'joy',\n",
       " 'ğŸ”—': 'joy',\n",
       " 'ğŸ†': 'joy',\n",
       " 'ğŸ¥': 'joy',\n",
       " 'ğŸ’¿': 'joy',\n",
       " 'â˜”': 'joy',\n",
       " 'ğŸ“': 'joy',\n",
       " 'â²': 'anticipation',\n",
       " 'ğŸ—»': 'joy',\n",
       " 'âš°': 'fear',\n",
       " 'ğŸ›€': 'joy',\n",
       " 'ğŸ¤–': 'joy',\n",
       " 'ğŸ“³': 'trust',\n",
       " 'ğŸ‘–': 'joy',\n",
       " 'ğŸ’—': 'joy',\n",
       " '\\U0001f91d': 'joy',\n",
       " '\\U0001f986': 'joy',\n",
       " '\\U0001f6f4': 'joy',\n",
       " 'ğŸ¨': 'joy',\n",
       " 'ğŸ‡º': 'anticipation',\n",
       " 'ğŸ›£': 'joy',\n",
       " 'ğŸŒ¡': 'joy',\n",
       " 'ğŸ“¯': 'joy',\n",
       " 'ğŸ“': 'joy',\n",
       " '\\U0001f987': 'joy',\n",
       " 'â›º': 'joy',\n",
       " 'ğŸ†’': 'joy',\n",
       " 'ğŸ™': 'joy',\n",
       " 'ğŸ‘…': 'joy',\n",
       " 'ğŸ†': 'disgust',\n",
       " 'ğŸ•¯': 'joy',\n",
       " 'ğŸ“‡': 'joy',\n",
       " 'â“': 'joy',\n",
       " 'ğŸ“': 'joy',\n",
       " 'ğŸ™„': 'disgust',\n",
       " '\\U0001f9e1': 'joy',\n",
       " '\\U0001f9d6': 'joy',\n",
       " 'ğŸŸ': 'joy',\n",
       " '\\U0001f943': 'joy',\n",
       " 'ğŸ“†': 'joy',\n",
       " 'ğŸ–Œ': 'joy',\n",
       " 'ğŸ’¦': 'joy',\n",
       " 'ğŸ•·': 'fear',\n",
       " 'â˜„': 'joy',\n",
       " 'âœ‚': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'â³': 'anticipation',\n",
       " '\\U0001f988': 'joy',\n",
       " 'â™‚': 'joy',\n",
       " 'ğŸ¹': 'trust',\n",
       " 'ğŸ’¬': 'joy',\n",
       " 'ğŸ“‚': 'trust',\n",
       " 'ğŸ ': 'joy',\n",
       " 'ğŸš˜': 'joy',\n",
       " 'ğŸ…': 'joy',\n",
       " 'ğŸ¦': 'anticipation',\n",
       " 'ğŸ“': 'joy',\n",
       " 'ğŸ«': 'anticipation',\n",
       " 'ğŸ„': 'joy',\n",
       " 'ğŸ˜ˆ': 'joy',\n",
       " 'ğŸ˜¼': 'joy',\n",
       " 'ğŸ˜±': 'surprise',\n",
       " 'ğŸš¦': 'joy',\n",
       " 'ğŸ”²': 'joy',\n",
       " 'ğŸš‰': 'joy',\n",
       " 'âš«': 'anticipation',\n",
       " 'â„¹': 'joy',\n",
       " 'ğŸ¦': 'trust',\n",
       " 'ğŸ––': 'joy',\n",
       " 'ğŸ”“': 'joy',\n",
       " 'ğŸ¡': 'joy',\n",
       " '\\U0001f967': 'trust',\n",
       " 'ğŸ’•': 'joy',\n",
       " 'ğŸŒµ': 'joy',\n",
       " 'ğŸ„': 'joy',\n",
       " 'â™ˆ': 'trust',\n",
       " '\\U0001f9d9': 'joy',\n",
       " '\\U0001f9dc': 'joy',\n",
       " 'ğŸ‘œ': 'joy',\n",
       " 'ğŸ“„': 'joy',\n",
       " 'ğŸ‘®': 'trust',\n",
       " 'ğŸ¦': 'joy',\n",
       " 'ğŸ™€': 'anticipation',\n",
       " 'ğŸƒ': 'anticipation',\n",
       " 'â›±': 'joy',\n",
       " 'ğŸ•¢': 'anticipation',\n",
       " '\\U0001f95a': 'joy',\n",
       " 'ğŸ¯': 'joy',\n",
       " 'ğŸŒ›': 'joy',\n",
       " '\\U0001f96b': 'trust',\n",
       " 'ğŸŒ»': 'joy',\n",
       " 'â›„': 'joy',\n",
       " 'â˜¹': 'sadness',\n",
       " 'ğŸƒ': 'joy',\n",
       " 'ğŸ£': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸŒ”': 'joy',\n",
       " 'â™‹': 'joy',\n",
       " 'ğŸ‡«': 'joy',\n",
       " 'ğŸŸ': 'joy',\n",
       " 'ğŸ ': 'joy',\n",
       " 'ğŸ•¦': 'sadness',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'âœˆ': 'joy',\n",
       " 'ğŸ’š': 'joy',\n",
       " 'ğŸ‡¼': 'trust',\n",
       " 'ğŸ‡¯': 'joy',\n",
       " 'ğŸŒ˜': 'joy',\n",
       " 'ğŸ˜³': 'sadness',\n",
       " 'ğŸ•¶': 'joy',\n",
       " 'ğŸ¹': 'joy',\n",
       " '\\U0001f9de': 'joy',\n",
       " 'ğŸ’™': 'joy',\n",
       " '\\U0001f92e': 'disgust',\n",
       " 'ğŸ‰': 'anticipation',\n",
       " 'ğŸ—¡': 'joy',\n",
       " 'ğŸ€': 'joy',\n",
       " 'â¬œ': 'joy',\n",
       " '\\U0001f932': 'trust',\n",
       " 'ğŸ‘«': 'joy',\n",
       " 'â˜˜': 'joy',\n",
       " 'ğŸ‰‘': 'joy',\n",
       " 'ğŸ…¿': 'joy',\n",
       " 'ğŸ’': 'joy',\n",
       " 'ğŸ”«': 'joy',\n",
       " 'â™Š': 'joy',\n",
       " 'âŒš': 'joy',\n",
       " 'ğŸ•': 'anticipation',\n",
       " 'ğŸ’‚': 'joy',\n",
       " 'ğŸ¾': 'joy',\n",
       " 'ğŸ”‘': 'joy',\n",
       " 'â«': 'joy',\n",
       " 'ğŸ‘¾': 'joy',\n",
       " 'ã€°': 'joy',\n",
       " 'ğŸŒƒ': 'joy',\n",
       " 'ğŸ’œ': 'joy',\n",
       " 'ğŸš¤': 'joy',\n",
       " 'ğŸ‘ˆ': 'joy',\n",
       " 'ğŸ¤•': 'sadness',\n",
       " 'ğŸ': 'joy',\n",
       " 'ğŸ„': 'joy',\n",
       " 'ğŸ§': 'joy',\n",
       " 'ğŸ™ˆ': 'joy',\n",
       " '\\U0001f938': 'joy',\n",
       " 'ğŸ£': 'joy',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'ğŸš¨': 'joy',\n",
       " 'ğŸš­': 'trust',\n",
       " 'ğŸ‘': 'joy',\n",
       " 'ğŸ¯': 'joy',\n",
       " 'ğŸ—‘': 'disgust',\n",
       " 'ğŸ•œ': 'joy',\n",
       " 'ğŸ—': 'joy',\n",
       " 'ğŸ•': 'anticipation',\n",
       " 'â›': 'joy',\n",
       " 'ğŸº': 'joy',\n",
       " 'ğŸ': 'joy',\n",
       " 'â†”': 'joy',\n",
       " 'ğŸ“±': 'joy',\n",
       " 'â™“': 'joy',\n",
       " 'ğŸ˜¿': 'sadness',\n",
       " '\\U0001f91a': 'joy',\n",
       " 'ğŸ’ˆ': 'joy',\n",
       " 'ğŸ¹': 'joy',\n",
       " 'ğŸ‘¨': 'joy',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'ğŸ¤‘': 'joy',\n",
       " 'ğŸ´': 'joy',\n",
       " 'ğŸ”Š': 'joy',\n",
       " 'ğŸ”‰': 'joy',\n",
       " 'ğŸ‘©': 'joy',\n",
       " 'ğŸ¦‚': 'joy',\n",
       " 'ğŸ“¢': 'joy',\n",
       " 'ğŸ±': 'joy',\n",
       " 'ğŸ• ': 'trust',\n",
       " '\\U0001f9e2': 'joy',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'ğŸ¸': 'joy',\n",
       " 'ğŸŒ…': 'joy',\n",
       " 'ğŸ‡¦': 'anger',\n",
       " 'ğŸ’«': 'joy',\n",
       " '\\U0001f925': 'sadness',\n",
       " 'ğŸ‘„': 'joy',\n",
       " 'â›ˆ': 'joy',\n",
       " 'âœ–': 'joy',\n",
       " 'ğŸ•’': 'anticipation',\n",
       " 'ğŸ¬': 'joy',\n",
       " 'ğŸ¢': 'joy',\n",
       " 'â˜¦': 'trust',\n",
       " 'ğŸš²': 'joy',\n",
       " 'â¸': 'joy',\n",
       " '\\U0001f959': 'joy',\n",
       " 'ğŸ™': 'joy',\n",
       " 'ğŸ‘‡': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'ğŸ“œ': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'ğŸ': 'anticipation',\n",
       " '\\U0001f57a': 'joy',\n",
       " 'â˜¢': 'joy',\n",
       " 'ğŸ›…': 'joy',\n",
       " 'ğŸ‘²': 'joy',\n",
       " 'ğŸ˜': 'joy',\n",
       " 'ğŸš«': 'joy',\n",
       " 'â™»': 'joy',\n",
       " 'ğŸ¦„': 'joy',\n",
       " 'ğŸš€': 'joy',\n",
       " 'ğŸ“›': 'joy',\n",
       " '\\U0001f949': 'trust',\n",
       " 'ğŸ·': 'joy',\n",
       " 'ğŸ˜’': 'disgust',\n",
       " 'ğŸ•': 'anticipation',\n",
       " 'ğŸ—': 'joy',\n",
       " 'ğŸ‹': 'joy',\n",
       " 'ğŸ’‰': 'joy',\n",
       " 'ğŸ–': 'joy',\n",
       " 'ğŸš': 'anticipation',\n",
       " 'ğŸ—¿': 'anticipation',\n",
       " 'âš±': 'fear',\n",
       " 'ğŸ”': 'joy',\n",
       " 'ğŸ¦ƒ': 'trust',\n",
       " 'â˜£': 'joy',\n",
       " '\\U0001f9e5': 'anticipation',\n",
       " 'ğŸ‘': 'anticipation',\n",
       " 'ğŸ’µ': 'joy',\n",
       " 'ğŸ•§': 'trust',\n",
       " 'â”': 'anticipation',\n",
       " 'ğŸš±': 'sadness',\n",
       " 'ğŸ‡²': 'joy',\n",
       " '\\U0001f936': 'joy',\n",
       " 'ã€½': 'joy',\n",
       " 'â—¾': 'joy',\n",
       " 'ğŸ”‡': 'joy',\n",
       " 'ğŸ’‹': 'joy',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'âš½': 'anticipation',\n",
       " '\\U0001f9e4': 'joy',\n",
       " 'ğŸŒ•': 'joy',\n",
       " 'ğŸ‘´': 'joy',\n",
       " 'ğŸ™': 'joy',\n",
       " '\\U0001f98d': 'joy',\n",
       " 'ğŸ’›': 'joy',\n",
       " 'ğŸ±': 'joy',\n",
       " 'ğŸ”µ': 'anticipation',\n",
       " 'ğŸ–¨': 'joy',\n",
       " '\\U0001f960': 'joy',\n",
       " 'â†™': 'anticipation',\n",
       " 'ğŸ“ƒ': 'joy',\n",
       " 'ğŸ½': 'trust',\n",
       " 'â†—': 'trust',\n",
       " 'ğŸ˜‰': 'joy',\n",
       " 'â˜•': 'joy',\n",
       " 'ğŸŒ': 'joy',\n",
       " 'ğŸŒ«': 'joy',\n",
       " 'ğŸ”´': 'anticipation',\n",
       " 'ğŸ“˜': 'joy',\n",
       " 'ğŸ“²': 'joy',\n",
       " 'ğŸš¼': 'joy',\n",
       " 'ğŸšŒ': 'anticipation',\n",
       " 'â˜€': 'joy',\n",
       " 'ğŸ²': 'joy',\n",
       " 'ğŸ•µ': 'joy',\n",
       " '\\U0001f931': 'joy',\n",
       " 'ğŸ†': 'joy',\n",
       " 'ğŸ•¥': 'anticipation',\n",
       " 'ğŸ“¥': 'joy',\n",
       " 'ğŸ‡': 'joy',\n",
       " 'ğŸš·': 'joy',\n",
       " '\\U0001f963': 'joy',\n",
       " 'ğŸ¬': 'joy',\n",
       " '\\U0001f6d1': 'joy',\n",
       " 'ğŸ˜¬': 'anticipation',\n",
       " 'ğŸ£': 'joy',\n",
       " 'ğŸ«': 'joy',\n",
       " '\\U0001f962': 'joy',\n",
       " 'ğŸ•‘': 'joy',\n",
       " 'âœ¨': 'joy',\n",
       " 'ğŸ–¥': 'joy',\n",
       " 'ğŸª': 'joy',\n",
       " '\\U0001f937': 'joy',\n",
       " 'ğŸŸ': 'anticipation',\n",
       " 'ğŸ­': 'joy',\n",
       " 'ğŸš»': 'anticipation',\n",
       " 'ğŸ’–': 'joy',\n",
       " '\\U0001f6f6': 'joy',\n",
       " 'ğŸ–±': 'trust',\n",
       " 'â™‘': 'joy',\n",
       " 'ğŸ®': 'joy',\n",
       " 'â˜º': 'joy',\n",
       " 'ğŸ“': 'joy',\n",
       " 'ğŸ†“': 'joy',\n",
       " 'â—€': 'joy',\n",
       " 'âŒ¨': 'joy',\n",
       " 'ğŸ‡µ': 'joy',\n",
       " 'ğŸ‘Š': 'joy',\n",
       " 'ğŸƒ': 'fear',\n",
       " 'ğŸ•´': 'joy',\n",
       " 'ğŸ•': 'anticipation',\n",
       " 'ğŸ“”': 'joy',\n",
       " 'ğŸ‡': 'joy',\n",
       " '\\U0001f5a4': 'joy',\n",
       " 'â™¥': 'joy',\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.unicode_codes import EMOJI_UNICODE, UNICODE_EMOJI\n",
    "\n",
    "list_cal_emoji = df_input.replace_tag.apply(\n",
    "    lambda list_word: \n",
    "    [char for char in \" \".join(list_word) if char in UNICODE_EMOJI]\n",
    ")\n",
    "\n",
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']\n",
    "dict_emoji_all_emotion = defaultdict(list)\n",
    "dict_emoji_emotion = {}\n",
    "seties_emotion = df_input.emotion\n",
    "for idx, list_emoji in enumerate(list_cal_emoji): \n",
    "    list_emoji = list(set(list_emoji))  \n",
    "    for emoji in list_emoji:\n",
    "        if not dict_emoji_all_emotion[emoji]: dict_emoji_all_emotion[emoji]=[0]*len(emotion_array)\n",
    "        dict_emoji_all_emotion[emoji][emotion_array.index(seties_emotion.iloc[idx])] += 1\n",
    "\n",
    "for emoji, list_emoji_num in dict_emoji_all_emotion.items():\n",
    "    dict_emoji_emotion[emoji] = emotion_array[np.argmax(list_emoji_num)]\n",
    "\n",
    "dict_emoji_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...  \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]  \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...  \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_replaced_emoji = []\n",
    "for list_word in df_input.replace_tag:\n",
    "    sentence_before = \" \".join(list_word)\n",
    "    sentence_after = \"\"\n",
    "    for idx, char in enumerate(sentence_before):\n",
    "#         if idx: \n",
    "#             print(char, sentence_before[idx-1], (char == sentence_before[idx-1]))\n",
    "        if char in UNICODE_EMOJI :\n",
    "            if not idx: sentence_after += dict_emoji_emotion[char]\n",
    "            elif char != sentence_before[idx-1]: sentence_after += dict_emoji_emotion[char]\n",
    "            else: continue\n",
    "        else: sentence_after += char\n",
    "    list_replaced_emoji.append(sentence_after.split())\n",
    "df_input[\"replace_emoji\"] = np.array(list_replaced_emoji)\n",
    "\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Replace abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup abbreviation to sentence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7K': ['Sick'],\n",
       " ':-D': ['Laugher'],\n",
       " 'A3': ['Anytime', 'Anywhere', 'Anyplace'],\n",
       " 'AFAIK': ['As', 'Far', 'As', 'I', 'Know'],\n",
       " 'AFK': ['Away', 'From', 'Keyboard'],\n",
       " 'ASAP': ['As', 'Soon', 'As', 'Possible'],\n",
       " 'ASL': ['Age', 'Sex', 'Location'],\n",
       " 'ATK': ['At', 'The', 'Keyboard'],\n",
       " 'ATM': ['At', 'The', 'Moment'],\n",
       " 'B4': ['Before'],\n",
       " 'B4N': ['Bye', 'For', 'Now'],\n",
       " 'BAK': ['Back', 'At', 'Keyboard'],\n",
       " 'BBL': ['Be', 'Back', 'Later'],\n",
       " 'BBS': ['Be', 'Back', 'Soon'],\n",
       " 'BFN': ['Bye', 'For', 'Now'],\n",
       " 'BRB': ['Be', 'Right', 'Back'],\n",
       " 'BRT': ['Be', 'Right', 'There'],\n",
       " 'BTW': ['By', 'The', 'Way'],\n",
       " 'CU': ['See', 'You'],\n",
       " 'CUL8R': ['See', 'You', 'Later'],\n",
       " 'CYA': ['See', 'You'],\n",
       " 'FAQ': ['Frequently', 'Asked', 'Questions'],\n",
       " 'FC': ['Fingers', 'Crossed'],\n",
       " 'FWIW': ['For', 'What', \"It's\", 'Worth'],\n",
       " 'FYI': ['For', 'Your', 'Information'],\n",
       " 'G9': ['Genius'],\n",
       " 'GAL': ['Get', 'A', 'Life'],\n",
       " 'GG': ['Good', 'Game'],\n",
       " 'GMTA': ['Great', 'Minds', 'Think', 'Alike'],\n",
       " 'GN': ['Good', 'Night'],\n",
       " 'GR8': ['Great'],\n",
       " 'IC': ['I', 'See'],\n",
       " 'ICQ': ['I', 'Seek', 'you'],\n",
       " 'ILU': ['I', 'Love', 'You'],\n",
       " 'IMHO': ['In', 'My', 'Honest'],\n",
       " 'IMO': ['In', 'My', 'Opinion'],\n",
       " 'IOW': ['In', 'Other', 'Words'],\n",
       " 'IRL': ['In', 'Real', 'Life'],\n",
       " 'KISS': ['Keep', 'It', 'Simple,', 'Stupid'],\n",
       " 'L8R': ['Later'],\n",
       " 'LDR': ['Long', 'Distance', 'Relationship'],\n",
       " 'LMAO': ['Laugh', 'My', 'A', 'Off'],\n",
       " 'LOL': ['Laughing', 'Out', 'Loud'],\n",
       " 'LTNS': ['Long', 'Time', 'No', 'See'],\n",
       " 'M8': ['Mate'],\n",
       " 'MTE': ['My', 'Thoughts', 'Exactly'],\n",
       " 'NRN': ['No', 'Reply', 'Necessary'],\n",
       " 'OIC': ['Oh', 'I', 'See'],\n",
       " 'PITA': ['Pain', 'In', 'The', 'A'],\n",
       " 'PRT': ['Party'],\n",
       " 'PRW': ['Parents', 'Are', 'Watching'],\n",
       " 'QPSA': ['Que', 'Pasa'],\n",
       " 'ROFL': ['Rolling', 'On', 'The', 'Floor', 'Laughing'],\n",
       " 'ROFLOL': ['Rolling', 'On', 'The', 'Floor', 'Laughing', 'Out', 'Loud'],\n",
       " 'ROTFLMAO': ['Rolling', 'On', 'The', 'Floor', 'Laughing', 'My', 'A', 'Off'],\n",
       " 'SK8': ['Skate'],\n",
       " 'STATS': ['Your', 'sex', 'and', 'age'],\n",
       " 'THX': ['Thank', 'You'],\n",
       " 'TTFN': ['Ta-Ta', 'For', 'Now'],\n",
       " 'TTYL': ['Talk', 'To', 'You', 'Later'],\n",
       " 'U': ['You'],\n",
       " 'U2': ['You', 'Too'],\n",
       " 'U4E': ['Yours', 'For', 'Ever'],\n",
       " 'W8': ['Wait'],\n",
       " 'WB': ['Welcome', 'Back'],\n",
       " 'WTF': ['What', 'The', 'Fuck'],\n",
       " 'WTG': ['Way', 'To', 'Go'],\n",
       " 'WUF': ['Where', 'Are', 'You', 'From']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_abbreviation = {}\n",
    "with open(\"data/abbreviations.txt\", 'r') as f_abbreviation:\n",
    "    for idx, line in enumerate(f_abbreviation.readlines()):\n",
    "        abbreviation, sentence = line.strip().split('=')\n",
    "        dict_abbreviation[abbreviation] = sentence.split()\n",
    "dict_abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...  \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]  \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...  \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"replace_abbreviation\"] = df_input.replace_emoji.apply(\n",
    "    lambda list_word: \n",
    "    [word for list_word_after in [dict_abbreviation[word] if word in dict_abbreviation else [word] for word in list_word] for word in list_word_after]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Remove punctuation, LH and turn to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "      <th>remove_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], as, we, see, trump, is, dangerous, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[now, issa, is, stalking, tasha, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                  remove_punctuation  \n",
       "0  [people, who, post, add, me, on, snapchat, mus...  \n",
       "1  [[tag], as, we, see, trump, is, dangerous, to,...  \n",
       "3              [now, issa, is, stalking, tasha, joy]  \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...  \n",
       "6      [still, waiting, on, those, supplies, liscus]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"remove_punctuation\"] = df_input.replace_abbreviation.apply(\n",
    "    lambda list_word: \n",
    "    [word for word in [re.sub('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'), '', word.replace(\"<LH>\", '').lower()) if word!=\"[tag]\" else word for word in list_word ] if word]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "      <th>remove_punctuation</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>remove_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[[people, who, post, add, me, on, snapchat, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], as, we, see, trump, is, dangerous, to,...</td>\n",
       "      <td>[[tag], a, we, see, trump, be, dangerous, to, ...</td>\n",
       "      <td>[[[tag], a, we, see, trump, be, dangerous, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[now, issa, is, stalking, tasha, joy]</td>\n",
       "      <td>[now, issa, be, stalk, tasha, joy]</td>\n",
       "      <td>[[now, issa, be, stalk, tasha, joy]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[[tag], [tag], thx, for, the, best, time, ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus]</td>\n",
       "      <td>[still, wait, on, those, supply, liscus]</td>\n",
       "      <td>[[still, wait, on, those, supply, liscus]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                  remove_punctuation  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], as, we, see, trump, is, dangerous, to,...   \n",
       "3              [now, issa, is, stalking, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6      [still, waiting, on, those, supplies, liscus]   \n",
       "\n",
       "                                       lemmatization  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], a, we, see, trump, be, dangerous, to, ...   \n",
       "3                 [now, issa, be, stalk, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6           [still, wait, on, those, supply, liscus]   \n",
       "\n",
       "                                     remove_stopword  \n",
       "0  [[people, who, post, add, me, on, snapchat, mu...  \n",
       "1  [[[tag], a, we, see, trump, be, dangerous, to,...  \n",
       "3               [[now, issa, be, stalk, tasha, joy]]  \n",
       "5  [[[tag], [tag], thx, for, the, best, time, ton...  \n",
       "6         [[still, wait, on, those, supply, liscus]]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(list_word):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(list_word):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n",
    "\n",
    "# lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split()])\n",
    "df_input[\"lemmatization\"] = df_input.remove_punctuation.apply(\n",
    "    lambda list_word: \n",
    "    lemmatize_sentence(list_word)\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Remove stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup stopword with tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stopword_all.txt\", 'r') as f_stopword:\n",
    "    list_word = stopwords.words('english')\n",
    "    for line in f_stopword.readlines():\n",
    "#         print(line.strip().split(','))\n",
    "        list_word.extend(line.strip().split(','))\n",
    "    list_word = list(set(list_word))\n",
    "\n",
    "with open(\"data/stopword.txt\", 'w') as f_stopword:\n",
    "    f_stopword.write(\"\\n\".join(list_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read stopword file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['broken',\n",
       " 'rice',\n",
       " '',\n",
       " 'recent',\n",
       " 'uploaded',\n",
       " 'viv',\n",
       " 'created',\n",
       " 'builds',\n",
       " 'bloggers',\n",
       " 'requests',\n",
       " 'couple',\n",
       " 'enables',\n",
       " 'ordering',\n",
       " 'eaten',\n",
       " 'dinner',\n",
       " 'rental',\n",
       " 'money',\n",
       " 'byeeee',\n",
       " 'serves',\n",
       " \"mustn't\",\n",
       " 'online',\n",
       " 'courses',\n",
       " 'whatev',\n",
       " 'newly',\n",
       " 'whaow',\n",
       " 'events',\n",
       " 'dear',\n",
       " 'neways',\n",
       " \"should've\",\n",
       " 'gift',\n",
       " 'programs',\n",
       " 'updates',\n",
       " 'lols',\n",
       " 'nooo',\n",
       " 'tired',\n",
       " 'doubles',\n",
       " 'ymmv',\n",
       " 'shops',\n",
       " 'waited',\n",
       " 'feels',\n",
       " 'com',\n",
       " 'shouldn',\n",
       " 'morning',\n",
       " 'distract',\n",
       " 'excluded',\n",
       " 'recovers',\n",
       " 'writes',\n",
       " 'occured',\n",
       " 'yhoo',\n",
       " 'gave',\n",
       " 'treated',\n",
       " '.com',\n",
       " 'homeworks',\n",
       " 'upgrade',\n",
       " 'afaik',\n",
       " 'projects',\n",
       " 'issue',\n",
       " 'albums',\n",
       " 'isn',\n",
       " 'catch',\n",
       " 'hee',\n",
       " 'btw',\n",
       " 'box',\n",
       " 'february',\n",
       " 'is',\n",
       " 'singled',\n",
       " 'hehehe',\n",
       " 'safe',\n",
       " 'produces',\n",
       " 'approve',\n",
       " 'wahaha',\n",
       " 'buys',\n",
       " 'dont',\n",
       " '#followme',\n",
       " 'furnish',\n",
       " 'does',\n",
       " 'sms',\n",
       " 'loaned',\n",
       " 'primary',\n",
       " 'june',\n",
       " 'refuses',\n",
       " 'talked',\n",
       " 'whom',\n",
       " 'removes',\n",
       " 'dated',\n",
       " 'duper',\n",
       " 'condominium',\n",
       " 'huge',\n",
       " 'obvious',\n",
       " 'tbh',\n",
       " 'woohoo',\n",
       " 'runs',\n",
       " 'yeah',\n",
       " 'company',\n",
       " 'balance',\n",
       " 'saved',\n",
       " 'months',\n",
       " 'today',\n",
       " 'choice',\n",
       " 'interacting',\n",
       " 'iirc',\n",
       " 'purchased',\n",
       " 'weeks',\n",
       " 'hahahaha',\n",
       " 'no',\n",
       " 'hmmm',\n",
       " 'form',\n",
       " 'ftl',\n",
       " 'significant',\n",
       " 'holidays',\n",
       " \"you've\",\n",
       " 'both',\n",
       " 'watever',\n",
       " 'kid',\n",
       " 'organize',\n",
       " 'here',\n",
       " 'order',\n",
       " 'study',\n",
       " 'mens',\n",
       " 'wasnt',\n",
       " 'that',\n",
       " 'sets',\n",
       " 'sqft',\n",
       " 'gg',\n",
       " 'bed',\n",
       " 'allergy',\n",
       " 'resort',\n",
       " 'update',\n",
       " 'omg',\n",
       " 'early',\n",
       " 'why',\n",
       " 'only',\n",
       " 'hour',\n",
       " 'his',\n",
       " 'hopefully',\n",
       " 'consultant',\n",
       " 'announcement',\n",
       " 'bought',\n",
       " 'yah',\n",
       " 'saves',\n",
       " 'replaced',\n",
       " \"it's\",\n",
       " 'some',\n",
       " 'down',\n",
       " 'shitz',\n",
       " 'locations',\n",
       " 'hadn',\n",
       " 'lalalalalalala',\n",
       " 'lmaolmao',\n",
       " 'sunday',\n",
       " 'searches',\n",
       " 'email',\n",
       " 'introduction',\n",
       " 'lose',\n",
       " 'affirmation',\n",
       " 'pics',\n",
       " 'accounts',\n",
       " 'service',\n",
       " 'presentation',\n",
       " 'girl',\n",
       " 'hihi',\n",
       " 'complete',\n",
       " 'congratulation',\n",
       " 'whenz',\n",
       " 'secs',\n",
       " 'tumblr',\n",
       " 'ate',\n",
       " 'notified',\n",
       " 'app',\n",
       " 'female',\n",
       " 'takes',\n",
       " 'country',\n",
       " 'hey',\n",
       " 'each',\n",
       " 'years',\n",
       " 'course',\n",
       " \"weren't\",\n",
       " 'ahead',\n",
       " 'byeee',\n",
       " 'recovered',\n",
       " 'easier',\n",
       " 'bag',\n",
       " 'profiles',\n",
       " 'due',\n",
       " 'clothes',\n",
       " 'most',\n",
       " 'customer',\n",
       " 'single',\n",
       " 'seems',\n",
       " 'turns',\n",
       " 'wish',\n",
       " 'whee',\n",
       " 'yw',\n",
       " 'ideas',\n",
       " 'very',\n",
       " 'activities',\n",
       " 'exclude',\n",
       " 'notes',\n",
       " 'lights',\n",
       " 'acne',\n",
       " 'votes',\n",
       " 'right',\n",
       " 'tuesday',\n",
       " 'yeahhhhh',\n",
       " 'thanky',\n",
       " 'bright',\n",
       " 'argh',\n",
       " 'yall',\n",
       " 'parents',\n",
       " 'minutes',\n",
       " 'j/k',\n",
       " 'slightly',\n",
       " 'common',\n",
       " 'again',\n",
       " 'pm',\n",
       " 'mentions',\n",
       " 'screened',\n",
       " 'double',\n",
       " 'goodnight',\n",
       " 'indicating',\n",
       " 'juz',\n",
       " 'soft',\n",
       " 'fun',\n",
       " 'conv',\n",
       " 'presentations',\n",
       " 'heart',\n",
       " 'comments',\n",
       " 'kill',\n",
       " 'key',\n",
       " 'source',\n",
       " 'problem',\n",
       " 'detail',\n",
       " 'feed',\n",
       " 'having',\n",
       " 'hahah',\n",
       " 'signed',\n",
       " 'awesome',\n",
       " 'bad',\n",
       " 'show',\n",
       " 'im',\n",
       " 'boxes',\n",
       " 'light',\n",
       " 'dad',\n",
       " 'fail',\n",
       " 'when',\n",
       " 'blog',\n",
       " 'yar',\n",
       " 'reviews',\n",
       " 'about',\n",
       " 'stks',\n",
       " 'var',\n",
       " 'y',\n",
       " 'involve',\n",
       " 'stupids',\n",
       " 'msgs',\n",
       " 'ran',\n",
       " 'install',\n",
       " 'rented',\n",
       " 'pair',\n",
       " 'pissed',\n",
       " 'yeh',\n",
       " 'quick',\n",
       " 'http',\n",
       " 'hohoho',\n",
       " 'who',\n",
       " 'safety',\n",
       " 'year',\n",
       " 'monday',\n",
       " 'gifted',\n",
       " 'hears',\n",
       " 'parent',\n",
       " 'pwned',\n",
       " 'targets',\n",
       " 'candies',\n",
       " 'issues',\n",
       " 'hahahahahahahahaha',\n",
       " 'previous',\n",
       " 'stories',\n",
       " 'they',\n",
       " 'places',\n",
       " 'ltd',\n",
       " 'neato',\n",
       " 'until',\n",
       " 'opened',\n",
       " 'sucked',\n",
       " 'sell',\n",
       " 'dunno',\n",
       " 'fast',\n",
       " 'this',\n",
       " 'ini',\n",
       " 'cuz',\n",
       " 'gigs',\n",
       " \"you'll\",\n",
       " 'customers',\n",
       " 'details',\n",
       " 'attend',\n",
       " 'prices',\n",
       " 'bored',\n",
       " 'affiliate',\n",
       " 'fake',\n",
       " 'asses',\n",
       " 'started',\n",
       " 'resulted',\n",
       " 'hosted',\n",
       " 'pictures',\n",
       " 'tweet',\n",
       " 'consults',\n",
       " 'noooooooooo',\n",
       " 'time',\n",
       " 'allow',\n",
       " 'removed',\n",
       " 'boy',\n",
       " 'returned',\n",
       " 'singapore',\n",
       " 'visits',\n",
       " 'shld',\n",
       " 'canceled',\n",
       " 'thinks',\n",
       " 'wow',\n",
       " 'feeds',\n",
       " 'live',\n",
       " 'yeahhh',\n",
       " 'door',\n",
       " 'females',\n",
       " 'with',\n",
       " 'sues',\n",
       " 'hello',\n",
       " 'wrote',\n",
       " 'under',\n",
       " 'late',\n",
       " 'laaa',\n",
       " 'rename',\n",
       " 'loh',\n",
       " 'sync',\n",
       " 'monthly',\n",
       " 'lfg',\n",
       " 'breakfast',\n",
       " 'stfu',\n",
       " 'cum',\n",
       " 'enter',\n",
       " 'for',\n",
       " 'anycase',\n",
       " 'taking',\n",
       " 'close',\n",
       " 'once',\n",
       " 'adding',\n",
       " 'completing',\n",
       " 'lunch',\n",
       " 'cloth',\n",
       " 'managed',\n",
       " 'costs',\n",
       " 'nite',\n",
       " 'specific',\n",
       " 'start',\n",
       " 'launches',\n",
       " 'sides',\n",
       " 'life',\n",
       " 'those',\n",
       " 'area',\n",
       " 'continues',\n",
       " 'hahahahahahaha',\n",
       " 'provides',\n",
       " 'into',\n",
       " 'companies',\n",
       " 'bloody',\n",
       " 'lololol',\n",
       " 'met',\n",
       " 'spends',\n",
       " 'found',\n",
       " 'kinda',\n",
       " 'closed',\n",
       " 'lives',\n",
       " 'me',\n",
       " 'roles',\n",
       " 'coz',\n",
       " 'throats',\n",
       " 'brought',\n",
       " 'think',\n",
       " 'whats',\n",
       " 'doubled',\n",
       " 'stopped',\n",
       " 'means',\n",
       " 'condo',\n",
       " 'min',\n",
       " 'rss',\n",
       " 'pay',\n",
       " 'mustn',\n",
       " 'geez',\n",
       " 'favorite',\n",
       " 'packs',\n",
       " 't',\n",
       " 'worse',\n",
       " 'i',\n",
       " 'wooohoooo',\n",
       " 'sung',\n",
       " 'merry',\n",
       " 'asks',\n",
       " 'friday',\n",
       " 'earns',\n",
       " 'approves',\n",
       " 'stays',\n",
       " 'types',\n",
       " 'wa',\n",
       " 'haah',\n",
       " 'replace',\n",
       " 'minute',\n",
       " 'visited',\n",
       " 'tm',\n",
       " 'during',\n",
       " \"won't\",\n",
       " 'tht',\n",
       " 'shouldnt',\n",
       " 'line',\n",
       " 'report',\n",
       " 'yeahhhh',\n",
       " 'related',\n",
       " 'photo',\n",
       " 'page',\n",
       " 'stops',\n",
       " 'wondered',\n",
       " 'abt',\n",
       " 'website',\n",
       " 'season',\n",
       " 'womens',\n",
       " 'mad',\n",
       " 'fwah',\n",
       " 'vn',\n",
       " 'interacted',\n",
       " 'search',\n",
       " 'yahoocurrency',\n",
       " 'lolol',\n",
       " 'watnot',\n",
       " 'free',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'ask',\n",
       " 'hates',\n",
       " 'flights',\n",
       " 'followed',\n",
       " 'posted',\n",
       " 'baby',\n",
       " 'plzz',\n",
       " 'ya',\n",
       " 'admission',\n",
       " 'totals',\n",
       " 'wrong',\n",
       " 'irl',\n",
       " 'sent',\n",
       " 'him',\n",
       " 'its',\n",
       " 'hoo',\n",
       " 'doesn',\n",
       " 'thks',\n",
       " 'checked',\n",
       " 'pte',\n",
       " 'annoy',\n",
       " 'eng',\n",
       " 'home',\n",
       " 'leavg',\n",
       " 'screens',\n",
       " 'finished',\n",
       " 'main',\n",
       " 'ly',\n",
       " 'locates',\n",
       " 'cookies',\n",
       " 'mine',\n",
       " 'supporting',\n",
       " 'info',\n",
       " 'lanuch',\n",
       " 'guess',\n",
       " 'highest',\n",
       " 'eats',\n",
       " 'win',\n",
       " 'holiday',\n",
       " 'ftw',\n",
       " 'closes',\n",
       " 'yr',\n",
       " 'sue',\n",
       " '#200aday',\n",
       " 'hours',\n",
       " 'great',\n",
       " 'mixes',\n",
       " 'changed',\n",
       " 'idk',\n",
       " 'rated',\n",
       " 'interact',\n",
       " 'opportunity',\n",
       " 'https',\n",
       " 'twittering',\n",
       " 'passed',\n",
       " 'ain',\n",
       " 'emos',\n",
       " 'spots',\n",
       " 'yup',\n",
       " 'pls',\n",
       " 'enable',\n",
       " 'cluttered',\n",
       " 'funny',\n",
       " 'old',\n",
       " 'served',\n",
       " 'jobs',\n",
       " 'liked',\n",
       " 'tree',\n",
       " 'tel',\n",
       " 'ahahahahah',\n",
       " 'car',\n",
       " 'lor',\n",
       " 'fucks',\n",
       " 'shudder',\n",
       " 'book',\n",
       " 'give',\n",
       " \"doesn't\",\n",
       " 'contacted',\n",
       " 'screen',\n",
       " 'flat',\n",
       " 'opportunities',\n",
       " 'himself',\n",
       " 'rooms',\n",
       " 'cove',\n",
       " 'ordered',\n",
       " 'purchase',\n",
       " 'favourite',\n",
       " 'tbl',\n",
       " 'result',\n",
       " 'enters',\n",
       " 'readers',\n",
       " \"couldn't\",\n",
       " 'were',\n",
       " 'boys',\n",
       " 'create',\n",
       " 'shows',\n",
       " 'products',\n",
       " 'zz',\n",
       " 'treats',\n",
       " 'over',\n",
       " 'daily',\n",
       " 'imo',\n",
       " 'renamed',\n",
       " 'retweets',\n",
       " 'articles',\n",
       " 'seasons',\n",
       " 'hurt',\n",
       " 'pf',\n",
       " 'slept',\n",
       " 'emo',\n",
       " 'excuses',\n",
       " 'killed',\n",
       " 'voted',\n",
       " 'being',\n",
       " 'enabling',\n",
       " 'reminded',\n",
       " 'such',\n",
       " 'contents',\n",
       " 'huh',\n",
       " 'take',\n",
       " 'upgraded',\n",
       " 'ni',\n",
       " 'tym',\n",
       " 'w00t',\n",
       " 'presented',\n",
       " 'remembers',\n",
       " 'watch',\n",
       " 'realize',\n",
       " 'dinners',\n",
       " 'movie',\n",
       " 'month',\n",
       " 'bai',\n",
       " 'occur',\n",
       " 'gtg',\n",
       " 'flight',\n",
       " 'fadein',\n",
       " 'tweeting',\n",
       " 'blogging',\n",
       " 'profile',\n",
       " 'heard',\n",
       " '#followback',\n",
       " 'special',\n",
       " 'hihihi',\n",
       " 'calls',\n",
       " 'diff',\n",
       " 'indicates',\n",
       " '#ifollowback',\n",
       " 'should',\n",
       " 'selected',\n",
       " 'version',\n",
       " 'pretty',\n",
       " 'locked',\n",
       " 'classes',\n",
       " 'sleep',\n",
       " 'looks',\n",
       " 'comeback',\n",
       " 'zzzzz',\n",
       " 'seemed',\n",
       " 'docs',\n",
       " 'kills',\n",
       " 'resorts',\n",
       " 'laughs',\n",
       " 'consecutive',\n",
       " 'students',\n",
       " 'choices',\n",
       " 'notify',\n",
       " 'soup',\n",
       " 'women',\n",
       " 'pick',\n",
       " 'de',\n",
       " 'gonna',\n",
       " 'throat',\n",
       " 'picks',\n",
       " 'sang',\n",
       " 'afternoon',\n",
       " 'lunches',\n",
       " 'had',\n",
       " 'umm',\n",
       " 'learns',\n",
       " 'watching',\n",
       " 'vote',\n",
       " 'kyou',\n",
       " 'allowed',\n",
       " 'hand',\n",
       " 'hated',\n",
       " 'searched',\n",
       " 'areas',\n",
       " 'present',\n",
       " 'mean',\n",
       " 'whatevar',\n",
       " 'd',\n",
       " 'the',\n",
       " 'doing',\n",
       " \"wasn't\",\n",
       " 'while',\n",
       " 'refuse',\n",
       " 'biz',\n",
       " 'jus',\n",
       " 'interactions',\n",
       " 'obviously',\n",
       " 'idiots',\n",
       " 'requires',\n",
       " 'haha',\n",
       " 'yum',\n",
       " 'movies',\n",
       " 'rents',\n",
       " 'damn',\n",
       " 'plz',\n",
       " 'forming',\n",
       " 'eat',\n",
       " 'lightly',\n",
       " 'messages',\n",
       " 'world',\n",
       " 'by',\n",
       " \"isn't\",\n",
       " 'off',\n",
       " 'tomolow',\n",
       " 'learn',\n",
       " 'fucking',\n",
       " 'false',\n",
       " 'shares',\n",
       " 'dies',\n",
       " 'couldn',\n",
       " 'wham',\n",
       " 'dreamt',\n",
       " 'son',\n",
       " 'wasn',\n",
       " 'furnished',\n",
       " 'accented',\n",
       " '#followfriday',\n",
       " 'site',\n",
       " 'below',\n",
       " 'tyty',\n",
       " 'fall',\n",
       " 'yay',\n",
       " 'through',\n",
       " 'yumyum',\n",
       " 'follows',\n",
       " 'hairs',\n",
       " 'few',\n",
       " 'usd',\n",
       " 'check',\n",
       " 'spending',\n",
       " 'banks',\n",
       " 'families',\n",
       " 'interacts',\n",
       " 'distracted',\n",
       " 'huhuhu',\n",
       " 'project',\n",
       " 'depend',\n",
       " 'link',\n",
       " 'chicken',\n",
       " 'replied',\n",
       " 'issued',\n",
       " 'hehe',\n",
       " 'above',\n",
       " 'information',\n",
       " 'wat',\n",
       " 'cancel',\n",
       " 'love',\n",
       " 'nor',\n",
       " 'lender',\n",
       " 'wtt',\n",
       " 'adds',\n",
       " 'mini',\n",
       " 'posts',\n",
       " 'july',\n",
       " 'hands',\n",
       " 'she',\n",
       " 'important',\n",
       " 'regular',\n",
       " 'clutter',\n",
       " 'presents',\n",
       " 'web',\n",
       " 'happened',\n",
       " 'wanna',\n",
       " 'message',\n",
       " 'hmm',\n",
       " 'kkkk',\n",
       " 'request',\n",
       " 'noodles',\n",
       " 'photos',\n",
       " 'share',\n",
       " 'wrt',\n",
       " 'dark',\n",
       " 'tsk',\n",
       " 'prefer',\n",
       " 'pre',\n",
       " 'lah',\n",
       " 'phones',\n",
       " 'music',\n",
       " 'zomg',\n",
       " 'phew',\n",
       " 'scratch',\n",
       " 'ish',\n",
       " 'parks',\n",
       " 'girls',\n",
       " 'whatcha',\n",
       " 'wb',\n",
       " 'location',\n",
       " 'receive',\n",
       " 'be',\n",
       " 'listed',\n",
       " 'attended',\n",
       " 'chocolates',\n",
       " 'bye',\n",
       " 'can',\n",
       " 'activity',\n",
       " 'yesterday',\n",
       " 'themselves',\n",
       " 'returns',\n",
       " 'teehee',\n",
       " 'youre',\n",
       " 'watched',\n",
       " 'didnt',\n",
       " 'contact',\n",
       " 'va',\n",
       " 'faster',\n",
       " 'lawl',\n",
       " 'pro',\n",
       " 'creats',\n",
       " 'havent',\n",
       " 'rofl',\n",
       " 'did',\n",
       " 'curr',\n",
       " 'follow',\n",
       " 'fries',\n",
       " 'lots',\n",
       " 'whore',\n",
       " \"mightn't\",\n",
       " 'bday',\n",
       " 'involves',\n",
       " 'sounds',\n",
       " 'byebye',\n",
       " 'bags',\n",
       " 'apparently',\n",
       " 'tart',\n",
       " 'between',\n",
       " 'was',\n",
       " 'blogger',\n",
       " 'further',\n",
       " 'thing',\n",
       " 'learned',\n",
       " 'shown',\n",
       " 'net',\n",
       " 'isnt',\n",
       " 'fuck',\n",
       " 'arrives',\n",
       " 'harder',\n",
       " 'waiting',\n",
       " 'doesnt',\n",
       " 'class',\n",
       " 'installations',\n",
       " 'loss',\n",
       " 'reminds',\n",
       " 'justwit',\n",
       " 'lenders',\n",
       " 'mix',\n",
       " 'phone',\n",
       " 'orders',\n",
       " 'job',\n",
       " 'find',\n",
       " 'kthxbai',\n",
       " 'completes',\n",
       " 'an',\n",
       " 'careers',\n",
       " 'shoe',\n",
       " 'congrats',\n",
       " 'html',\n",
       " 'woah',\n",
       " 'food',\n",
       " 'eh',\n",
       " 'sells',\n",
       " 'ohgod',\n",
       " 'aft',\n",
       " 'updated',\n",
       " 'review',\n",
       " 'chickens',\n",
       " 'reported',\n",
       " 'completed',\n",
       " 'teach',\n",
       " 'humm',\n",
       " 'packed',\n",
       " 'stayed',\n",
       " 'album',\n",
       " 'media',\n",
       " 'imho',\n",
       " 'set',\n",
       " 'wondering',\n",
       " 'hahahahha',\n",
       " 'sq',\n",
       " 'talk',\n",
       " 'belong',\n",
       " 'then',\n",
       " \"hadn't\",\n",
       " 'realised',\n",
       " 'mushroom',\n",
       " 'ticket',\n",
       " 'from',\n",
       " 'whatever',\n",
       " 'congratulations',\n",
       " 'worlds',\n",
       " 'relate',\n",
       " 'myself',\n",
       " 'screws',\n",
       " 'october',\n",
       " 'production',\n",
       " 'earned',\n",
       " 'earn',\n",
       " 'hers',\n",
       " 'idiot',\n",
       " 'esp',\n",
       " 'full',\n",
       " '2moro',\n",
       " 'pains',\n",
       " 'dun',\n",
       " 'reviewed',\n",
       " 'hear',\n",
       " 'productions',\n",
       " 'launched',\n",
       " 'asked',\n",
       " 'sing',\n",
       " 'butter',\n",
       " 'supported',\n",
       " 'career',\n",
       " 'paid',\n",
       " 'pigs',\n",
       " 'males',\n",
       " 'friend',\n",
       " 'announcements',\n",
       " 'problems',\n",
       " 'previews',\n",
       " 'sale',\n",
       " 'requested',\n",
       " 'finds',\n",
       " 'miss',\n",
       " 'works',\n",
       " 'suckz',\n",
       " 'srsly',\n",
       " 'hope',\n",
       " '.org',\n",
       " '#teamfollowback',\n",
       " 'white',\n",
       " 'crowded',\n",
       " 'laa',\n",
       " \"shan't\",\n",
       " 'a',\n",
       " 'tat',\n",
       " 'wth',\n",
       " 'november',\n",
       " 'work',\n",
       " 'brings',\n",
       " 'wished',\n",
       " 'versions',\n",
       " 'tweets',\n",
       " 'ehhh',\n",
       " 'doc',\n",
       " 'whores',\n",
       " 'mom',\n",
       " 'veh',\n",
       " 've',\n",
       " 'side',\n",
       " 'cost',\n",
       " 'famous',\n",
       " 'whatz',\n",
       " 'opens',\n",
       " '#autofollow',\n",
       " 'lined',\n",
       " 'changes',\n",
       " 'noodle',\n",
       " 'theirs',\n",
       " 'ad',\n",
       " 'wonder',\n",
       " 'locks',\n",
       " 'huhu',\n",
       " 'broke',\n",
       " 'like',\n",
       " 'received',\n",
       " '.net',\n",
       " 'sources',\n",
       " 'studies',\n",
       " 'doors',\n",
       " 'million',\n",
       " 'sqm',\n",
       " 'cuties',\n",
       " 'contacts',\n",
       " 'sold',\n",
       " 'sites',\n",
       " 'candy',\n",
       " 'change',\n",
       " 'yeahh',\n",
       " 'took',\n",
       " 'emails',\n",
       " '#followgain',\n",
       " 'reply',\n",
       " 'realizes',\n",
       " 'suck',\n",
       " 'not',\n",
       " 'caught',\n",
       " 'tips',\n",
       " 'trees',\n",
       " 'my',\n",
       " 'tgif',\n",
       " 'previewed',\n",
       " 'target',\n",
       " \"haven't\",\n",
       " 'realises',\n",
       " 'o',\n",
       " 'buy',\n",
       " 'breakfasts',\n",
       " 'on',\n",
       " 'listened',\n",
       " 'at',\n",
       " 'affiliates',\n",
       " \"you're\",\n",
       " 'drs',\n",
       " 'you',\n",
       " 'flats',\n",
       " 'ppl',\n",
       " 'offices',\n",
       " 'shit',\n",
       " 'bleh',\n",
       " 'settled',\n",
       " 'gotta',\n",
       " 'rate',\n",
       " 'road',\n",
       " 'manages',\n",
       " 'office',\n",
       " 'happen',\n",
       " 'bcoz',\n",
       " 'shoes',\n",
       " 'wts',\n",
       " 'article',\n",
       " 'unit',\n",
       " 'formed',\n",
       " 'powerful',\n",
       " 'listening',\n",
       " 'will',\n",
       " 'deposit',\n",
       " 'own',\n",
       " 'later',\n",
       " 'sg',\n",
       " 'wateva',\n",
       " 'product',\n",
       " 'document',\n",
       " 'slipped',\n",
       " 'thursday',\n",
       " 'autoindustry',\n",
       " 'exploded',\n",
       " '#500aday',\n",
       " 'm',\n",
       " 'wins',\n",
       " 'hahahahahah',\n",
       " 'other',\n",
       " 'allows',\n",
       " 're',\n",
       " '#follow',\n",
       " 'story',\n",
       " 'painful',\n",
       " 'nw',\n",
       " '#followngain',\n",
       " 'anymore',\n",
       " 'wooohooo',\n",
       " 'good',\n",
       " 'notifies',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/stopword.txt\", 'r') as f_stopword:\n",
    "    list_stopword = []\n",
    "    for line in f_stopword.readlines():\n",
    "        list_stopword.append(line.strip())\n",
    "    \n",
    "list_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>split_to_word</th>\n",
       "      <th>replace_tag</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>replace_abbreviation</th>\n",
       "      <th>remove_punctuation</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>remove_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[people, who, post, add, me, on, snapchat, mus...</td>\n",
       "      <td>[snapchat, must, dehydrate, thats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], As, we, see,, Trump, is, dangerous, to...</td>\n",
       "      <td>[[tag], as, we, see, trump, is, dangerous, to,...</td>\n",
       "      <td>[[tag], a, we, see, trump, be, dangerous, to, ...</td>\n",
       "      <td>[[tag], see, trump, dangerous, freepress, arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, joy, &lt;LH&gt;]</td>\n",
       "      <td>[now, issa, is, stalking, tasha, joy]</td>\n",
       "      <td>[now, issa, be, stalk, tasha, joy]</td>\n",
       "      <td>[issa, stalk, tasha, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>[@RISKshow, @TheKevinAllison, Thx, for, the, B...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], Thx, for, the, BEST, TIME, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[tag], [tag], thx, for, the, best, time, toni...</td>\n",
       "      <td>[[tag], [tag], thx, best, heartbreakingly, aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[Still, waiting, on, those, supplies, Liscus.,...</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus]</td>\n",
       "      <td>[still, wait, on, those, supply, liscus]</td>\n",
       "      <td>[still, supply, liscus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "3                             []   \n",
       "5      [authentic, LaughOutLoud]   \n",
       "6                             []   \n",
       "\n",
       "                                                text  tweet_id       emotion  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20  anticipation   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350       sadness   \n",
       "3                Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚ <LH>  0x1cd5b0          fear   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c           joy   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8  anticipation   \n",
       "\n",
       "  identification                                      split_to_word  \\\n",
       "0          train  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1          train  [@brianklaas, As, we, see,, Trump, is, dangero...   \n",
       "3          train        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5          train  [@RISKshow, @TheKevinAllison, Thx, for, the, B...   \n",
       "6          train  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                         replace_tag  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, ğŸ˜‚ğŸ˜‚ğŸ˜‚, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                replace_abbreviation  \\\n",
       "0  [People, who, post, \"add, me, on, #Snapchat\", ...   \n",
       "1  [[tag], As, we, see,, Trump, is, dangerous, to...   \n",
       "3        [Now, ISSA, is, stalking, Tasha, joy, <LH>]   \n",
       "5  [[tag], [tag], Thx, for, the, BEST, TIME, toni...   \n",
       "6  [Still, waiting, on, those, supplies, Liscus.,...   \n",
       "\n",
       "                                  remove_punctuation  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], as, we, see, trump, is, dangerous, to,...   \n",
       "3              [now, issa, is, stalking, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6      [still, waiting, on, those, supplies, liscus]   \n",
       "\n",
       "                                       lemmatization  \\\n",
       "0  [people, who, post, add, me, on, snapchat, mus...   \n",
       "1  [[tag], a, we, see, trump, be, dangerous, to, ...   \n",
       "3                 [now, issa, be, stalk, tasha, joy]   \n",
       "5  [[tag], [tag], thx, for, the, best, time, toni...   \n",
       "6           [still, wait, on, those, supply, liscus]   \n",
       "\n",
       "                                     remove_stopword  \n",
       "0                 [snapchat, must, dehydrate, thats]  \n",
       "1  [[tag], see, trump, dangerous, freepress, arou...  \n",
       "3                          [issa, stalk, tasha, joy]  \n",
       "5  [[tag], [tag], thx, best, heartbreakingly, aut...  \n",
       "6                            [still, supply, liscus]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"remove_stopword\"] = df_input.lemmatization.apply(\n",
    "    lambda list_word: \n",
    "    [word for word in list_word if word not in list_stopword]\n",
    ")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Pickle after Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.to_pickle(\"data/train_df_han_preprocess.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.unicode_codes import EMOJI_UNICODE, UNICODE_EMOJI\n",
    "\n",
    "list_cal_emoji = df_input.replace_tag.apply(\n",
    "    lambda list_word: \n",
    "    [char for char in \" \".join(list_word) if char in UNICODE_EMOJI]\n",
    ")\n",
    "\n",
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']\n",
    "dict_emoji_all_emotion = defaultdict(list)\n",
    "dict_emoji_emotion = {}\n",
    "seties_emotion = df_input.emotion\n",
    "for idx, list_emoji in enumerate(list_cal_emoji): \n",
    "    list_emoji = list(set(list_emoji))  \n",
    "    for emoji in list_emoji:\n",
    "        if not dict_emoji_all_emotion[emoji]: dict_emoji_all_emotion[emoji]=[0]*len(emotion_array)\n",
    "        dict_emoji_all_emotion[emoji][emotion_array.index(seties_emotion.iloc[idx])] += 1\n",
    "\n",
    "for emoji, list_emoji_num in dict_emoji_all_emotion.items():\n",
    "    dict_emoji_emotion[emoji] = emotion_array[np.argmax(list_emoji_num)]\n",
    "\n",
    "dict_emoji_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_abbreviation = {}\n",
    "with open(\"data/abbreviations.txt\", 'r') as f_abbreviation:\n",
    "    for idx, line in enumerate(f_abbreviation.readlines()):\n",
    "        abbreviation, sentence = line.strip().split('=')\n",
    "        dict_abbreviation[abbreviation] = sentence.split()\n",
    "dict_abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_sentence(list_word):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(list_word):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/stopword.txt\", 'r') as f_stopword:\n",
    "    list_stopword = []\n",
    "    for line in f_stopword.readlines():\n",
    "        list_stopword.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(name, df, dict_emoji_emotion, dict_abbreviation, list_stopword):\n",
    "    df[\"preprocessing\"] = df[\"text\"].str.split()\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [\"[tag]\" if word[0]=='@' else word for word in list_word]\n",
    "    )\n",
    "    \n",
    "    list_replaced_emoji = []\n",
    "    for list_word in df.preprocessing:\n",
    "        sentence_before = \" \".join(list_word)\n",
    "        sentence_after = \"\"\n",
    "        for idx, char in enumerate(sentence_before):\n",
    "    #         if idx: \n",
    "    #             print(char, sentence_before[idx-1], (char == sentence_before[idx-1]))\n",
    "            if char in UNICODE_EMOJI :\n",
    "                emotion = dict_emoji_emotion.get(char)\n",
    "                if not emotion: continue\n",
    "                if not idx: sentence_after += emotion\n",
    "                elif char != sentence_before[idx-1]: \n",
    "                    sentence_after += emotion\n",
    "                else: continue\n",
    "            else: sentence_after += char\n",
    "        list_replaced_emoji.append(sentence_after.split())\n",
    "    df[\"preprocessing\"] = np.array(list_replaced_emoji)\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [word for list_word_after in [dict_abbreviation[word] if word in dict_abbreviation else [word] for word in list_word] for word in list_word_after]\n",
    "    )\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [word for word in [re.sub('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'), '', word.replace(\"<LH>\", '').lower()) if word!=\"[tag]\" else word for word in list_word ] if word]\n",
    "    )\n",
    "    \n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        lemmatize_sentence(list_word)\n",
    "    )\n",
    "    \n",
    "    df[\"preprocessing\"] = df[\"preprocessing\"].apply(\n",
    "        lambda list_word: \n",
    "        [word for word in list_word if word not in list_stopword]\n",
    "    )\n",
    "\n",
    "    df.to_pickle(\"data/{}.pkl\".format(name)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(\"test_df_han_preprocess\", test_df, dict_emoji_emotion, dict_abbreviation, list_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load a pickle file\n",
    "trains = pd.read_pickle(\"data/train_df_han_preprocess.pkl\")\n",
    "tests = pd.read_pickle(\"data/test_df_han_preprocess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re, csv\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors   \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "# TF-IDF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    use_idf=True, \n",
    "    tokenizer=tknzr.tokenize,\n",
    "    stop_words=stopwords.words('english'))\n",
    "\n",
    "tfidf_vectorizer.fit(trains['remove_stopword'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Data to Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = int(trains.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "BOW_train = trains.remove_stopword[:sep]\n",
    "BOW_train_label = trains.emotion[:sep]\n",
    "BOW_val = trains.remove_stopword[sep:]\n",
    "BOW_val_label = trains.emotion[sep:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.transform(BOW_train)\n",
    "y_train = BOW_train_label\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(BOW_val)\n",
    "y_val = BOW_val_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Emoji List for Encode / Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = emotion_to_sequence(y_train)\n",
    "y_val = emotion_to_sequence(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW Model Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE1_DIM = 256\n",
    "DENSE2_DIM = 256\n",
    "DENSE3_DIM = 64\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 40\n",
    "DROPOUT_DENSE = 0.2\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "TIME_NOW = datetime.now().isoformat().split('.')[0]\n",
    "\n",
    "#############################################################################################\n",
    "MODEL_DESCRIPTION = \"{}_{}\".format(\"BOW\", \"ThreeHidden\")\n",
    "#############################################################################################\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    'logs/{}_{}.csv'.format(MODEL_DESCRIPTION, TIME_NOW))\n",
    "\n",
    "SUMMARY_REPORT_PATH = 'summary/{}_{}.txt'.format(MODEL_DESCRIPTION, TIME_NOW)\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, texts_val, labels_val):\n",
    "    #######################################\n",
    "    # build up the model \n",
    "    # input: texts (np), labels (np)\n",
    "    # output: callback\n",
    "    #######################################\n",
    "    \n",
    "    shuffle_indices = np.random.permutation(texts.shape[0],)\n",
    "    texts = texts[shuffle_indices]\n",
    "    labels = labels[shuffle_indices]\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    model.add(Dense(DENSE3_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    \n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.summary()\n",
    "    with open(SUMMARY_REPORT_PATH, mode='w') as file_summary:\n",
    "        model.summary(print_fn=lambda line: file_summary.write(line+'\\n'))\n",
    "        \n",
    "    history_callback = model.fit(\n",
    "        texts_train, labels_train, \n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    model.save('model/{}_{}.h5'.format(MODEL_DESCRIPTION,TIME_NOW))\n",
    "    \n",
    "    return history_callback\n",
    "\n",
    "history_callback = model_setting(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package for Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors   \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Embedding, Dense, Bidirectional, LSTM, GRU, Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "def text_to_sequence(list_list_word):\n",
    "    tokenized_texts = []\n",
    "    for list_word in list_list_word:\n",
    "        tokenized_text = []\n",
    "        for idx, word in enumerate(list_word):\n",
    "            \n",
    "            # truncate\n",
    "            if idx >= MAX_SEQUENCE_LENGTH: break \n",
    "                \n",
    "            try:\n",
    "                token = token_dict[word.lower()]\n",
    "            except:\n",
    "                token = MAX_WORD\n",
    "                \n",
    "            tokenized_text.append(token)\n",
    "            \n",
    "        # padding \n",
    "        if len(tokenized_text) < MAX_SEQUENCE_LENGTH: \n",
    "            tokenized_text.extend([MAX_WORD]*(MAX_SEQUENCE_LENGTH-len(tokenized_text)))\n",
    "                \n",
    "        tokenized_texts.append(tokenized_text)\n",
    "    \n",
    "    return np.array(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "MAX_WORD = 15000\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_WORD, \n",
    "    use_idf=True, \n",
    "    )\n",
    "tfidf_vectorizer.fit(trains['remove_stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "for idx, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    token_dict[token] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model and Setup Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\n",
    "    \"data/model_ENC3.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "def word2vec_matrix():\n",
    "    matrix_emb = np.zeros((len(token_dict)+1, EMBEDDING_DIM)) # replace MAX_WORD to len(token_dict)\n",
    "    for word, idx in token_dict.items():\n",
    "        try:\n",
    "            vector =  word2vec_model.wv[word]\n",
    "        except:\n",
    "            vector = np.zeros(300,)\n",
    "        if idx < MAX_WORD:\n",
    "            matrix_emb[idx] = np.array(vector)\n",
    "    matrix_emb[MAX_WORD] = np.zeros(300,)     \n",
    "    return matrix_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Emoji List for Encode / Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = text_to_sequence(trains['remove_stopword'])\n",
    "train_label = emotion_to_sequence(trains['emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM1_DIM = 256\n",
    "DENSE1_DIM = 256\n",
    "DENSE2_DIM = 256\n",
    "DENSE3_DIM = 64\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 7\n",
    "DROPOUT_LSTM = 0.2\n",
    "DROPOUT_DENSE = 0.2\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "TIME_NOW = datetime.now().isoformat().split('.')[0]\n",
    "\n",
    "#############################################################################################\n",
    "MODEL_DESCRIPTION = \"{}_{}_{}\".format(\"BiGRU_TwoHidden\", str(LSTM1_DIM), str(DROPOUT_LSTM))\n",
    "#############################################################################################\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    'logs/{}_{}.csv'.format(MODEL_DESCRIPTION, TIME_NOW))\n",
    "\n",
    "SUMMARY_REPORT_PATH = 'summary/{}_{}.txt'.format(MODEL_DESCRIPTION, TIME_NOW)\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, matrix_emb=\"uniform\"):\n",
    "    #######################################\n",
    "    # build up the model \n",
    "    # input: texts (np), labels (np), embedding matrix (np)\n",
    "    # output: callback\n",
    "    #######################################\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n",
    "        \n",
    "    shuffle_indices = np.random.permutation(texts.shape[0],)\n",
    "    texts = texts[shuffle_indices]\n",
    "    labels = labels[shuffle_indices]\n",
    "        \n",
    "    #  split data to train & validation\n",
    "    validation_size = int(texts.shape[0]*VALIDATION_SPLIT)\n",
    "    texts_train = texts[:-validation_size]\n",
    "    texts_val = texts[-validation_size:]\n",
    "    labels_train = labels[:-validation_size]\n",
    "    labels_val = labels[-validation_size:]\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORD+1, EMBEDDING_DIM, embeddings_initializer=matrix_emb, input_length=MAX_SEQUENCE_LENGTH, trainable=True)) # with embedding matrix\n",
    "    model.add(GRU(LSTM1_DIM, dropout=DROPOUT_LSTM, recurrent_dropout=DROPOUT_LSTM))\n",
    "\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(DROPOUT_DENSE))\n",
    "    \n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.summary()\n",
    "    with open(SUMMARY_REPORT_PATH, mode='w') as file_summary:\n",
    "        model.summary(print_fn=lambda line: file_summary.write(line+'\\n'))\n",
    "        \n",
    "    history_callback = model.fit(\n",
    "        texts_train, labels_train, \n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    model.save('model/{}_{}.h5'.format(MODEL_DESCRIPTION,TIME_NOW))\n",
    "    \n",
    "    return history_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback = model_setting(train_sequence, train_label, word2vec_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss and Accuracy of Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "df_his_acc = pd.DataFrame({'train_acc':history_callback.history['acc'], 'validation_acc':history_callback.history['val_acc']})\n",
    "show_train_history(history_callback, 'acc', 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "df_his_acc = pd.DataFrame({'train_loss':history_callback.history['loss'], 'validation_loss':history_callback.history['val_loss']})\n",
    "show_train_history(history_callback, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "tests = pd.read_pickle(\"data/train_df_han_preprocess.pkl\")\n",
    "X_test = text_to_sequence(tests['text'])\n",
    "model = load_model(\"model/{}_{}.h5\".format(MODEL_DESCRIPTION, TIME_NOW))\n",
    "\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result = np.array(\n",
    "    [emotion_array[pred_result_idx] for pred_result_idx in np.argmax(pred_result, axis=1)])\n",
    "\n",
    "tests['emotion'] = pred_result\n",
    "tests = tests.drop('hashtags', axis=1)\n",
    "tests = tests.drop('text', axis=1)\n",
    "tests = tests.drop('identification', axis=1)\n",
    "tests.rename(columns={'tweet_id':'id'}, inplace=True)\n",
    "tests.to_csv('predict_result/prediction_{}_{}.csv'.format(MODEL_DESCRIPTION, TIME_NOW), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "datamining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
